{"meta":{"title":"LITTLEMEEMOON","subtitle":"Every day with dreams is wonderful.","description":"This is my place where I can share my things about life, academic or just for fun, enjoy. :)","author":"Kayleen","url":"https://littlelittlemoon.github.io"},"pages":[{"title":"about","date":"2019-12-12T14:14:36.000Z","updated":"2020-04-08T13:08:47.938Z","comments":false,"path":"about/index.html","permalink":"https://littlelittlemoon.github.io/about/index.html","excerpt":"","text":"[Bear Cave - Kayleen]] 与&nbsp; Kayleen&nbsp; （ 真（ま）白（しろ） ） 对话中... bot_ui_ini()","keywords":"关于"},{"title":"bangumi","date":"2019-02-10T13:32:48.000Z","updated":"2020-04-06T16:05:44.917Z","comments":false,"path":"bangumi/index.html","permalink":"https://littlelittlemoon.github.io/bangumi/index.html","excerpt":"","text":"","keywords":null},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2020-04-06T16:22:30.837Z","comments":false,"path":"client/index.html","permalink":"https://littlelittlemoon.github.io/client/index.html","excerpt":"","text":"直接下载 or 扫码下载：","keywords":"Android客户端"},{"title":"categories","date":"2019-12-20T15:13:48.000Z","updated":"2020-04-09T07:57:24.966Z","comments":true,"path":"categories/index.html","permalink":"https://littlelittlemoon.github.io/categories/index.html","excerpt":"","text":"","keywords":"文章分类"},{"title":"donate","date":"2019-12-20T15:13:05.000Z","updated":"2020-04-07T11:26:06.240Z","comments":false,"path":"donate/index.html","permalink":"https://littlelittlemoon.github.io/donate/index.html","excerpt":"","text":"","keywords":"谢谢饲主了喵~"},{"title":"comment","date":"2019-12-20T15:13:48.000Z","updated":"2020-04-07T11:26:02.901Z","comments":true,"path":"comment/index.html","permalink":"https://littlelittlemoon.github.io/comment/index.html","excerpt":"","text":"念两句诗 人闲桂花落，夜静春山空。 月出惊山鸟，时鸣春涧中。 王维 ·《鸟鸣涧》","keywords":"留言板"},{"title":"lab","date":"2019-01-05T13:47:59.000Z","updated":"2020-04-07T11:26:09.358Z","comments":false,"path":"lab/index.html","permalink":"https://littlelittlemoon.github.io/lab/index.html","excerpt":"","text":"sakura主题 balabala","keywords":"Lab实验室"},{"title":"links","date":"2018-12-19T15:11:06.000Z","updated":"2020-04-07T11:26:13.463Z","comments":true,"path":"links/index.html","permalink":"https://littlelittlemoon.github.io/links/index.html","excerpt":"","text":"","keywords":"友人帐"},{"title":"music","date":"2018-12-20T15:14:28.000Z","updated":"2020-04-09T08:55:04.838Z","comments":false,"path":"music/index.html","permalink":"https://littlelittlemoon.github.io/music/index.html","excerpt":"","text":"","keywords":"小悦邀你停下来听会歌😊"},{"title":"rss","date":"2018-12-20T15:09:03.000Z","updated":"2020-04-06T16:05:44.919Z","comments":true,"path":"rss/index.html","permalink":"https://littlelittlemoon.github.io/rss/index.html","excerpt":"","text":""},{"title":"theme-sakura","date":"2019-01-04T14:53:25.000Z","updated":"2020-04-06T16:05:44.919Z","comments":false,"path":"theme-sakura/index.html","permalink":"https://littlelittlemoon.github.io/theme-sakura/index.html","excerpt":"","text":"Hexo主题Sakura修改自WordPress主题Sakura，感谢原作者Mashiro","keywords":"Hexo 主题 Sakura 🌸"},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2020-04-07T11:26:20.372Z","comments":true,"path":"tags/index.html","permalink":"https://littlelittlemoon.github.io/tags/index.html","excerpt":"","text":"","keywords":"标签"},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2020-04-06T16:05:44.919Z","comments":false,"path":"video/index.html","permalink":"https://littlelittlemoon.github.io/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '放送时间: 2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 番组计划 这里将是永远的回忆 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"B站"}],"posts":[{"title":"僵尸企业分类|模型训练|LightGBM","slug":"僵尸企业分类-模型训练-LightGBM","date":"2020-04-11T11:25:37.000Z","updated":"2020-04-11T10:27:42.117Z","comments":true,"path":"2020/04/11/僵尸企业分类-模型训练-LightGBM/","link":"","permalink":"https://littlelittlemoon.github.io/2020/04/11/%E5%83%B5%E5%B0%B8%E4%BC%81%E4%B8%9A%E5%88%86%E7%B1%BB-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-LightGBM/","excerpt":"","text":"写到前面的话 在上一篇文章中，我对僵尸企业分类问题的数据预处理做了介绍，这篇文章主要介绍模型训练部分，采用的算法框架是LightGBM(Light Gradient Boosting Machine)，是一个实现 GBDT 算法的框架。而GBDT(Gradient Boosting Decision Tree)的主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。 LightGBM有各种很好的特性，类比很多boosting tools优化速度和内存的使用上采用基于预排序的算法进行决策树学习，LightGBM使用基于直方图的算法，支持高效率的并行训练，并且具有以下优点： 1. 更快的训练速度 2. 更低的内存消耗 3. 更好的准确率 4. 分布式支持，可以快速处理海量数据 具体算法原理我也没搞太明白，哈哈。我准备后面好好学习一番，然后再写一篇关于LightGBM的文章，主要是记录自己的学习过程，写一些自己的理解。这里大家如果感兴趣可以去看看官方文档. 模型训练 定义模型训练函数 import lightgbm as lgb from sklearn import metrics def train_model(X_train, y_train, X_valid, y_valid, test=None, feature_cols=None, is_base=True): if feature_cols is None: feature_cols = X_train.columns.drop([&quot;行业&quot;, &quot;区域&quot;, &quot;企业类型&quot;, &quot;控制人类型&quot;]) dtrain = lgb.Dataset(X_train[feature_cols], label=y_train) dvalid = lgb.Dataset(X_valid[feature_cols], label=y_valid) param = {&#39;num_leaves&#39;: 64, &#39;objective&#39;: &#39;binary&#39;, &#39;metric&#39;: &#39;auc&#39;, &#39;seed&#39;: 7} num_round = 1000 print(&quot;Training model!&quot;) bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=20, verbose_eval=False) # 预测结果是概率值，将其转换为binary value valid_pred = bst.predict(X_valid[feature_cols]) valid_pred = valid_pred &gt; 0.5 valid_pred = valid_pred.astype(int) valid_score = metrics.roc_auc_score(y_valid, valid_pred) print(&quot;precision recall fscore support:&quot;) print(metrics.precision_recall_fscore_support(y_valid, valid_pred, average=&#39;micro&#39;)) print(f&quot;Validation AUC score: {valid_score}&quot;) if test is not None: test_pred = bst.predict(test[feature_cols]) test_pred = test_pred &gt; 0.5 test_pred = test_pred.astype(int) test_pred = test[[&#39;ID&#39;]].join(pd.DataFrame({&#39;flag&#39;: test_pred})) if is_base: test_pred.to_csv(&#39;test_base.txt&#39;, sep=&#39;,&#39;, index=False) else: test_pred.to_csv(&#39;test_.txt&#39;, sep=&#39;,&#39;, index=False) return bst, valid_score else: return bst, valid_score 加载处理好的数据集 import pandas as pd # load training data all_data = pd.read_csv(&quot;data/train/train.csv&quot;) # load testing data test = pd.read_csv(&quot;data/test/test.csv&quot;) test_base = pd.read_csv(&quot;data/test/base-test.csv&quot;) 将训练数据集拆分为训练集与验证集 from sklearn.preprocessing import OneHotEncoder, LabelEncoder from sklearn.model_selection import train_test_split import category_encoders as ce from sklearn.utils import shuffle all_data_X = all_data[all_data.columns.drop([&quot;flag&quot;])] all_data_y = all_data[&quot;flag&quot;] # shuffle data（optional） all_data_X, all_data_y = shuffle(all_data_X, all_data_y) # test = shuffle(test) train_X, valid_X, train_y, valid_y = train_test_split(all_data_X, all_data_y, random_state=66) 类型变量处理 通常，本质上是分类的任何数据属性都是离散值，这些离散值属于类别或类的特定有限集合。在属性或由模型预测的变量的上下文中，通常也称为类或标签。 这些离散值本质上可以是文本或数字。这次base data里面的行业，区域，企业类型，控制人类型等就是属于这一类属性，我刚开始的想法是把这类属性值做一个encoding，具体什么类型的encoding可以根据属性对模型的影响程度和模型训练效果来确定。这里只给出了count encoding的例子，如果后面有时间我会专门写一篇处理这类属性的常用方法，做一个详细的总结，这里就不展开了。 Count Encoding for categorical variables cat_features = [&quot;行业&quot;, &quot;区域&quot;, &quot;企业类型&quot;, &quot;控制人类型&quot;] count_enc = ce.CountEncoder(cols=cat_features) # Learn encoding from the training set count_enc.fit(train_X[cat_features]) train_encoded_X = train_X.join(count_enc.transform(train_X[cat_features]) .add_suffix(&quot;_count&quot;)) valid_encoded_X = valid_X.join(count_enc.transform(valid_X[cat_features]) .add_suffix(&quot;_count&quot;)) test_encoded = test.join(count_enc.transform(test[cat_features]) .add_suffix(&quot;_count&quot;)) 模型训练 原始数据 为了看分类数据处理对模型性能的影响，下面的训练没有用编码后的类别数据，训练的时候会把类别属性列删掉。 print(&quot;Baseline model&quot;) _ = train_model(train_X, train_y, valid_X, valid_y, test, is_base=True) Baseline model Training model! precision recall fscore support: (1.0, 1.0, 1.0, None) Validation AUC score: 1.0 结果超出意料，就是最粗糙的数据，没有做任何特征选择和优化，就可以得到这么搞得精度，说实话我当时是怀疑自己的，反复确认了好几遍代码，发现没啥问题。🤣再次证明数据预处理的重要性，数据处理好了，最简单的模型也可以达到很好的效果。 Count Encoding类别属性后的数据 print(&quot;Count Encoding model&quot;) _ = train_model(train_encoded_X, train_y, valid_encoded_X, valid_y, test_encoded, is_base=False) Count Encoding model Training model! precision recall fscore support: (1.0, 1.0, 1.0, None) Validation AUC score: 1.0 两次的训练结果没有任何差别，也失去了特征优化的动力😃","categories":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Machine Learning","slug":"Technology/Machine-Learning","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/"},{"name":"Modeling","slug":"Technology/Machine-Learning/Modeling","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/Modeling/"}],"tags":[{"name":"LightGBM","slug":"LightGBM","permalink":"https://littlelittlemoon.github.io/tags/LightGBM/"},{"name":"Count Encoding","slug":"Count-Encoding","permalink":"https://littlelittlemoon.github.io/tags/Count-Encoding/"}],"keywords":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Machine Learning","slug":"Technology/Machine-Learning","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/"},{"name":"Modeling","slug":"Technology/Machine-Learning/Modeling","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/Modeling/"}]},{"title":"僵尸企业分类|数据预处理","slug":"僵尸企业分类-数据预处理","date":"2020-04-10T12:02:34.000Z","updated":"2020-04-11T08:13:35.896Z","comments":true,"path":"2020/04/10/僵尸企业分类-数据预处理/","link":"","permalink":"https://littlelittlemoon.github.io/2020/04/10/%E5%83%B5%E5%B0%B8%E4%BC%81%E4%B8%9A%E5%88%86%E7%B1%BB-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/","excerpt":"","text":"问题描述 僵尸企业是指缺乏盈利能力却能够以低于市场最优利率成本获得信贷资源，依靠外界输血而缺乏自生能力的企业。僵尸企业的存在破坏了市场机制，加剧了信贷资源的错配，带来了严重的产能过剩问题，还对其他非僵尸企业产生了投资挤出效应。 因此需要对正常企业和僵尸企业进行分类，现给出一批有标签的企业数据作为训练集，标签为0表示正常企业，标签为1表示僵尸企业；同时给出无标签数据作为测试集，请对无标签数据进行分类。 数据集说明 数据集包括训练集和测试集两部分，每个部分又包括: 1. 企业基本数据: 包含企业的一些基本属性以及企业的标签（即flag--0：正常企业，1：僵尸企业）; 2. 企业知识产权数据: 包含企业的知识产权相关信息，根据id可与基本数据一一对应; 3. 企业金融数据: 包含企业2015~2017三年的金融相关信息，根据id可与基本数据相对应; 4. 企业年报数据: 包含企业2015~2017三年的年报数据，根据id可与基本数据相对应。 数据预处理 工具函数定义 combined_base_knowledge_data 函数说明 该函数用于根据ID合并base data和knowledge data数据集。 函数实现 def combined_base_knowledge_data(base_data, knowledge_data): combined_base_knowledge_data = base_data .set_index(&#39;ID&#39;) .join(knowledge_data .set_index(&#39;ID&#39;)) combined_base_knowledge_data = combined_base_knowledge_data .fillna(combined_base_knowledge_data .median()) return combined_base_knowledge_data fill_na 函数说明 该函数用于填充数据集中缺失的数据： 1. 年份数据：根据ID和年份查找出对应ID缺失的年份，然后补全缺失的年份； 2. 其他缺失数据统一用中位数补齐。 函数实现 # fill null year def fill(oragin_data, years = [2015, 2016, 2017]): # pick null year data null_years = oragin_data.loc[oragin_data.year.isna()] # fill year for year in years: IDs = oragin_data[[&quot;ID&quot;]].loc[oragin_data.year == year] for null_year_id in null_years[&quot;ID&quot;].unique(): tmp = IDs.loc[IDs.ID == null_year_id] if tmp.empty: index = oragin_data.loc[( oragin_data.ID == null_year_id) &amp; (oragin_data.year.isna() )].index.tolist() if len(index) != 0: oragin_data.loc[index[0]:index[0], &quot;year&quot;] = year # fill other missing value with median value new_data = oragin_data.fillna(oragin_data.median()) return new_data; combined_new_year_money_data 函数说明 该方法用于根据ID和year合并year data和money data。 函数实现 def combined_new_year_money_data(new_year_data, new_money_data): return pd.merge(new_year_data, new_money_data, on=[&#39;ID&#39;, &#39;year&#39;]) split_data 函数说明 该函数用于根据ID和year拆分新特征，将原数据集三年的数据按年份拆分成新的特征，使拆分后的数据集可与base data数据集用ID一一对应。主要用于year data数据集和money data数据集的拆分。 函数实现 def split_data(combined_new_year_money_data): # split data with year combined_new_year_data_2015 = combined_new_year_money_data.loc[ combined_new_year_money_data.year == 2015] .set_index(&#39;ID&#39;) .add_suffix(&quot;_2015&quot;).drop(columns=[&#39;year_2015&#39;]) combined_new_year_data_2016 = combined_new_year_money_data.loc[ combined_new_year_money_data.year == 2016] .set_index(&#39;ID&#39;) .add_suffix(&quot;_2016&quot;).drop(columns=[&#39;year_2016&#39;]) combined_new_year_data_2017 = combined_new_year_money_data.loc[ combined_new_year_money_data.year == 2017] .set_index(&#39;ID&#39;) .add_suffix(&quot;_2017&quot;).drop(columns=[&#39;year_2017&#39;]) # marge data with ID combined_new_splited_year_money_data = pd.merge( combined_new_year_data_2015, combined_new_year_data_2016, on=[&#39;ID&#39;]) combined_new_splited_year_money_data = pd.merge( combined_new_splited_year_money_data, combined_new_year_data_2017, on=[&#39;ID&#39;]) return combined_new_splited_year_money_data 训练数据集处理 加载原始数据 import pandas as pd # load training data base_train_data = pd.read_csv(&quot;data/train/base-train.csv&quot;) year_train_data = pd.read_csv(&quot;data/train/year-train.csv&quot;) knowledge_train_data = pd.read_csv(&quot;data/train/knowledge-train.csv&quot;) money_train_data = pd.read_csv(&quot;data/train/money-train.csv&quot;) 查看原始数据集信息：base data and knowledge data base_train_data base_train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID 注册时间 注册资本 行业 区域 企业类型 控制人类型 控制人持股比例 flag 0 5986361 2014.0 7090.0 服务业 湖北 有限责任公司 自然人 0.93 0 1 5991749 2007.0 5940.0 零售业 湖南 合伙企业 企业法人 0.57 0 2 5998154 2002.0 9720.0 工业 福建 合伙企业 自然人 0.74 0 3 5984390 2000.0 4800.0 商业服务业 山东 股份有限公司 NaN 0.90 0 4 5980535 2004.0 4530.0 零售业 广东 农民专业合作社 自然人 0.95 0 base_train_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID 注册时间 注册资本 控制人持股比例 flag count 2.851900e+04 28230.000000 28220.000000 28223.000000 28519.000000 mean 4.332423e+06 2007.010627 5024.659816 0.754786 0.392721 std 2.161092e+06 4.326800 2860.157458 0.145008 0.488364 min 2.800000e+01 2000.000000 100.000000 0.510000 0.000000 25% 2.324856e+06 2003.000000 2530.000000 0.630000 0.000000 50% 5.981915e+06 2007.000000 5010.000000 0.750000 0.000000 75% 5.990992e+06 2011.000000 7490.000000 0.880000 1.000000 max 6.000000e+06 2014.000000 10000.000000 1.000000 1.000000 knowledge_train_data knowledge_train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID 专利 商标 著作权 0 28 0.0 1.0 1.0 1 230 0.0 0.0 0.0 2 693 0.0 0.0 0.0 3 990 0.0 0.0 0.0 4 1274 0.0 0.0 0.0 knowledge_train_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID 专利 商标 著作权 count 2.851900e+04 28233.000000 28216.00000 28237.000000 mean 4.332423e+06 0.342507 0.36334 0.371428 std 2.161092e+06 0.474557 0.48097 0.483195 min 2.800000e+01 0.000000 0.00000 0.000000 25% 2.324856e+06 0.000000 0.00000 0.000000 50% 5.981915e+06 0.000000 0.00000 0.000000 75% 5.990992e+06 1.000000 1.00000 1.000000 max 6.000000e+06 1.000000 1.00000 1.000000 合并 base data 和 knowledge data combined_base_knowledge_train_data = combined_base_knowledge_data( base_train_data, knowledge_train_data ) combined_base_knowledge_train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 注册时间 注册资本 行业 区域 企业类型 控制人类型 控制人持股比例 flag 专利 商标 著作权 ID 5986361 2014.0 7090.0 服务业 湖北 有限责任公司 自然人 0.93 0 0.0 0.0 0.0 5991749 2007.0 5940.0 零售业 湖南 合伙企业 企业法人 0.57 0 1.0 1.0 0.0 5998154 2002.0 9720.0 工业 福建 合伙企业 自然人 0.74 0 1.0 1.0 0.0 5984390 2000.0 4800.0 商业服务业 山东 股份有限公司 NaN 0.90 0 0.0 0.0 0.0 5980535 2004.0 4530.0 零售业 广东 农民专业合作社 自然人 0.95 0 0.0 1.0 1.0 查看 year data 和 money data数据集信息 year_train_data year_train_data.head(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year 从业人数 资产总额 负债总额 营业总收入 主营业务收入 利润总额 纳税总额 所有者权益合计 0 28 2015.0 794.0 16400.0 28700.0 72160.0 28864.0 7216.0 0.0 -12300.0 1 230 2015.0 485.0 23520.0 10080.0 115248.0 57624.0 57624.0 0.0 13440.0 2 693 2015.0 534.0 133760.0 125400.0 655424.0 262169.6 196627.2 0.0 8360.0 3 990 2015.0 863.0 33760.0 25320.0 145168.0 58067.2 14516.8 0.0 8440.0 4 1274 2015.0 254.0 74900.0 104325.0 277130.0 110852.0 55426.0 0.0 -29425.0 5 1560 2015.0 491.0 105000.0 98000.0 147000.0 73500.0 29400.0 0.0 7000.0 6 3261 2015.0 799.0 417000.0 822880.0 1751400.0 1401120.0 350280.0 0.0 -405880.0 7 3313 2015.0 784.0 501600.0 986480.0 2156880.0 1294128.0 431376.0 0.0 -484880.0 8 3537 2015.0 647.0 17800.0 13350.0 8900.0 4450.0 2670.0 0.0 4450.0 9 3719 2015.0 369.0 317000.0 465990.0 380400.0 228240.0 152160.0 0.0 -148990.0 year_train_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year 从业人数 ... 利润总额 纳税总额 所有者权益合计 count 8.554800e+04 84692.000000 84743.000000 ... 8.469900e+04 8.473100e+04 84673.000000 mean 4.332626e+06 2015.999870 510.808421 ... 1.027939e+05 7.079659e+04 -27390.880033 std 2.160933e+06 0.816398 283.129690 ... 1.536672e+05 1.588261e+05 108355.730296 min 2.800000e+01 2015.000000 20.000000 ... 7.800000e+00 0.000000e+00 -828340.000000 25% 2.325192e+06 2015.000000 266.000000 ... 1.396755e+04 0.000000e+00 -53130.000000 50% 5.981916e+06 2016.000000 512.000000 ... 4.514400e+04 1.240200e+03 250.000000 75% 5.990992e+06 2017.000000 756.000000 ... 1.238400e+05 6.668040e+04 8900.000000 max 5.999999e+06 2017.000000 1000.000000 ... 1.807398e+06 2.089620e+06 429570.000000 year_train_data money_train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year 债权融资额度 债权融资成本 股权融资额度 股权融资成本 内部融资和贸易融资额度 内部融资和贸易融资成本 项目融资和政策融资额度 项目融资和政策融资成本 0 28 2015.0 0.0 0.0 0.0 0.000 21648.0 1298.88 0.0 0.000 1 230 2015.0 0.0 0.0 0.0 0.000 0.0 0.00 470.4 28.224 2 693 2015.0 0.0 0.0 0.0 0.000 0.0 0.00 5350.4 321.024 3 990 NaN 0.0 0.0 0.0 0.000 0.0 0.00 675.2 40.512 4 1274 2015.0 0.0 0.0 11085.2 443.408 0.0 0.00 NaN 0.000 money_train_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year 债权融资额度 ... 内部融资和贸易融资成本 项目融资和政策融资额度 项目融资和政策融资成本 count 8.554800e+04 84703.000000 84739.000000 ... 84720.000000 84686.000000 84695.000000 mean 4.332626e+06 2016.000378 3353.349261 ... 1555.894230 1020.851124 61.231978 std 2.160933e+06 0.816496 8883.814614 ... 4811.138407 3000.062130 179.871750 min 2.800000e+01 2015.000000 0.000000 ... 0.000000 0.000000 0.000000 25% 2.325192e+06 2015.000000 0.000000 ... 0.000000 0.000000 0.000000 50% 5.981916e+06 2016.000000 0.000000 ... 0.000000 0.000000 0.000000 75% 5.990992e+06 2017.000000 0.000000 ... 10.719000 41.000000 2.520000 max 5.999999e+06 2017.000000 84830.000000 ... 72925.920000 39720.000000 2383.200000 填充缺失数据 year_train_data # fill null new_year_train_data = fill_na(year_train_data) new_year_train_data.set_index([&#39;ID&#39;, &#39;year&#39;]) new_year_train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year 从业人数 资产总额 负债总额 营业总收入 主营业务收入 利润总额 纳税总额 所有者权益合计 0 28 2015.0 794.0 16400.0 28700.0 72160.0 28864.0 7216.0 0.0 -12300.0 1 230 2015.0 485.0 23520.0 10080.0 115248.0 57624.0 57624.0 0.0 13440.0 2 693 2015.0 534.0 133760.0 125400.0 655424.0 262169.6 196627.2 0.0 8360.0 3 990 2015.0 863.0 33760.0 25320.0 145168.0 58067.2 14516.8 0.0 8440.0 4 1274 2015.0 254.0 74900.0 104325.0 277130.0 110852.0 55426.0 0.0 -29425.0 money_train_data # fill null new_money_train_data = fill_na(money_train_data) new_money_train_data.set_index([&#39;ID&#39;, &#39;year&#39;]) # new_money_train_data.head(8) new_money_train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year 债权融资额度 债权融资成本 股权融资额度 股权融资成本 内部融资和贸易融资额度 内部融资和贸易融资成本 项目融资和政策融资额度 项目融资和政策融资成本 0 28 2015.0 0.0 0.0 0.0 0.000 21648.0 1298.88 0.0 0.000 1 230 2015.0 0.0 0.0 0.0 0.000 0.0 0.00 470.4 28.224 2 693 2015.0 0.0 0.0 0.0 0.000 0.0 0.00 5350.4 321.024 3 990 2015.0 0.0 0.0 0.0 0.000 0.0 0.00 675.2 40.512 4 1274 2015.0 0.0 0.0 11085.2 443.408 0.0 0.00 0.0 0.000 合并year data和money data数据集 # Merge new year and money data combined_new_year_money_train_data = combined_new_year_money_data( new_year_train_data, new_money_train_data ) combined_new_year_money_train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year 从业人数 资产总额 ... 内部融资和贸易融资额度 内部融资和贸易融资成本 项目融资和政策融资额度 项目融资和政策融资成本 0 28 2015.0 794.0 16400.0 ... 21648.0 1298.88 0.0 0.000 1 230 2015.0 485.0 23520.0 ... 0.0 0.00 470.4 28.224 2 693 2015.0 534.0 133760.0 ... 0.0 0.00 5350.4 321.024 3 990 2015.0 863.0 33760.0 ... 0.0 0.00 675.2 40.512 4 1274 2015.0 254.0 74900.0 ... 0.0 0.00 0.0 0.000 拆分新特征 将合并后的数据集中每一年的数据拆分成新的特征，使之与base data通过ID一一对应。 splited_year_money_train_data = split_data(combined_new_year_money_train_data) splited_year_money_train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 从业人数_2015 资产总额_2015 负债总额_2015 ... 内部融资和贸易融资额度_2017 内部融资和贸易融资成本_2017 项目融资和政策融资额度_2017 项目融资和政策融资成本_2017 ID 28 794.0 16400.0 28700.0 ... 0.0 0.000 0.0 0.0 230 485.0 23520.0 10080.0 ... 0.0 0.000 0.0 0.0 693 534.0 133760.0 125400.0 ... 0.0 0.000 0.0 0.0 990 863.0 33760.0 25320.0 ... 111661.2 6699.672 0.0 0.0 1274 254.0 74900.0 104325.0 ... 0.0 0.000 0.0 0.0 5 rows × 48 columns 合并处理好的数据集 train = pd.merge(combined_base_knowledge_train_data, splited_year_money_train_data, on=[&#39;ID&#39;]) train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 注册时间 注册资本 行业 ... 内部融资和贸易融资额度_2017 内部融资和贸易融资成本_2017 项目融资和政策融资额度_2017 项目融资和政策融资成本_2017 ID 5986361 2014.0 7090.0 服务业 ... 0.0 0.0 0.0 0.0 5991749 2007.0 5940.0 零售业 ... 80190.0 4811.4 0.0 0.0 5998154 2002.0 9720.0 工业 ... 0.0 0.0 0.0 0.0 5984390 2000.0 4800.0 商业服务业 ... 48960.0 2937.6 0.0 0.0 5980535 2004.0 4530.0 零售业 ... 0.0 0.0 0.0 0.0 5 rows × 59 columns 保存处理好的训练数据集 train.to_csv(&quot;data/train/train.csv&quot;) train.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 注册时间 注册资本 控制人持股比例 ... 内部融资和贸易融资额度_2017 内部融资和贸易融资成本_2017 项目融资和政策融资额度_2017 项目融资和政策融资成本_2017 count 28516.000000 28516.000000 28516.000000 ... 2.851600e+04 28516.000000 28516.000000 28516.000000 mean 2007.010836 5024.064385 0.754743 ... 2.850957e+04 1712.928322 1115.153493 67.013075 std 4.304831 2844.938780 0.144249 ... 8.651585e+04 5202.890113 3229.079861 193.992216 min 2000.000000 100.000000 0.510000 ... 0.000000e+00 0.000000 0.000000 0.000000 25% 2003.000000 2560.000000 0.630000 ... 0.000000e+00 0.000000 0.000000 0.000000 50% 2007.000000 5010.000000 0.750000 ... 0.000000e+00 0.000000 0.000000 0.000000 75% 2011.000000 7470.000000 0.880000 ... 0.000000e+00 0.000000 0.000000 0.000000 max 2014.000000 10000.000000 1.000000 ... 1.215432e+06 72925.920000 38930.000000 2335.800000 8 rows × 55 columns 训练数据集处理 按照刚刚处理训练集的流程处理测试数据集。 ### 加载原始数据集 # load testing data base_test_data = pd.read_csv(&quot;data/test/base-test.csv&quot;) year_test_data = pd.read_csv(&quot;data/test/year-test.csv&quot;) knowledge_test_data = pd.read_csv(&quot;data/test/knowledge-test.csv&quot;) money_test_data = pd.read_csv(&quot;data/test/money-test.csv&quot;) 合并 base data 和 knowledge data combined_base_knowledge_test_data = combined_base_knowledge_data( base_test_data, knowledge_test_data ) combined_base_knowledge_test_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 注册时间 注册资本 控制人持股比例 专利 商标 著作权 count 7132.000000 7132.000000 7132.000000 7132.000000 7132.000000 7132.000000 mean 2007.077257 5039.914470 0.754799 0.342821 0.358385 0.373528 std 4.321929 2839.476208 0.143953 0.474686 0.479560 0.483774 min 2000.000000 100.000000 0.510000 0.000000 0.000000 0.000000 25% 2003.000000 2640.000000 0.630000 0.000000 0.000000 0.000000 50% 2007.000000 5040.000000 0.750000 0.000000 0.000000 0.000000 75% 2011.000000 7450.000000 0.880000 1.000000 1.000000 1.000000 max 2014.000000 10000.000000 1.000000 1.000000 1.000000 1.000000 year data and money data 填充缺失值并验证填充结果 year_test_data # fill null new_year_test_data = fill_na(year_test_data) new_year_test_data.set_index([&#39;ID&#39;, &#39;year&#39;]) new_year_test_data.describe() print(&quot;unique ID count in base data:&quot;, base_test_data[&quot;ID&quot;] .nunique()) print(&quot;2015 unique ID count in year data:&quot;, new_year_test_data[&quot;ID&quot;] .loc[new_year_test_data.year==2015] .nunique()) print(&quot;2016 unique ID count in year data:&quot;, new_year_test_data[&quot;ID&quot;] .loc[new_year_test_data.year==2016] .nunique()) print(&quot;2017 unique ID count in year data:&quot;, new_year_test_data[&quot;ID&quot;] .loc[new_year_test_data.year==2017] .nunique()) unique ID count in base data: 7132 2015 unique ID count in year data: 7132 2016 unique ID count in year data: 7132 2017 unique ID count in year data: 7132 money_test_data # fill null new_money_test_data = fill_na(money_test_data) new_money_test_data.set_index([&#39;ID&#39;, &#39;year&#39;]) # new_money_train_data.head(8) new_money_test_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year 债权融资额度 ... 内部融资和贸易融资额度 内部融资和贸易融资成本 项目融资和政策融资额度 项目融资和政策融资成本 count 2.139600e+04 21396.000000 21396.000000 ... 2.139600e+04 21396.000000 21396.000000 21396.000000 mean 4.332655e+06 2016.000000 3209.631146 ... 2.677436e+04 1598.629057 1025.091606 61.698997 std 2.163020e+06 0.816516 8711.231069 ... 8.272665e+04 4950.288392 3050.157292 183.387577 min 4.290000e+02 2015.000000 0.000000 ... 0.000000e+00 0.000000 0.000000 0.000000 25% 2.331607e+06 2015.000000 0.000000 ... 0.000000e+00 0.000000 0.000000 0.000000 50% 5.981952e+06 2016.000000 0.000000 ... 0.000000e+00 0.000000 0.000000 0.000000 75% 5.990780e+06 2017.000000 0.000000 ... 2.565000e+02 0.850500 0.000000 0.000000 max 5.999998e+06 2017.000000 ... 1.257150e+06 75429.000000 40970.000000 2458.200000 print(&quot;unique ID count in base data:&quot;, base_test_data[&quot;ID&quot;].nunique()) print(&quot;2015 unique ID count in money data:&quot;, new_money_test_data[&quot;ID&quot;] .loc[new_money_test_data.year==2015] .nunique()) print(&quot;2016 unique ID count in money data:&quot;, new_money_test_data[&quot;ID&quot;] .loc[new_money_test_data.year==2016] .nunique()) print(&quot;2017 unique ID count in money data:&quot;, new_money_test_data[&quot;ID&quot;] .loc[new_money_test_data.year==2017] .nunique()) unique ID count in base data: 7132 2015 unique ID count in money data: 7132 2016 unique ID count in money data: 7132 2017 unique ID count in money data: 7132 合并 year data 和 money data combined_new_year_money_test_data = combined_new_year_money_data( new_year_test_data, new_money_test_data ) combined_new_year_money_test_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year 从业人数 ... 内部融资和贸易融资额度 内部融资和贸易融资成本 项目融资和政策融资额度 项目融资和政策融资成本 0 429 2015.0 136.0 ... 0.0 0.000 0.0 0.00 1 727 2015.0 375.0 ... 0.0 0.000 0.0 0.00 2 1137 2015.0 289.0 ... 24460.8 1467.648 0.0 0.00 3 1873 2015.0 889.0 ... 0.0 0.000 0.0 0.00 4 2260 2015.0 689.0 ... 0.0 0.000 11287.5 677.25 print(&quot;unique ID count in combined data:&quot;, combined_new_year_money_test_data[&quot;ID&quot;].nunique()) unique ID count in combined data: 7132 拆分新特征 splited_year_money_test_data = split_data(combined_new_year_money_test_data) splited_year_money_test_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 从业人数_2015 资产总额_2015 负债总额_2015 ... 内部融资和贸易融资额度_2017 内部融资和贸易融资成本_2017 项目融资和政策融资额度_2017 项目融资和政策融资成本_2017 ID 429 136.0 193400.0 183730.0 ... 0.0 0.000 0.0 0.00 727 375.0 366240.0 536280.0 ... 0.0 0.000 0.0 0.00 1137 289.0 87200.0 40320.0 ... 0.0 0.000 1008.0 60.48 1873 889.0 229320.0 222950.0 ... 12612.6 756.756 0.0 0.00 2260 689.0 225750.0 325080.0 ... 0.0 0.000 0.0 0.00 5 rows × 48 columns 合并处理好的数据集 根据ID合并处理好的数据集。 test = pd.merge(combined_base_knowledge_test_data, splited_year_money_test_data, on=[&#39;ID&#39;]) test.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 注册时间 注册资本 行业 ... 内部融资和贸易融资额度_2017 内部融资和贸易融资成本_2017 项目融资和政策融资额度_2017 项目融资和政策融资成本_2017 ID 5991927 2010.0 8790.0 工业 ... 341491.5 20489.49 0.0 0.000 5998351 2005.0 270.0 服务业 ... 0.0 0.00 194.4 11.664 5992703 2012.0 230.0 服务业 ... 0.0 0.00 0.0 0.000 5979231 2003.0 5980.0 商业服务业 ... 75348.0 4520.88 0.0 0.000 5995422 2007.0 160.0 工业 ... 8856.0 531.36 0.0 0.000 5 rows × 58 columns 保存处理好的测试数据集 test.to_csv(&quot;data/test/test.csv&quot;) 总结 这次数据处理比较简单，主要注意以下几个点： 1. 对于年份数据缺失值的处理，可以根据ID和年份判断缺失的年份应该是哪一年，然后填充相应的年份值即可，如果是同一ID缺失两年数据，这个就随缘了😆，我是按照从上到下的顺序填充的（可能有更好的方法，大家可以提出自己的想法），例如ID为123的企业缺失2015和2017的年份，那么依次将year属性值填充为2015和2017； 2. 对于其他数据我就直接粗暴的填充中位数，本来是想先这么填充然后看看效果再优化，结果训练测试后的结果还不错，所以也就没有再优化了； 下篇文章中我会对模型训练过程作一个说明，使用的是LightGBM框架（LightGBM是使用基于树的学习算法的梯度增强框架）。","categories":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Machine Learning","slug":"Technology/Machine-Learning","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/"},{"name":"Data pre-processing","slug":"Technology/Machine-Learning/Data-pre-processing","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/Data-pre-processing/"}],"tags":[{"name":"Data pre-processing","slug":"Data-pre-processing","permalink":"https://littlelittlemoon.github.io/tags/Data-pre-processing/"},{"name":"Categorical Data","slug":"Categorical-Data","permalink":"https://littlelittlemoon.github.io/tags/Categorical-Data/"},{"name":"Missing value","slug":"Missing-value","permalink":"https://littlelittlemoon.github.io/tags/Missing-value/"}],"keywords":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Machine Learning","slug":"Technology/Machine-Learning","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/"},{"name":"Data pre-processing","slug":"Technology/Machine-Learning/Data-pre-processing","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/Data-pre-processing/"}]},{"title":"Leetcode Note in Apri 2020 | part 2 |day 08-14","slug":"Leetcode-Note-in-Apri-2020-part-2-day-08-14","date":"2020-04-08T05:54:16.000Z","updated":"2020-04-11T16:26:06.065Z","comments":true,"path":"2020/04/08/Leetcode-Note-in-Apri-2020-part-2-day-08-14/","link":"","permalink":"https://littlelittlemoon.github.io/2020/04/08/Leetcode-Note-in-Apri-2020-part-2-day-08-14/","excerpt":"","text":"写在前面的话 第二次更新，继续前进~ Backspace String Compare Given two strings S and T, return if they are equal when both are typed into empty text editors. # means a backspace character. Examples: &gt; Input: S = \"ab#c\", T = \"ad#c\" Output: true Explanation: Both S and T become \"ac\". &gt; Input: S = \"ab##\", T = \"c#d#\" Output: true Explanation: Both S and T become \"\". &gt; Input: S = \"a##c\", T = \"#a#c\" Output: true Explanation: Both S and T become \"c\". &gt; Input: S = \"a#c\", T = \"b\" Output: false Explanation: S becomes \"c\" while T becomes \"b\". Solution 这个题对于我来说有点难度，昨天(9号)晚上做了两个小时吧，重点是通不过所有测试用例，总是有一些细节没有考虑到，而且代码越写越复杂，加上昨天中午没睡午觉，到最后自己把自己都搞懵了，气😠。后面睡觉的时候发现自己脑子真的是一根筋，换个思路这题会简单很多。先看看代码吧。 Mine 核心思路：从后往前找可能留下来的字符，并一一比对两个字符串对应位置能留下来的的字符是否相等。注意循环判断条件的设置！ class Solution: def backspaceCompare(self, S: str, T: str) -&gt; bool: s_i = len(S) - 1 t_i = len(T) - 1 s_back = t_back = 0 while True: # Loop will be stoped when any of the following situations occur: # 1. s_i completed the last character access # 2. S[s_i] is a letter (S[s_i] != &#39;#&#39;) and don&#39;t need to go back # currently (s_back = 0) # Same as T, t_i and t_back while s_i &gt;= 0: if S[s_i] == &#39;#&#39;: s_i, s_back = s_i - 1, s_back + 1 elif S[s_i] != &#39;#&#39; and s_back &gt; 0: s_i, s_back = s_i - 1, s_back - 1 else: break while t_i &gt;= 0: if T[t_i] == &#39;#&#39;: t_i, t_back = t_i - 1, t_back + 1 elif T[t_i] != &#39;#&#39; and t_back &gt; 0: t_i, t_back = t_i - 1, t_back - 1 else: break # There are only two situations will return True: # 1. both s_i and t_i are not out of index and S[s_i] = T[t_i] # 2. both s_i and t_i are out of index, it means no letter needs # to be compared and previous corresponding letters are equal. # otherwise, it will return false in situation 1 if s_i &gt;= 0 and t_i &gt;= 0: if S[s_i] != T[t_i]: return False s_i, t_i = s_i - 1, t_i - 1 else: return s_i &lt; 0 and t_i &lt; 0 其实还是蛮复杂的，可能有更好的解决方法，希望小伙伴们给出你们的思路和建议。我刚开始的思路是从前往后去找可能留下的字母，但是这个变化太大了，而且判断条件特别多，总是有很多情况考虑不到，整了一晚上也没整出来。后面睡觉的时候突然想起我为啥不从后往前找呢？🤣具体解法都在注释里，emmmm，可能还有语法错误，勿介😊。 Min Stack Design a stack that supports push, pop, top, and retrieving the minimum element in constant time. push(x) -- Push element x onto stack. pop() -- Removes the element on top of the stack. top() -- Get the top element. getMin() -- Retrieve the minimum element in the stack. Example: &gt; MinStack minStack = new MinStack(); minStack.push(-2); minStack.push(0); minStack.push(-3); minStack.getMin(); --&gt; Returns -3. minStack.pop(); minStack.top(); --&gt; Returns 0. minStack.getMin(); --&gt; Returns -2. Solution 这个题不难，并且有很多中实现方式。我们需要实现的这个栈是在传统的栈已有的功能上增加了返回最小值的功能，这就涉及到怎么存储最小值的问题，这里有三个思路，大家有更好的思路也可以在评论区给出： 时间换空间 只设计一个栈，返回最小值时通过排序方式返回。 class MinStack: def __init__(self): &quot;&quot;&quot; initialize your data structure here. &quot;&quot;&quot; self.stack = [] def push(self, x: int) -&gt; None: self.stack.append(x) def pop(self) -&gt; None: self.stack.pop() def top(self) -&gt; int: if len(self.stack) &lt; 0: return None return self.stack[len(self.stack)-1] def getMin(self) -&gt; int: if len(self.stack) &lt; 0: return None s_sorted = sorted(self.stack) return s_sorted[0] 更节省时间和空间的方式(更巧妙) 设计一个栈，一个min变量，但是栈里面只存放每个元素与最小值的差值（x-min），当出现更小的最小值时，保存的是负数，故课通过这个条件追溯之前的最小值，具体实现见下面的代码。 class MinStack: def __init__(self): &quot;&quot;&quot; initialize your data structure here. &quot;&quot;&quot; self.stack = [] self.s_min = None def push(self, x: int) -&gt; None: if self.s_min == None: self.s_min = x self.stack.append(x-self.s_min) self.s_min = min(self.s_min, x) def pop(self) -&gt; None: pop = self.stack.pop() if len(self.stack) &gt; 0: if pop &lt; 0: self.s_min = self.s_min - pop else: self.s_min = None def top(self) -&gt; int: if len(self.stack) &lt; 0: return None top = self.stack[len(self.stack)-1] if top &lt; 0: return self.s_min return top + self.s_min def getMin(self) -&gt; int: return self.s_min 空间换时间 设计两个栈，多出来的那个栈用来记录出现过的最小值，我这里没有去实现了，大家可以试试。 Diameter of Binary Tree Given a binary tree, you need to compute the length of the diameter of the tree. The diameter of a binary tree is the length of the longest path between any two nodes in a tree. This path may or may not pass through the root. Example: Given a binary tree Return 3, which is the length of the path [4,2,1,3] or [5,2,1,3]. Note: The length of path between two nodes is represented by the number of edges between them. Solution 核心思路：根据半径的定义可知，半径所在的路径一定有一个中心节点，故可通过判断二叉树每个节点（当前可能的中心点）左右子树的深度和的大小来确定半径。 # Definition for a binary tree node. class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = None class Solution: def diameterOfBinaryTree(self, root: TreeNode) -&gt; int: self.diameter = 0 def count_depth(node): if not(node): return 0 l_dep, r_dep = count_depth(node.left), count_depth(node.right) self.diameter = max(self.diameter, l_dep+r_dep) # max depth: max(left depth, right depth) # +1: count node return 1 + max(l_dep, r_dep) count_depth(root) return self.diameter 这题刚开始我是没有什么好的思路的，太久没做关于二叉树的题了，不对，准确来讲应该是课后就丢了，哈哈哈，后来去查了下二叉树的相关资料，看到求二叉树的深度问题给了我灵感。希望能把以前丢掉的知识点慢慢捡起来，就拿这个树相关的知识点来说，其实很多算法底层原理都有涉及，为了以后能走快一点，现在应该多走几步。 Updating... 未完待续...","categories":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Coding","slug":"Technology/Coding","permalink":"https://littlelittlemoon.github.io/categories/Technology/Coding/"},{"name":"Leetcode","slug":"Technology/Coding/Leetcode","permalink":"https://littlelittlemoon.github.io/categories/Technology/Coding/Leetcode/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://littlelittlemoon.github.io/tags/Leetcode/"},{"name":"Python","slug":"Python","permalink":"https://littlelittlemoon.github.io/tags/Python/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://littlelittlemoon.github.io/tags/Algorithm/"}],"keywords":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Coding","slug":"Technology/Coding","permalink":"https://littlelittlemoon.github.io/categories/Technology/Coding/"},{"name":"Leetcode","slug":"Technology/Coding/Leetcode","permalink":"https://littlelittlemoon.github.io/categories/Technology/Coding/Leetcode/"}]},{"title":"Leetcode Note in Apri 2020 | part 1 | day 01-07","slug":"Leetcode Note","date":"2020-04-01T01:51:55.000Z","updated":"2020-04-11T16:16:26.754Z","comments":true,"path":"2020/04/01/Leetcode Note/","link":"","permalink":"https://littlelittlemoon.github.io/2020/04/01/Leetcode%20Note/","excerpt":"","text":"写在前面的话 这一系列文章主要是用于记录我在LeetCode上刷题时遇到的我认为比较有难度和有意思的题，以及是挖掘到的其他小伙伴比较好的解法和思路。这是第一篇，不出意外这个系列会一直更新，也算是督促自己坚持下去吧。对了我写作水平真的很差，希望大家不要嫌弃，也可以和我留言给出建议什么的，期待和大家个共同进步！那我们开始吧 ~ Sigle number Given a non-empty array of integers, every element appears twice except for one. Find that single one. Note: Your algorithm should have a linear runtime complexity. Could you implement it without using extra memory? Example: &gt; Input: [4,1,2,1,2] Output: 4 Solution Mine: tuple + list class Solution: def singleNumber(self, nums: List[int]) -&gt; int: # store the count of positive number p_count = [0] * (max(tuple(nums)) + 1) # store the positive number p_nums = () # store the count of negative number n_count = [0] * (abs(min(tuple(nums))) + 1) # store the negative number n_nums = () # counting for num in nums: if num &gt;= 0: p_nums = p_nums + (num, ) p_count[num] = p_count[num] + 1 else: n_nums = n_nums + (num, ) n_count[abs(num)] = n_count[abs(num)] + 1 # find the sigle number for num in p_nums: if p_count[num] == 1: return num for num in n_nums: if n_count[abs(num)] == 1: return num return None Space complexity： O(n) Time complexity： O(n) Optimization using XOR The most crucial trick here is to recognize that if you XOR any same number together, you cancel it out (=0). Explanation: nums = [2, 4, 5, 4, 3, 5, 2] XORing everything together = 2 ^ 4 ^ 5 ^ 4 ^ 3 ^ 5 ^ 2 = (2^2) ^ (4^4) ^ (5^5) ^ 3 = 0 ^ 0 ^0 ^ 3 = 3 class Solution: def singleNumber(self, nums: List[int]) -&gt; int: return reduce(lambda x, y: x^y, nums, 0) Space complexity： O(1) time complexity： O(n) Reference Reduce list(map(str, [1, 2, 3, 4, 5, 6, 7, 8, 9])) # output: [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;] Map reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4) # convert string to integer DIGITS = {&#39;0&#39;: 0, &#39;1&#39;: 1, &#39;2&#39;: 2, &#39;3&#39;: 3, &#39;4&#39;: 4, &#39;5&#39;: 5, &#39;6&#39;: 6, &#39;7&#39;: 7, &#39;8&#39;: 8, &#39;9&#39;: 9} def char2num(s): return DIGITS[s] def str2int(s): return reduce(lambda x, y: x * 10 + y, map(char2num, s)) Happy number A happy number is a number defined by the following process: Starting with any positive integer, replace the number by the sum of the squares of its digits, and repeat the process until the number equals 1 (where it will stay), or it loops endlessly in a cycle which does not include 1. Those numbers for which this process ends in 1 are happy numbers. Example: &gt; Input: 19 Output: true Explanation: \\(1^2 + 9^2 = 82\\) \\(8^2 + 2^2 = 68\\) \\(6^2 + 8^2 = 100\\) \\(1^2 + 0^2 + 0^2 = 1\\) Solution Floyd's cycle detection 核心思路：如果一个数字不是 happy number 那么它拆分后的数字的和一定会循环，通过Floyd's cycle detection算法做循环检测。 Short Version utilizing walrus operator := class Solution: def isHappy(self, n: int) -&gt; bool: def next_num(num): return sum(map(lambda x:int(x)**2, str(num))) slow, fast = n, next_num(n) while (slow:=next_num(slow)) != (fast:=next_num(next_num(fast))) and fast != 1: continue return fast == 1 or not slow == fast Easier to understand version class Solution: def isHappy(self, n: int) -&gt; bool: def next_num(num): return sum(map(lambda x:int(x)**2, str(num))) slow, fast = n, next_num(n) while slow != fast and fast != 1: slow = next_num(slow) fast = next_num(next_num(fast)) return fast == 1 or not slow == fast Reference Algorithm： Floyd's cycle detection What we need to do in case we need the starting point of the loop ? - Once we know for sure that a loop is present. - Move the slowPointer to start of the list,(i.e headNode) and let fastPointer remain there at the meeting point - Now move both the pointers one node at a time - The point where both pointers will meet, is our required start of the loop. The algorithm uses O(λ + μ) operations of these types, and O(1) storage space. Detecting start of a loop in singly Linked List Floyd's Cycle detection algorithm | Determining the starting point of cycle Another solution class Solution: def isHappy(self, n: int) -&gt; bool: # let&#39;s try different n: # true (1) -&gt; 1 # false (2) -&gt; 4 -&gt; 16 -&gt; 37 -&gt; 58 -&gt; 89 -&gt; 145 -&gt; 42 -&gt; 20 -&gt; 4 # false (3) -&gt; 9 -&gt; 81 -&gt; 65 -&gt; 61 -&gt; 37 (look at 2) # false (4) -&gt; (look at 2) # false (5) -&gt; 25 -&gt; 29 -&gt; 85 -&gt; 89 (look at 2) # false (6) -&gt; 36 -&gt; 45 -&gt; 41 -&gt; 17 -&gt; 50 -&gt; 25 (look at 5) # true (7) -&gt; 49 -&gt; 97 -&gt; 10 # false (8) -&gt; 64 -&gt; 52 -&gt; 29 (look at 5) # false (9) -&gt; 9 -&gt; 81 -&gt; 65 (look at 3) # # All other n &gt;= 10, while computing will become [1-9], # So there are two cases 1 and 7 which are true. # # Notice, that all falses has the same path as 2 (loop). counting = 0 num = n while True: counting = 0 for str_num in str(num): counting = counting + pow(int(str_num), 2) if counting &gt;= 1 and counting &lt;=9: if counting == 1 or counting == 7: return True else: return False else: num = counting Group Anagrams Given an array of strings, group anagrams together. Example: &gt; Input: [\"eat\", \"tea\", \"tan\", \"ate\", \"nat\", \"bat\"], Output: [ [\"ate\",\"eat\",\"tea\"], [\"nat\",\"tan\"], [\"bat\"] ] My solution（bad） class Solution: def groupAnagrams(self, strs: List[str]) -&gt; List[List[str]]: group_list = [] group_list.append([strs[0]]) for i in range(1, len(strs)): sorted_str = sorted(strs[i]) for j in range(0, len(group_list)): sorted_comp_str = sorted(group_list[j][0]) if sorted_str == sorted_comp_str: group_list[j].append(strs[i]) break if j == len(group_list) - 1: group_list.append([strs[i]]) break return group_list Result: Time Limit Exceeded Summary 这个解法看起来没什么毛病，如果strs长度很小，也能正常工作，但是当字符串数组strs长度特别大时， 毛病就暴露出来了，速度很慢，效率低，会出现超时（Time Limit Exceeded）未完成的情况。 Nice solution with dictionary and tuple class Solution: def groupAnagrams(self, strs: List[str]) -&gt; List[List[str]]: # create a null dictionary group_list = {} for s in strs: # sort the string&#39;s letters and save as a tuple key = tuple(sorted(s)) # search dictionary with the str&#39;s tuple and save the new value group_list[key] = group_list.get(key, []) + [s] return group_list.values() Summary Tuple can be used for the dictionary's key. wow~ Dictionary can speed up the search. Maximum Subarray Given an integer array nums, find the contiguous subarray (containing at least one number) which has the largest sum and return its sum. Example: &gt; Input: [-2,1,-3,4,-1,2,1,-5,4], Output: 6 Explanation: [4,-1,2,1] has the largest sum = 6. Follow up: If you have figured out the O(n) solution, try coding another solution using the divide and conquer approach, which is more subtle. Solution 核心思路： 当sum&lt;0时，sum+x一定比x小，故当sum为负数时，可直接将sum置零。 class Solution: def maxSubArray(self, nums: List[int]) -&gt; int: sum_ = 0 max_sum = nums[0] for num in nums: sum_ = sum_ + num if sum_ &gt; max_sum: max_sum = sum_ sum_ = max(sum_, 0) return max_sum Best Time to Buy and Sell Stock II Say you have an array prices for which the ith element is the price of a given stock on day i. Design an algorithm to find the maximum profit. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times). Note: You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again). Example 1: &gt; Input: [7,1,5,3,6,4] Output: 7 Explanation: Buy on day 2 (price = 1) and sell on day 3 (price = 5), profit = 5-1 = 4. Then buy on day 4 (price = 3) and sell on day 5 (price = 6), profit = 6-3 = 3. Solution 哇这个题，真的让我怀疑自己的智商了...😂，我刚开始的答案写了好多行代码，然后去讨论区学习的时候发现大家都是人才啊...比如下面这个： class Solution: def maxProfit(self, prices: List[int]) -&gt; int: max_profit = 0 for i in range(0, len(prices)-1): max_profit += max(prices[i+1]-prices[i], 0) return max_profit Mine class Solution: def maxProfit(self, prices: List[int]) -&gt; int: max_profit = 0 buy_i = 0 sell_i = 1 need_sell = 1 need_buy = 0 for i in range(1, len(prices)): if prices[sell_i] &lt; prices[buy_i]: need_buy = 1 need_sell = 0 else: need_buy = 0 need_sell = 1 if need_buy == 1: if prices[i] &lt; prices[buy_i]: buy_i = i if i &lt; len(prices) - 1: sell_i = i + 1 else: sell_i = i continue if need_sell == 1: if prices[i] &gt;= prices[sell_i]: sell_i = i if i == len(prices) - 1: max_profit += prices[sell_i] - prices[buy_i] else: max_profit += prices[sell_i] - prices[buy_i] buy_i = i if i &lt; len(prices) - 1: sell_i = i + 1 need_buy = 1 need_sell = 0 return max_profit emmmm...我需要反思一下😞 Counting Elements Given an integer array arr, count element x such that x + 1 is also in arr. If there're duplicates in arr, count them seperately. Examples: &gt; Input: arr = [1,2,3] Output: 2 Explanation: 1 and 2 are counted cause 2 and 3 are in arr. &gt; Input: arr = [1,1,3,3,5,5,7,7] Output: 0 Explanation: No numbers are counted, cause there's no 2, 4, 6, or 8 in arr. &gt; Input: arr = [1,3,2,3,5,0] Output: 3 Explanation: 0, 1 and 2 are counted cause 1, 2 and 3 are in arr. Solution Mine 这个题没有找到大家的讨论，所以我也不知道其他小伙伴的解题思路是什么样的，但是我总觉得大家会有更好的解决方案，我这边就先po出我的解法，欢迎大家提供自己的思路和建议。 思路： 现将数组排序，然后通过两个指针：pre和cur进行对比，需要注意的是数字相同的数应该同时被计数或都不计数。 class Solution: def countElements(self, arr: List[int]) -&gt; int: arr_sorted = sorted(arr) count = 0 # 指向当前需要判断是否能被计数的数 pre = 0 # 指向当前pre后面的数，用于判断pre是否能被计数 cur = 1 # 如果cur指向的数等于pre指向的数，则增加leave， # 如果pre可以被计数，则和pre相等的数也应该被计数 leave = 1 # 当pre和cur指向同一index时停止循环 while pre^cur: sub = arr_sorted[cur] - arr_sorted[pre] if sub &lt;= 1: if sub == 1: count += leave pre = cur leave = 1 elif cur == len(arr)-1: break; else: leave +=1 else: pre = cur leave = 1 cur = min(cur+1, len(arr)-1) return count Summary 我发现我的写作能力真的好差啊，有的时候知道怎么回事就是写不出来，捉急。。。希望看到的小伙伴不要介意，我在努力，希望以后会越来越好，唉😌，对不起高中语文老师一对一的辅导啊。好啦，这期就更新完了。 对了，我是准备一次更新一周的内容，然后可能想起来就会更新一部分发布出来，如果没更完最后会显示Updating...。一起期待第二期吧~😊","categories":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Coding","slug":"Technology/Coding","permalink":"https://littlelittlemoon.github.io/categories/Technology/Coding/"},{"name":"Leetcode","slug":"Technology/Coding/Leetcode","permalink":"https://littlelittlemoon.github.io/categories/Technology/Coding/Leetcode/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://littlelittlemoon.github.io/tags/Leetcode/"},{"name":"Python","slug":"Python","permalink":"https://littlelittlemoon.github.io/tags/Python/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://littlelittlemoon.github.io/tags/Algorithm/"}],"keywords":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Coding","slug":"Technology/Coding","permalink":"https://littlelittlemoon.github.io/categories/Technology/Coding/"},{"name":"Leetcode","slug":"Technology/Coding/Leetcode","permalink":"https://littlelittlemoon.github.io/categories/Technology/Coding/Leetcode/"}]},{"title":"Fraud Detection | Imbalanced data modeling","slug":"Credit Card Fraud Detection","date":"2020-03-21T13:53:06.000Z","updated":"2020-04-09T07:01:31.447Z","comments":true,"path":"2020/03/21/Credit Card Fraud Detection/","link":"","permalink":"https://littlelittlemoon.github.io/2020/03/21/Credit%20Card%20Fraud%20Detection/","excerpt":"","text":"LDA, QDA and LR for fraud detection | Imbalanced data modeling %matplotlib inline # import warnings filter from warnings import simplefilter # ignore all future warnings simplefilter(action=&#39;ignore&#39;, category=FutureWarning) prepare data import pandas as pd # load data default_data = pd.read_csv(&quot;data/Default.csv&quot;) # prepare data default_data.loc[default_data[&#39;default&#39;] == &#39;No&#39;, &quot;default&quot;] = 0 default_data.loc[default_data[&#39;default&#39;] == &#39;Yes&#39;, &quot;default&quot;] = 1 default_data.loc[default_data[&#39;student&#39;] == &#39;No&#39;, &quot;student&quot;] = 0 default_data.loc[default_data[&#39;student&#39;] == &#39;Yes&#39;, &quot;student&quot;] = 1 default_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 default student balance income count 10000.00000 10000.000000 10000.000000 10000.000000 10000.000000 mean 5000.50000 0.033300 0.294400 835.374886 33516.981876 std 2886.89568 0.179428 0.455795 483.714985 13336.639563 min 1.00000 0.000000 0.000000 0.000000 771.967729 25% 2500.75000 0.000000 0.000000 481.731105 21340.462903 50% 5000.50000 0.000000 0.000000 823.636973 34552.644802 75% 7500.25000 0.000000 1.000000 1166.308386 43807.729272 max 10000.00000 1.000000 1.000000 2654.322576 73554.233495 split training and testing set from sklearn.model_selection import train_test_split # create features and target features = [&quot;balance&quot;, &quot;income&quot;] X = default_data[features] y = default_data.default # slipt data set into training and testing set train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.7, random_state=1) import numpy as np import matplotlib as mpl from scipy import linalg from matplotlib import colors import matplotlib.pyplot as plt from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.linear_model import LogisticRegression Plot function # set colormap cmap = colors.LinearSegmentedColormap( &#39;red_blue_classes&#39;, {&#39;red&#39;: [(0, 1, 1), (1, 0.7, 0.7)], &#39;green&#39;: [(0, 0.7, 0.7), (1, 0.7, 0.7)], &#39;blue&#39;: [(0, 0.7, 0.7), (1, 1, 1)]}) plt.cm.register_cmap(cmap=cmap) # Plot function def plot_data(model, X, y, y_pred): plt.ylabel(&#39;income&#39;) plt.xlabel(&#39;balance&#39;) tp = (y == y_pred) # True Positive tp0, tp1 = tp[y == 0], tp[y == 1] X0, X1 = X[y == 0], X[y == 1] X0_tp, X0_fp = X0[tp0], X0[~tp0] X1_tp, X1_fp = X1[tp1], X1[~tp1] # true class 0: dots, false class 0: x plt.scatter(X0_tp[&quot;balance&quot;], X0_tp[&quot;income&quot;], marker=&#39;.&#39;, color=&#39;red&#39;) plt.scatter(X0_fp[&quot;balance&quot;], X0_fp[&quot;income&quot;], marker=&#39;x&#39;, s=20, color=&#39;#990000&#39;) # dark red # true class 1: dots, false class 1: x plt.scatter(X1_tp[&quot;balance&quot;], X1_tp[&quot;income&quot;], marker=&#39;.&#39;, color=&#39;blue&#39;) plt.scatter(X1_fp[&quot;balance&quot;], X1_fp[&quot;income&quot;], marker=&#39;x&#39;, s=20, color=&#39;#000099&#39;) # dark blue # class 0 and 1 : all areas for decision boundary nx, ny = 200, 100 x_min, x_max = plt.xlim() y_min, y_max = plt.ylim() xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx), np.linspace(y_min, y_max, ny)) Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()]) Z = Z[:, 1].reshape(xx.shape) plt.pcolormesh(xx, yy, Z, cmap=&#39;red_blue_classes&#39;, norm=colors.Normalize(0., 1.), zorder=0) # plot decision boundary plt.contour(xx, yy, Z, [0.5], linewidths=2., colors=&#39;white&#39;) plt.axis(&#39;tight&#39;) plt.tight_layout() plt.subplots_adjust(top=0.92) plt.show() Linear Discriminant Analysis # Linear Discriminant Analysis plt.figure(figsize=(6, 7), facecolor=&#39;white&#39;) plt.title(&#39;Linear Discriminant Analysis&#39;, y=1, fontsize=15) lda = LinearDiscriminantAnalysis(solver=&quot;svd&quot;, store_covariance=True) y_pred = lda.fit(train_X, train_y).predict(test_X) plot_data(lda, test_X, test_y, y_pred) png Quadratic Discriminant Analysis # Quadratic Discriminant Analysis plt.figure(figsize=(6, 7), facecolor=&#39;white&#39;) plt.title(&#39;Quadratic Discriminant Analysis&#39;, y=1, fontsize=15) qda = QuadraticDiscriminantAnalysis(store_covariance=True) y_pred = qda.fit(train_X, train_y).predict(test_X) plot_data(qda, test_X, test_y, y_pred) png Use LogisticRegression directly to model the data If I use this data directly to feed the LogisticRegression model, the model will prefer to predict all as 0 for a high accuracy of 0 prediction. print(default_data.default.value_counts(dropna = False)) print(&quot;The mean of default: &quot;, default_data.default.mean()) 0 9667 1 333 Name: default, dtype: int64 The mean of default: 0.0333 NOTE: As it showing above: the provided data with very low proportion of positive signals. Conclusion: The provided data is imbalanced ! Solution: usually for imbalanced data, there are some solutions: 1. Collect more data 2. Down-Sampling or Over-Sampling to get balanced samples 3. Change the Thresholds to adjust the prediction 4. Assign class weights for the low rate class from sklearn.metrics import confusion_matrix, auc, roc_curve, roc_auc_score, recall_score, precision_recall_curve from sklearn.metrics import make_scorer, precision_score from sklearn.model_selection import GridSearchCV # Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = .33, stratify = y) logitreg_parameters = {&#39;C&#39;: np.power(10.0, np.arange(-3, 3))} logitreg = LogisticRegression(verbose = 3, warm_start = True) logitreg_grid = GridSearchCV(logitreg, param_grid = logitreg_parameters, scoring = &#39;roc_auc&#39;, n_jobs = 1) logitreg_grid.fit(train_X, train_y) GridSearchCV(cv=None, error_score=nan, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=3, warm_start=True), iid=&#39;deprecated&#39;, n_jobs=1, param_grid={&#39;C&#39;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=&#39;roc_auc&#39;, verbose=0) # draw decision boundary with LogisticRegression directly plt.figure(figsize=(6, 7), facecolor=&#39;white&#39;) plt.title(&#39;Logistic Regression directly&#39;, y=1, fontsize=15) y_pred = logitreg_grid.predict(test_X) splot = plot_data(logitreg_grid, test_X, test_y, y_pred) png # on OVER-Sampled TRAINing data print(&quot;\\n The recall score on Training data is:&quot;, recall_score(train_y, logitreg_grid.predict(train_X))) # 0.32 print(&quot;\\n The precision score on Training data is:&quot;, precision_score(train_y, logitreg_grid.predict(train_X))) # 0.74 # on the separated TEST data print(&quot;\\n Thre recall score on Test data is:&quot;, recall_score(test_y, logitreg_grid.predict(test_X))) # 0.32 print(&quot;\\n Thre precision score on Test data is:&quot;, precision_score(test_y, logitreg_grid.predict(test_X))) # 0.75 print(&quot;\\n Thre Confusion Matrix on Test data is:&quot;, confusion_matrix(test_y, logitreg_grid.predict(test_X))) # [[3178 12][ 74 36]] The recall score on Training data is: 0.32231404958677684 The precision score on Training data is: 0.7222222222222222 Thre recall score on Test data is: 0.3626373626373626 Thre precision score on Test data is: 0.673469387755102 Thre Confusion Matrix on Test data is: [[2893 16] [ 58 33]] Conclusions: From the output above, on the training data, the recall score is 0.32 which means 32 over 100 of the True positive conditions are predicted correctly. And 74 over 100 of the predicted positives are True Positive. On the Test data, the model performance metric evalued by recall or precision are close to the Training data. There is a precision score of 0.81 on the Test data, which means 81 out of 100 predicted positives are True positives. From Confusion Matrix, 36 of 110 True Positives are predicted as positives. And of all 48 predicted as positive, 36 of them are True positives. Change the Thresholds plot roc curve def plot_roc(new_thresholds, logitreg_grid): y_train_pred_probas = logitreg_grid.predict_proba(train_X)[:, 1] # prob of predict as 1 fpr, tpr, thresholds = roc_curve(train_y, y_train_pred_probas) # precision_recall_curve roc = pd.DataFrame({&#39;FPR&#39;:fpr, &#39;TPR&#39;:tpr, &#39;Thresholds&#39;:thresholds}) plt.figure() plt.title(&#39;ROC Curve&#39;, y = 1, fontsize = 15) plt.plot(roc.FPR, roc.TPR) plt.axvline(new_thresholds, color = &#39;#00C851&#39;, linestyle = &#39;--&#39;) plt.xlabel(&quot;FPR&quot;) plt.ylabel(&quot;TPR&quot;) plt.show() new_threshold = 0.1 # 0.5 is the default value plot_roc(new_threshold, logitreg_grid) png By default, the threshold is 0.5. Since the recall score is low, I'm trying to lower the threshold to get more predicted as Positive. At the same time, more True Negative data will be falsely predicted as Positive. So the Precision score will be lower. y_test_pred_probas = logitreg_grid.predict_proba(test_X)[:, 1] y_test_pred = (y_test_pred_probas &gt;= new_threshold).astype(int) print(&quot;After change threshold to 0.1, the recall socre on Test data is:&quot;) print(recall_score(test_y, y_test_pred)) # 0.736 print(&quot;After change threshold to 0.1, the precision socre on Test data is:&quot;) print(precision_score(test_y, y_test_pred)) # 0.301 print(&quot;After change threshold to 0.1, the Confusion Matrix on Test data is:&quot;) print(confusion_matrix(test_y, y_test_pred)) # [[3002 188][ 29 81]] After change threshold to 0.1, the recall socre on Test data is: 0.7142857142857143 After change threshold to 0.1, the precision socre on Test data is: 0.25 After change threshold to 0.1, the Confusion Matrix on Test data is: [[2714 195] [ 26 65]] Create Over-sampling data and Fit the model oversample_ratio = sum(train_y == 0) / sum(train_y == 1) # size to repeat y == 1 # repeat the positive data for X and y y_train_pos_oversample = pd.concat([train_y[train_y==1]] * int(oversample_ratio), axis = 0) X_train_pos_oversample = pd.concat([train_X.loc[train_y==1, :]] * int(oversample_ratio), axis = 0) # concat the repeated data with the original data together y_train_oversample = pd.concat([train_y, y_train_pos_oversample], axis = 0).reset_index(drop = True) X_train_oversample = pd.concat([train_X, X_train_pos_oversample], axis = 0).reset_index(drop = True) print(y_train_oversample.value_counts(dropna = False, normalize = True)) logitreg_parameters = {&#39;C&#39;: np.power(10.0, np.arange(-3, 3))} logitreg = LogisticRegression(verbose = 3, warm_start = True) logitreg_grid = GridSearchCV(logitreg, param_grid = logitreg_parameters, scoring = &#39;roc_auc&#39;, n_jobs = 1) logitreg_grid.fit(X_train_oversample, y_train_oversample) 1 0.500665 0 0.499335 Name: default, dtype: float64 GridSearchCV(cv=None, error_score=nan, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=3, warm_start=True), iid=&#39;deprecated&#39;, n_jobs=1, param_grid={&#39;C&#39;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=&#39;roc_auc&#39;, verbose=0) # Logistic Regression with Over-sampling plt.figure(figsize=(6, 7), facecolor=&#39;white&#39;) plt.title(&#39;Logistic Regression with Over-sampling&#39;, y=1, fontsize=15) y_pred = logitreg_grid.predict(test_X) plot_data(logitreg_grid, test_X, test_y, y_pred) png # on OVER-Sampled TRAINing data print(&quot;After Over-Sampling, the recall score on Training data is&quot;) print(recall_score(y_train_oversample, logitreg_grid.predict(X_train_oversample))) # 0.865 print(&quot;After Over-Sampling, the precision score on Training data is&quot;) print(precision_score(y_train_oversample, logitreg_grid.predict(X_train_oversample))) # 0.727 # on the TESTing data print(&quot;After Over-Sampling, the recall score on Test data is&quot;) print(recall_score(test_y, logitreg_grid.predict(test_X))) # 0.854 print(&quot;After Over-Sampling, the precision score on Test data is&quot;) print(precision_score(test_y, logitreg_grid.predict(test_X))) # 0.080 print(&quot;After Over-Sampling, the Confusion Matrix on Test data is&quot;) print(confusion_matrix(test_y, logitreg_grid.predict(test_X))) # [[2113 1077][ 16 94]] After Over-Sampling, the recall score on Training data is 0.8884297520661157 After Over-Sampling, the precision score on Training data is 0.8717057631045467 After Over-Sampling, the recall score on Test data is 0.8791208791208791 After Over-Sampling, the precision score on Test data is 0.17094017094017094 After Over-Sampling, the Confusion Matrix on Test data is [[2521 388] [ 11 80]] Conclusion: From the output above, on the training data, the recall score is 0.865 which means 86.5 over 100 of the True conditions are predicted correctly. And 85.4 over 100 of the predicted positives are really positive. However, there is only a precision score of 0.080 on the Test data, which means only 8 out of 100 predicted positives are real positives. From Confusion Matrix, 94 of 110 True Positives are predicted as positives. However, the model predicted 1077 Negative data as Positive. That is, this model has pretty strong over-fitting. Change the Thresholds new_threshold = 0.2 plot_roc(new_threshold, logitreg_grid) png y_test_pred_probas = logitreg_grid.predict_proba(test_X)[:, 1] y_test_pred = (y_test_pred_probas &gt;= new_threshold).astype(int) print(&quot;After change threshold to 0.2, the recall socre on Test data is:&quot;) print(recall_score(test_y, y_test_pred)) # 0.990 print(&quot;After change threshold to 0.2, the precision socre on Test data is:&quot;) print(precision_score(test_y, y_test_pred)) # 0.047 print(&quot;After change threshold to 0.2, the Confusion Matrix on Test data is:&quot;) print(confusion_matrix(test_y, y_test_pred)) # [[ 1013 2177][ 1 109]] After change threshold to 0.2, the recall socre on Test data is: 0.9340659340659341 After change threshold to 0.2, the precision socre on Test data is: 0.10023584905660378 After change threshold to 0.2, the Confusion Matrix on Test data is: [[2146 763] [ 6 85]] Conclusion: After over-sampling, the model will have higher recall rate. That is, the model will work better on detect the Frauds from True Frauds. The price is the lower precision rate. Logistic Regression with class_weight Rather than over-sampling, we can assign more weights to the lower rate class. we can write out the Likelihood function for Logistic Regression, the Over-Sampling and the assigning more Weights will be equivalent. positive_weight = sum(train_y == 0) / sum(train_y == 1) # size to repeat y == 1 logitreg_parameters = {&#39;C&#39;: np.power(10.0, np.arange(-3, 3))} logitreg = LogisticRegression(class_weight = {0 : 1, 1 : positive_weight}, verbose = 3, warm_start = True) logitreg_grid = GridSearchCV(logitreg, param_grid = logitreg_parameters, scoring = &#39;roc_auc&#39;, n_jobs = 1) logitreg_grid.fit(train_X, train_y) GridSearchCV(cv=None, error_score=nan, estimator=LogisticRegression(C=1.0, class_weight={0: 1, 1: 27.925619834710744}, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=3, warm_start=True), iid=&#39;deprecated&#39;, n_jobs=1, param_grid={&#39;C&#39;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=&#39;roc_auc&#39;, verbose=0) # Logistic Regression with class_weight plt.figure(figsize=(6, 7), facecolor=&#39;white&#39;) plt.title(&#39;Logistic Regression&#39;, y=1, fontsize=15) y_pred = logitreg_grid.predict(test_X) plot_data(logitreg_grid, test_X, test_y, y_pred) png print(&quot;After assign class_weight, the recall score on Training data is&quot;) print(recall_score(y_train_oversample, logitreg_grid.predict(X_train_oversample))) # 0.856 print(&quot;After assign class_weight, the precision score on Training data is&quot;) print(precision_score(y_train_oversample, logitreg_grid.predict(X_train_oversample))) # 0.729 # on the separated TEST data print(&quot;After assign class_weight, the recall score on Test data is&quot;) print(recall_score(test_y, logitreg_grid.predict(test_X))) # 0.845 print(&quot;After assign class_weight, the precision score on Test data is&quot;) print(precision_score(test_y, logitreg_grid.predict(test_X))) # 0.081 print(&quot;After assign class_weight, the Confusion Matrix on Test data is&quot;) print(confusion_matrix(test_y, logitreg_grid.predict(test_X))) # [[2135 1055] [ 17 93]] print(&quot;After assign class_weight, the ROC AUC Score on Test data is&quot;) print(roc_auc_score(test_y, logitreg_grid.predict(test_X))) # 0.757 After assign class_weight, the recall score on Training data is 0.859504132231405 After assign class_weight, the precision score on Training data is 0.7171530599679843 After assign class_weight, the recall score on Test data is 0.8791208791208791 After assign class_weight, the precision score on Test data is 0.075046904315197 After assign class_weight, the Confusion Matrix on Test data is [[1923 986] [ 11 80]] After assign class_weight, the ROC AUC Score on Test data is 0.7700863934965 Conclusion: If I set up the class weight for the positive as the ratio of non-Fault / Fault, I will get the result close to the over-sampling. So, in summary: This specific data is about fraud detection. So the model should focus on to find the frauds to avoid potential loss for the bank. That is, we focus on recall rate. Conclusion If we use the imbalanced data directly, we will get low performance model since the model prefer to predict to the class with dominated frequency class. The recall rate is 0.31. That is, only 31% of the frauds can be detected by this model. To fix that, one way is to do over-sampling or down-sampling. If we use over-sampling, the model performance will be improved a lot. For this specific case, the recall rate on the independent test set will be improved from 0.31 to 0.87 Another way to improve the model performance is to assign more weights to the low frequency class. Generally speaking, for Logistic Regression, assigning weights is similar to over-sampling, from the likelihood function perspective. The final output results are close too as demonstrated above. Reference Credit Card Fraud Detection / Imbalanced data modeling - Part I: Logistic Regression Credit Fraud || Dealing with Imbalanced Datasets","categories":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Machine Learning","slug":"Technology/Machine-Learning","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/"},{"name":"Imbalanced Data Modeling","slug":"Technology/Machine-Learning/Imbalanced-Data-Modeling","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/Imbalanced-Data-Modeling/"}],"tags":[{"name":"LDA","slug":"LDA","permalink":"https://littlelittlemoon.github.io/tags/LDA/"},{"name":"QDA","slug":"QDA","permalink":"https://littlelittlemoon.github.io/tags/QDA/"},{"name":"LR","slug":"LR","permalink":"https://littlelittlemoon.github.io/tags/LR/"},{"name":"Imbalanced Data Modeling","slug":"Imbalanced-Data-Modeling","permalink":"https://littlelittlemoon.github.io/tags/Imbalanced-Data-Modeling/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://littlelittlemoon.github.io/tags/Machine-Learning/"},{"name":"Classify","slug":"Classify","permalink":"https://littlelittlemoon.github.io/tags/Classify/"}],"keywords":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Machine Learning","slug":"Technology/Machine-Learning","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/"},{"name":"Imbalanced Data Modeling","slug":"Technology/Machine-Learning/Imbalanced-Data-Modeling","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/Imbalanced-Data-Modeling/"}]},{"title":"Evalution of hair and scalp condition based on microscopy image analysis | Hair Thickness - part 1","slug":"Evalution of hair and scalp condition based on microscopy image analysis  | Hair Thickness - 1","date":"2019-12-29T13:21:24.000Z","updated":"2020-04-10T06:09:34.142Z","comments":true,"path":"2019/12/29/Evalution of hair and scalp condition based on microscopy image analysis  | Hair Thickness - 1/","link":"","permalink":"https://littlelittlemoon.github.io/2019/12/29/Evalution%20of%20hair%20and%20scalp%20condition%20based%20on%20microscopy%20image%20analysis%20%20|%20Hair%20Thickness%20-%201/","excerpt":"","text":"Paper: Evalution of hair and scalp condition based on microscopy image analysis 摘要翻译：由于IT技术的快速部署，医疗保健服务进入了一个新时代。诸如心脏监护之类的某些服务对于生命至关重要，并有助于挽救生命。另一方面，监测脱发是另一种有趣的保健服务。尽管这对生活并不重要，但人们还是会非常注意自己的头发状况。脱发是与头发状况有关的主要问题之一，因为过多和无意的脱发可能导致秃头。可以在护发店专业进行护发，但是这需要很多时间和成本。最近，由于廉价的智能设备，对头发状况的自我诊断已成为可能。仍然很少开发用于评估头发状况的应用。在本文中，我们提出了一种新方案，通过从显微镜图像中提取各种特征来评估头发和头皮的状况。其特征包括头发的厚度，头发的密度和头皮的斑点。通过对原型系统进行广泛的实验，我们证明了该方案的有效性。 为了分析头皮图像，应该将头发和头皮彼此分开。两者之间最明显的区别是它们的颜色。头皮相对明亮，头发相对深色。因此，在许多研究中，根据颜色对头发和头皮区域进行分类，并根据这种分离进行图像分析。 ## Overall steps for feature extraction Pre-processing 该论文中所提到的图片预处理方法和“An Unsupervised Hair Segmentation and Counting System in Microscopy Images”中相似，具体可参考：论文解读 - An Unsupervised Hair Segmentation and Counting System in Microscopy Images 之头发计数问题。 1. 裁切图像 2. 图像增强: 使用Contrast stretching方法，增加图像的对比度 3. Morphological opening：去除油性和湿润的头发在其显微镜图像中形成的亮点 4. 二值化：经过上述预处理后，将所得图像转换为灰度图像，然后根据Otsu阈值转换为二进制图像。 在二值图像中，“ 0”和“ 1”分别表示头发像素和头皮像素。 hair/scalp image analysis 头发检测 技术 使用Canny边缘检测算法从二进制图像中获取毛发轮廓。 结果 - 图2显示了检测和建模头发的所有步骤。对于图2（a）中的原始显微镜图像，我们可以计算出头发轮廓和骨骼。 - 通过使用二进制图像上的稀疏运算（Thinning operation） 来计算头发骨架。Thinning是一种形态学运算，可去除整个二进制图像中的前景。 - 图2（d）显示了通过叠加头发轮廓和骨骼得到的最终图像。 头发厚度计算 头发厚度可以通过与头发垂直线的长度来定义。要获得垂直线，我们首先需要计算头发方向. 通过考虑相邻像素并应用PCA（主成分分析）算法来计算每个像素的方向. 当计算头发骨架上所有点的方向时，可以计算出每个点的垂直线。然后，垂直线与头发边界的交点之间的距离就是头发的厚度，可以通过使用欧氏距离来计算： \\[ \\rho = \\sqrt{\\smash[b]{(x_2-x_1)^2 + (y_2-y_1)^2}} \\] 根据欧式距离\\(\\rho\\)可计算出头发的平均厚度： \\[ Thinckness_{avg} =\\frac{1}{n}\\displaystyle\\sum_{i=1}^n\\sqrt{\\smash[b]{(x_{i2}-x_{i1})^2 + (y_{i2}-y_{i1})^2}} \\] 这里\\(Thinckness_{avg}\\)的单位是像素，因此需要使用等式将其更改为仪表单位: \\[ Thinckness_{actual}(um) = \\frac{Thinckness_{avg}(px) ×UL(um/px)}{mf} \\] \\(mf\\): 是相机的放大倍率 \\(UL\\): 是单位长度，表示一个像素的微米长度 实验结果 为了评估我们的头发厚度评估方法的准确性，我们使用电子显微镜测量了实际的头发厚度。头发厚度测量的准确性如表1所示。 table_1_AVERAGE_HAIR_THICKNESS_AND_ERROR_RATE 据报道，韩国人的平均头发厚度为84.9μm。与此相比，我们的90.29μm的结果相当不错。实际值（通过电子显微镜计算）与估计值之间的差异很小。 可能的原因之一是阴影效果。另一个可能的原因是相机镜头变形。 图像尺寸为640x480。但是，相机所覆盖区域的真实形状几乎是椭圆形。因此，相机放大率在行和列之间具有差异。根据拍摄角度，放大倍率可能会有所不同。 总结 针对头发粗细（厚度）计算问题，上述论文提出了基于头发轮廓和骨骼图，通过计算每个像素点的方向和垂线，进一步使用欧式距离公式来计算头发的垂直直径，但对相关技术的应用细节描述不多。 参考文献 Evalution of hair and scalp condition based on microscopy image analysis An Unsupervised Hair Segmentation and Counting System in Microscopy Images Euclidean distance Principal component analysis","categories":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Paper Smash","slug":"Technology/Paper-Smash","permalink":"https://littlelittlemoon.github.io/categories/Technology/Paper-Smash/"}],"tags":[{"name":"Digital Image Processing","slug":"Digital-Image-Processing","permalink":"https://littlelittlemoon.github.io/tags/Digital-Image-Processing/"},{"name":"Line Detection","slug":"Line-Detection","permalink":"https://littlelittlemoon.github.io/tags/Line-Detection/"},{"name":"Paper","slug":"Paper","permalink":"https://littlelittlemoon.github.io/tags/Paper/"},{"name":"Hair Thickness","slug":"Hair-Thickness","permalink":"https://littlelittlemoon.github.io/tags/Hair-Thickness/"}],"keywords":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Paper Smash","slug":"Technology/Paper-Smash","permalink":"https://littlelittlemoon.github.io/categories/Technology/Paper-Smash/"}]},{"title":"An Unsupervised Hair Segmentation and Counting System in Microscopy Images | Hair Counting - part 2","slug":"An Unsupervised Hair Segmentation and Counting System in Microscopy Images | Hair Counting - 2","date":"2019-12-28T15:51:27.000Z","updated":"2020-04-10T06:12:17.760Z","comments":true,"path":"2019/12/28/An Unsupervised Hair Segmentation and Counting System in Microscopy Images | Hair Counting - 2/","link":"","permalink":"https://littlelittlemoon.github.io/2019/12/28/An%20Unsupervised%20Hair%20Segmentation%20and%20Counting%20System%20in%20Microscopy%20Images%20|%20Hair%20Counting%20-%202/","excerpt":"","text":"Paper: An Unsupervised Hair Segmentation and Counting System in Microscopy Images 摘要翻译：本文重点介绍使用高级图像处理算法开发用于临床的医学软件。本文讨论了头发分割和计数的三个关键问题: - 首先，去除由于油脂或水分引起的任何亮点，这些亮点在头发的中部形成圆形图案，并显着影响确定线条的准确性。 - 第二，识别出两个接触或重叠的头发，并将其视为单个头发。为了解决这个问题，我们提出了一种头发捆绑算法(hair-bundling algorithm)来计算任何隐藏的头发。 - 最后，头发可能呈波浪状或卷曲状，这使传统的基于Hough的线检测算法不合适，因为它会受到参数选择的影响，例如线段的最小长度以及线段之间的距离。我们提出的毛发计数算法比基于Hough的毛发计数算法要准确得多，并且在各种白平衡下对卷发，油性头皮，噪声腐蚀和重叠的头发都具有适用性。 - 关键词：毛发计数，头皮诊断，护发诊断，毛囊诊断，线段检测。 System flowchart figure 1 System flowchart 预处理阶段 (Preprocessing Stage) 使用对比度拉伸方法(the contrast-stretching method)来增加头皮和头发像素之间的对比度; 为了减少亮点的影响，提出了一种健壮的颜色形态算法(morphological algorithm)，以使颜色平滑并保持头发的保真度; 为每个颜色分量应用了Karhunen-Loève变换(KLT)，并保留了具有最高能量的分量，并使用Otsu阈值获得了可靠的二进制图像。 数据采集规定 这项研究的唯一假设: 头发的颜色比皮肤的颜色深。 头发图像是从数码显微镜相机（DMC）捕获，内置LED增强，可自动保持亮度稳定。应用85倍的变焦倍率捕获图像。通常，使用分辨率为1024×768，相当于头皮面积是0.25×0.19英寸。此外，基于DMC的白平衡将捕获的图像分为两组： - 具有日光的图像被分类为数据集＃1， - 具有荧光的图像被分类为数据集＃2。 使用对比度拉伸(Contrast Stretching)进行图像增强 目的 增加头发和头皮之间的对比度; 增加头发和头皮像素之间的色差。 技术 通过分段线性对比度拉伸(color transformation by means of piecewise linear contrast stretching)进行颜色变换来增强图像，提高对比度； stretched the middle-intensity level, and kept the levels of the low-intensity and high-intensity so as to prevent creating false colors. 结果 figure_2_image_enhancement_for_the_datasets 进行对比度拉伸时，不会更改原始头发像素，也不会夸大油亮像素： - 降低了头皮像素的强度; - 增加了头皮和头发之间的色差; - 亮点的像素保持不变。 Bright Spot Removal (BSR) 目的 除噪: 去除油性和湿润的头发在头发的中部产生的亮点: 技术 color morphological processing approach 1. 非线性中值滤波器(nonlinear median filter)消除白点; 2. 空间平滑滤波器(spatial smooth filter)降低白点的强度, 缺点是测试图像的非毛发区域也将变得模糊; 3. color-based mathematical morphology (MM) method, used it as an ordering process. adopted the MM opening operator to depress the bright spot in the middle of the hairs. - 侵蚀图像 - 放大图像 - opening operation of image f : γM,nB(f) = τM,nB(εM,nB(f)), 其中εM,nB和τM,nB分别表示结构元素B对大小为n的图像f的形态侵蚀和放大, 对于像素x: - εM,nB(f)(x) = {f(y) : f(y) = ∧M[f(z)],z ∈ n(Bx)} - τM,nB(f)(x) = {f(y) : f(y) = ∨M[f(z)],z ∈ n(Bx)} - ∧M和∨M分别表示M-ordering的最高和最小峰 4. 使用KLT将彩色图像转换为灰度图像； 5. 图像二值化步骤中，使用了Otsu阈值（指亮度的能量）以获得可靠的二值图像； 结果 图3示出了去除油性亮点的结果： 图4比较了使用BSR操作时的线路检测，并显示了对二值化和细化操作的明显影响。带有BSR的二值化图像具有减少的亮点反射，并且在细化图像中，亮点被转换为小圆圈。图4（f）显示，在线检测阶段，使用BSR生成的图像具有较少的不必要的线段： figure_4_comparisons_of_the_hair_thining_and_line_detection Multi-scale Line Detection Stage (MSLD) 采用改进的霍夫变换(the Hough transform)算法来检测不同的头发长度，并减少由于噪声引起的任何错误检测; 将弯曲的头发视为多条直线; 为了避免在应用稀疏过程时丢失头发，我们使用边缘信息(edge information)来发现任何隐藏或重叠的头发。 #### 总体结构分析 ##### 目的 提出多尺度框架来是为了提高头发检测的准确性； 应用平行线捆绑（PLB）算法（parallel line bundling algorithm）来还原任何隐藏或重叠的头发。 最后，将矢量化的线段用作毛发标记和计数模块的输入数据。 技术 对HT应用了三个比例的图像：1024×768（原始比例），512×384和256×192。对缩放比例图像应用了两种处理方法：边缘检测和细化处理。 PLB算法应用于边缘图像以发现缺失的线段 HT被应用于细化图像以提取线段。通过利用PLB算法，可以恢复隐藏和重叠的头发。 最后，将矢量化的线段重新缩放为原始尺寸1024×768，并由逻辑或运算符进行整合。 结果 如图5所示，由于头发的长度和卷曲度的变化，使用单尺度HT不能检测所有的头发： figure_5_comparison_of_the_single_scale_and_muti-scale_line_detection 图5（a）显示，当一根头发几乎平行出现，与其他头发重叠时，或者如果头发的曲率超过HT的容限，那么最终会遗漏大量的头发，许多线段标签错误。 图5（b）示出了使用来自所有缩放图的线段的结果，从而改善了单个刻度的不足。 Parallel Line Bundling (PLB) 原理 应用Canny边缘检测器获得边缘图。在图6中，假设检测到两条平行线A和B，用\\(ax + by + c_a = 0\\)和\\(ax + by + c_b = 0\\)表示, \\(d=\\frac{|c_a - c_b|} {\\sqrt{a^2 + b^2}}\\)表示线段A,B之间的距离; figure_6_Thining_line_sandwiched_by_two_parallel_lines_with_the_distances_d 计算出夹有细线的平行线之间的平均距离\\(d_{avg}\\)。 如果\\(d &gt; d_{avg}\\)，则当\\((dd_{avg})&gt;w_{th}\\)时，将发现隐藏的头发，其中\\(w_{th}\\)表示边界因子，将通过头皮图像的分辨率根据经验进行修改。 结果 在图7（a）中，圆圈表示隐藏的头发，如图7（b）所示： Hair Labling and Counting 使用MSLD模块，得出了一组线段。根据头发的曲率和方向，将头发实现为具有不同长度的分段线向量簇。这项研究的目的是准确计算头皮上的毛发数量。这可以看作是聚类和标记问题。目标是将一组线段组合成语义“头发”并分配唯一的标签。由于每根头发都由相互关联的线段组成，因此我们为每个簇分配了唯一的标签。 我们采用了松弛标记算法(Relaxation labeling algorithm)来识别每个单独的线段，以确定与哪个线段相关联。 图8（a）显示了10个单独的线段的示例，这些线段被标记为来自同一根头发，然后绘制到\\(（ρ，θ）\\)坐标系上，如图8（b）所示。 交叉点处累积图的结果峰值为10。 figure_8_Example_of_line_segment_labeling Relaxation Labeling (RL) 松弛标注（RL）是一种相互关联的回归方法，它使用符号来描述模型的形状。 它旨在将目标对象（即本文中的线段）与符号或所谓的标记（即头发标签）进行匹配。RL算法首先分配一组随机标记。然后，通过迭代计算，可以获得更准确，更精确的标记集。在本研究中，RL算法被视为用于标记每个线段的聚类方法。 原理 1. 令\\(C(i，λ，j，λ&#39;)\\)表示约束为λ的线段与约束为λ'的线段j的兼容性，其约束为: \\(\\displaystyle\\sum_{λ}C(i，λ，j，λ&#39;) = 1\\) for ∀i,j,λ,λ'。 2. 兼容性C表示标记为λ'的线段j和标记为λ的线段i之间的相互依赖性。 如果兼容性仅由到原点的距离决定，则可能会出现错误的解释。 因此将兼容性定义如下： \\[ C(i，λ，j，λ&#39;) = \\begin{cases} -1 &amp;\\text{if } i \\notin S_j \\\\ \\varepsilon |\\cos[\\theta_i(\\lambda) - \\theta_j(λ&#39;)]| + (1-\\varepsilon) \\frac{\\rho_i(\\lambda)}{\\rho_j(λ&#39;)} &amp;\\text{if } i∈S_j∩λ{=}\\mathllap{/\\,}λ&#39; \\\\ 0 &amp;\\text{otherwise } \\end{cases} \\] - 上式中： ε表示距离和方向置信度之间的加权因子 \\(S_j\\)表示线段j的相邻假设 \\(θ_i(λ)\\)表示从线段i到标记为λ的线段j的方向 \\(ρ_i(λ)\\)表示原点到线段i和标记为λ的线段j之间的距离 如果\\(ρ_j(λ)\\)高, 且\\(C(i，λ，j，λ&#39;)\\)为正，则\\(ρ_i(λ)\\)增加。 此标记算法是一个迭代并行过程，类似于概率松弛中使用的标记丢弃规则，操作员根据其他重量和兼容性反复调整标签重量。 对于每个线段和每个标签，新权重\\(q_i^{(r)}(\\lambda)\\)的计算如下： \\[ q_i^{(r)}(\\lambda) = \\sum_{\\mathclap{j, j{=}\\mathllap{/\\,}i}} \\sum_{\\mathclap{λ&#39;}}C(i，λ，j，λ&#39;)p_j^{(r)}(λ&#39;) \\] 其中: r表示第r次迭代 在等式中，乘积和是被标记为λ的给定线段i的期望 \\(q_i^{(r)}(\\lambda)\\)是当前赋值\\(p_j^{(r)}(λ&#39;)\\)的加权和。 新任务可以用已下公式更新： \\[ p_i^{(r+1)}(\\lambda) = \\frac{p_i^{(r)}(λ)[1+q_i^{(r)}(λ)]}{\\displaystyle\\sum_{j=1}^mp_i^{(r)}(λ&#39;)[1+q_i^{(r)}(λ&#39;)]} \\] 在这里，只选择\\(p_i^{(r)}(\\lambda)\\)和\\(C(i，λ，j，λ&#39;)\\)并应用该等式递归更新\\(p_i^{(r)}(\\lambda)\\)，直到它们停止变化或收敛到1。对每个线段进行迭代验证，直到将其分配给正确的语义标签“ hair”为止。 实验结果 为了评估该系统，我们从UPMOST（UPG622）DMC捕获了40个分辨率为1024×768的比例尺图像作为测试数据集。根据DMC的白平衡，我们将测试图像分为两组，分别是数据集1和数据集2。 Experiment 1: Cross-Validation of the Line Detection 1. 图10（b）-（d）显示了预处理模块的结果，包括亮点去除（BSR），二值化和稀化过程。 为了评估多尺度线检测算法的性能，测试图像我们将HT应用于三种不同尺度，以提取线段。 然后使用头发标签机制确定头发的数量。 2. 如图10（e）-（f）所示，彩色线代表头发的标签。 换句话说，即使头发交叉或重叠，也可以准确地标记头发。 以下部分演示了有关精确度和召回率的毛发计数的客观测量。 我们比较了使用BSR和MSLD模块组合的四种情况，包括BSR + MSLD，而没有同时使用BSR和MSLD，仅BSR和仅MSLD。 table_1_comarison_of_the_system_sensitivity_forthe_module_usages_in_the_line_detection 表I比较了基于模块使用情况的系统敏感性。基于形态学的BSR和MSLD机制，以提高头发检测的性能，其准确率分别为94.98％和98.05％。与传统的HT线检测方法相比，数据集1和数据集2分别提高了2％和1.5％。从召回率的角度来看，它也高于其他模块组合。 平均而言，我们的召回率分别为9％和10％。根据我们的观察，应用BSR后，准确率得到了提高。 但是，将MSLD应用于数据集＃2时，准确率下降，因为它在油性头皮区域中产生了大量的光反射。同样，头发中部的亮点也被大大增强，导致精确度降低。 Experiment 2: System Refinement Using the PLB Algorithm 1. 表II显示了PLB算法的效率。PLB方法使系统能够提取缺失的毛发，精确率提高了0.5％，召回率提高了5％。 2. PLB算法可以用作系统微调过程，将系统性能平均提高到96.89％。这是合理的，因为隐藏的头发不经常出现。 3. 此外，我们可以通过显微镜控制头发图像的分辨率和角度，以避免这种情况。 Experiment 3: Complexity Analysis table_3_time_occupancy_of_each_module_compared_with_total_execution 当涉及到系统复杂性分析时，我们知道拟议的头发计数系统会花费更多时间。平均而言，高分辨率测试图像需要不到4秒的时间即可得出毛发计数信息。 表III列出了每个模块所需的时间与总执行时间的比较。 MSLD使用三个比例尺来获取一组线段，这需要大部分执行时间。 HT将边缘投影到（ρ，θ）空间以提取局部最大值的过程非常耗时。 为了克服这个问题，我们在MSLD中仅使用了两个标度。但是，此尝试导致性能下降。 此外，细化过程占用了总处理时间的四分之一以上。这是执行时间和系统精度之间的权衡。 总结 这项研究提出了一种自动的头发分割和计数系统，以减少人工评估者进行详细头皮评估所需的时间。 1. 首先，油性和湿润的头发会在头发中间产生亮点。在计算头发数之前，我们需要消除头发上的亮点，以避免重复计算一些头发的问题。 2. 第二，波浪状和卷发容易导致线检测故障。当头发不直时，常规的线检测算法无效。 3. 第三，当头发相互交叉并互相咬合时，会出现对头发数量的低估，这使得精确定位所有头发非常困难。 4. 最后，由于头皮相对未曝光，因此头皮的图像通常模糊或难以看见。另外，头皮通常照明不足或曝光过度。 计数思路 通过对不同缩略图中的头发做HT线段检测，然后将所有检测结果进行“逻辑或”整合，以减少缺失头发的计数，解决重叠头发的漏计数的问题。 本文的框架可以被视为迈向化妆品和头皮治疗应用的智能计算机辅助医学图像处理的第一步。 技术总结 预处理阶段 对输入数据（头皮图片）进行预处理，为下一阶段对图片中头发进行精准计数等功能性操作做好铺垫。主要用到以下技术： #### Contrast Stretching (Normalization): - 对比度拉伸（通常称为归一化）是一种简单的图像增强技术,旨在通过“拉伸”图像所包含的强度值范围以覆盖所需的值范围来改善图像的对比度。 - 技术应用：增加头皮与头发之间的对比度，方便下阶段对头发进行语义分割。 Color Morphology EXTENDING MATHEMATICAL MORPHOLOGY TO COLOR IMAGE PROCESSING 技术应用： 除噪，去除油性和湿润的头发在头发的中部产生的亮点 Karhunen-Loeve Transform (KLT) Karhunen-Loeve变换（KLT）（也称为Hotelling变换和特征向量变换），它与主成分分析（PCA）密切相关，并广泛用于许多领域的数据分析中, KL变换基于图像的统计属性，并具有一些重要的属性，使其可用于图像处理，特别是图像压缩。 技术应用： 将彩色图像转换为灰度图像。 Otsu thresholding Otsu thresholding：简单说来该方法可将灰度图像还原为二进制图像。在图像处理和分析中，有时需要一种方法来分离两个相关数据，例如背景和前景。Otsu阈值是一种数据驱动的方法，该方法可以自适应地找到最佳阈值以区分两类数据。 技术应用： 图像分割和图像二值化。 多尺度线检测阶段（MSLD） 该阶段主要是通过各种技术，克服重叠头发无法计数，以及相邻头发之间的头皮被识别为头发等影响头发计数的相关问题。为下阶段对头发进行精准计数的做好铺垫。主要用到以下技术： 霍夫变换（HT) 霍夫变换（HT） 霍夫变换是一种可用于隔离图像中特定形状的特征的技术。因为它要求以某种参数形式指定所需的特征，所以经典的Hough变换最常用于检测规则曲线（例如直线，圆，椭圆等）。 技术应用： HT是最常用的线检测框架之一。但是当使用常规的单尺度HT时，可能会丢失大量的头发段。故提出将HT应用于三个比例的图像，包括1024×768（原始比例），512×384和256×192，最后通过逻辑或将三个比例图像中的被检测出的头发进行整合，以提高头发计数准确度。 Canny Edge Detection Canny边缘检测是一种多步骤算法，可以同时检测到噪声被抑制的边缘。在图像二值化步骤中，由于错误地假定了两根单独的头发的连接，位于两根头发之间的头皮像素被标记为头发的一部分。而且，当两根头发太靠近或彼此重叠时，如果直接应用稀疏算法，则会错过一根或两根头发。 技术应用： 使用边缘信息来找出隐藏或重叠的头发。可以从隐藏的头发或重叠的多根头发中提取两个平行的边缘。 Thinning Parallel Line Bundling (PLB) 头发标记和计数阶段（Hair Labling and Counting） 该阶段的目的是准确计算头皮上的毛发数量。这可以看作是聚类和标记问题。目标是将一组线段组合成语义“头发”并分配唯一的标签。 主要用到以下技术： Relaxation Labeling 松弛标记是一种图像处理方法。其目标是将标签与给定图像的像素或给定图的节点相关联。 技术应用： 在本研究中，RL算法被视为用于标记每个线段的聚类方法。 参考文献 An Unsupervised Hair Segmentation and Counting System in Microscopy Images Contrast Stretching (Normalization) Color Morphology EXTENDING MATHEMATICAL MORPHOLOGY TO COLOR IMAGE PROCESSING Karhunen-Loeve Transform (KLT) Otsu thresholding Hough Transform（HT) Canny Edge Detection Thinning Evaluation of a Bundling Technique for Parallel Coordinates Relaxation Labeling","categories":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Paper Smash","slug":"Technology/Paper-Smash","permalink":"https://littlelittlemoon.github.io/categories/Technology/Paper-Smash/"}],"tags":[{"name":"Hair Counting","slug":"Hair-Counting","permalink":"https://littlelittlemoon.github.io/tags/Hair-Counting/"},{"name":"Digital Image Processing","slug":"Digital-Image-Processing","permalink":"https://littlelittlemoon.github.io/tags/Digital-Image-Processing/"},{"name":"Line Detection","slug":"Line-Detection","permalink":"https://littlelittlemoon.github.io/tags/Line-Detection/"},{"name":"Paper","slug":"Paper","permalink":"https://littlelittlemoon.github.io/tags/Paper/"}],"keywords":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Paper Smash","slug":"Technology/Paper-Smash","permalink":"https://littlelittlemoon.github.io/categories/Technology/Paper-Smash/"}]},{"title":"Evalution of hair and scalp condition based on microscopy image analysis | Hair Counting - part 1","slug":"Evalution of hair and scalp condition based on microscopy image analysis  | Hair Counting - 1","date":"2019-12-24T12:35:33.000Z","updated":"2020-04-09T07:00:21.540Z","comments":true,"path":"2019/12/24/Evalution of hair and scalp condition based on microscopy image analysis  | Hair Counting - 1/","link":"","permalink":"https://littlelittlemoon.github.io/2019/12/24/Evalution%20of%20hair%20and%20scalp%20condition%20based%20on%20microscopy%20image%20analysis%20%20|%20Hair%20Counting%20-%201/","excerpt":"","text":"Paper: Evalution of hair and scalp condition based on microscopy image analysis 计数方法 典型的便携式显微镜相机通常覆盖5mm x 5mm的矩形。因此，如果一根头发长于5毫米，则它必须超出矩形。基于此观察，我们对毛发计数有两个假设： 1. 每根头发由一个起点和一个终点表示; 2. 起点位于矩形内部，终点位于图像的边界上。 头发计数：使用预处理阶段获得的骨架图像，基于上述两点假设，如果头发的骨架线的一个点在矩形内而另一点在边界上，则我们对头发进行计数。 fig_3_Example_of_hair_counting 图3示出了头发计数的示例。在图中，计数的像素用红色点标记，并且这些点叠加在原始图像上。然后，可以通过将计数的像素数除以图像尺寸来计算头发密度。 实验结果 为了评估头发计数算法，我们对200个头皮图像的数据集进行了实验。 表2列出了根据我们的算法计算出的实际毛发数与估计毛发数之间的精确度/召回率。 平均准确度和召回率分别为91.35％和92.01％。即使性能相当好，精度/调用率也可以进一步提高。错误的一个关键原因是预处理和原始图像的质量。在我们的实验中，一旦图像中出现模糊点，就不可能在预处理步骤中消除所有噪音。因此，考虑到相机噪声，精度非常好。 同样，我们测试了毛孔计数算法，结果显示平均准确率约为90％。 总结 计数思路 将图片边缘看作一矩形； 对满足“头发由一个起点和一个终点表示，且起点位于矩形内部，终点位于图像的边界上”条件的头发进行计数； 使用通过细化操作获得的头发骨架图像对满足条件的头发进行计数； 通过将计数的像素数除以图像尺寸来计算头发密度。 参考文献 Evalution of hair and scalp condition based on microscopy image analysis","categories":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Paper Smash","slug":"Technology/Paper-Smash","permalink":"https://littlelittlemoon.github.io/categories/Technology/Paper-Smash/"}],"tags":[{"name":"Hair Counting","slug":"Hair-Counting","permalink":"https://littlelittlemoon.github.io/tags/Hair-Counting/"},{"name":"Digital Image Processing","slug":"Digital-Image-Processing","permalink":"https://littlelittlemoon.github.io/tags/Digital-Image-Processing/"},{"name":"Line Detection","slug":"Line-Detection","permalink":"https://littlelittlemoon.github.io/tags/Line-Detection/"},{"name":"Paper","slug":"Paper","permalink":"https://littlelittlemoon.github.io/tags/Paper/"}],"keywords":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Paper Smash","slug":"Technology/Paper-Smash","permalink":"https://littlelittlemoon.github.io/categories/Technology/Paper-Smash/"}]},{"title":"Hexo-Theme-Sakura","slug":"Hexo-Theme-Sakura","date":"2018-12-12T14:16:01.000Z","updated":"2020-04-10T11:55:43.302Z","comments":true,"path":"2018/12/12/Hexo-Theme-Sakura/","link":"","permalink":"https://littlelittlemoon.github.io/2018/12/12/Hexo-Theme-Sakura/","excerpt":"","text":"hexo-theme-sakura主题 English document 基于WordPress主题Sakura修改成Hexo的主题。 demo预览 正在开发中...... 交流群 若你是使用者，加群QQ: 801511924 若你是创作者，加群QQ: 194472590 主题特性 首页大屏视频 首页随机封面 图片懒加载 valine评论 fancy-box相册 pjax支持，音乐不间断 aplayer音乐播放器 多级导航菜单（按现在大部分hexo主题来说，这也算是个特性了） 赞赏作者 如果喜欢hexo-theme-sakura主题，可以考虑资助一下哦~非常感激！ paypal | Alipay 支付宝 | WeChat Pay 微信支付 未完善的使用教程 那啥？老实说我目前也不是很有条理233333333~ 1、主题下载安装 hexo-theme-sakura建议下载压缩包格式，因为除了主题内容还有些source的配置对新手来说比较太麻烦，直接下载解压就省去这些麻烦咯。 下载好后解压到博客根目录（不是主题目录哦，重复的选择替换）。接着在命令行（cmd、bash）运行npm i安装依赖。 2、主题配置 博客根目录下的_config配置 站点 # Site title: 你的站点名 subtitle: description: 站点简介 keywords: author: 作者名 language: zh-cn timezone: 部署 deploy: type: git repo: github: 你的github仓库地址 # coding: 你的coding仓库地址 branch: master 备份 （使用hexo b发布备份到远程仓库） backup: type: git message: backup my blog of https://honjun.github.io/ repository: # 你的github仓库地址,备份分支名 （建议新建backup分支） github: https://github.com/honjun/honjun.github.io.git,backup # coding: https://git.coding.net/hojun/hojun.git,backup 主题目录下的_config配置 其中标明【改】的是需要修改部门，标明【选】是可改可不改，标明【非】是不用改的部分 # site name # 站点名 【改】 prefixName: さくら荘その siteName: hojun # favicon and site master avatar # 站点的favicon和头像 输入图片路径（下面的配置是都是cdn的相对路径，没有cdn请填写完整路径，建议使用jsdeliver搭建一个cdn啦，先去下载我的cdn替换下图片就行了，简单方便~）【改】 favicon: /images/favicon.ico avatar: /img/custom/avatar.jpg # 站点url 【改】 url: https://sakura.hojun.cn # 站点介绍（或者说是个人签名）【改】 description: Live your life with passion! With some drive! # 站点cdn，没有就为空 【改】 若是cdn为空，一些图片地址就要填完整地址了，比如之前avatar就要填https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/custom/avatar.jpg cdn: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6 # 开启pjax 【选】 pjax: 1 # 站点首页的公告信息 【改】 notice: hexo-Sakura主题已经开源，目前正在开发中... # 懒加载的加载中图片 【选】 lazyloadImg: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/loader/orange.progress-bar-stripe-loader.svg # 站点菜单配置 【选】 menus: 首页: { path: /, fa: fa-fort-awesome faa-shake } 归档: { path: /archives, fa: fa-archive faa-shake, submenus: { 技术: {path: /categories/技术/, fa: fa-code }, 生活: {path: /categories/生活/, fa: fa-file-text-o }, 资源: {path: /categories/资源/, fa: fa-cloud-download }, 随想: {path: /categories/随想/, fa: fa-commenting-o }, 转载: {path: /categories/转载/, fa: fa-book } } } 清单: { path: javascript:;, fa: fa-list-ul faa-vertical, submenus: { 书单: {path: /tags/悦读/, fa: fa-th-list faa-bounce }, 番组: {path: /bangumi/, fa: fa-film faa-vertical }, 歌单: {path: /music/, fa: fa-headphones }, 图集: {path: /tags/图集/, fa: fa-photo } } } 留言板: { path: /comment/, fa: fa-pencil-square-o faa-tada } 友人帐: { path: /links/, fa: fa-link faa-shake } 赞赏: { path: /donate/, fa: fa-heart faa-pulse } 关于: { path: /, fa: fa-leaf faa-wrench , submenus: { 我？: {path: /about/, fa: fa-meetup}, 主题: {path: /theme-sakura/, fa: iconfont icon-sakura }, Lab: {path: /lab/, fa: fa-cogs }, } } 客户端: { path: /client/, fa: fa-android faa-vertical } RSS: { path: /atom.xml, fa: fa-rss faa-pulse } # Home page sort type: -1: newer first，1: older first. 【非】 homePageSortType: -1 # Home page article shown number) 【非】 homeArticleShown: 10 # 背景图片 【选】 bgn: 8 # startdash面板 url, title, desc img 【改】 startdash: - {url: /theme-sakura/, title: Sakura, desc: 本站 hexo 主题, img: /img/startdash/sakura.md.png} - {url: http://space.bilibili.com/271849279, title: Bilibili, desc: 博主的b站视频, img: /img/startdash/bilibili.jpg} - {url: /, title: hojun的万事屋, desc: 技术服务, img: /img/startdash/wangshiwu.jpg} # your site build time or founded date # 你的站点建立日期 【改】 siteBuildingTime: 07/17/2018 # 社交按钮(social) url, img PC端配置 【改】 social: github: {url: http://github.com/honjun, img: /img/social/github.png} sina: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/sina.png} wangyiyun: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/wangyiyun.png} zhihu: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/zhihu.png} email: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/email.svg} wechat: {url: /#, qrcode: /img/custom/wechat.jpg, img: /img/social/wechat.png} # 社交按钮(msocial) url, img 移动端配置 【改】 msocial: github: {url: http://github.com/honjun, fa: fa-github, color: 333} weibo: {url: http://weibo.com/mashirozx?is_all=1, fa: fa-weibo, color: dd4b39} qq: {url: https://wpa.qq.com/msgrd?v=3&amp;uin=954655431&amp;site=qq&amp;menu=yes, fa: fa-qq, color: 25c6fe} # 赞赏二维码（其中wechatSQ是赞赏单页面的赞赏码图片）【改】 donate: alipay: /img/custom/donate/AliPayQR.jpg wechat: /img/custom/donate/WeChanQR.jpg wechatSQ: /img/custom/donate/WeChanSQ.jpg # 首页视频地址为https://cdn.jsdelivr.net/gh/honjun/hojun@1.2/Unbroken.mp4，配置如下 【改】 movies: url: https://cdn.jsdelivr.net/gh/honjun/hojun@1.2 # 多个视频用逗号隔开，随机获取。支持的格式目前已知MP4,Flv。其他的可以试下，不保证有用 name: Unbroken.mp4 # 左下角aplayer播放器配置 主要改id和server这两项，修改详见[aplayer文档] 【改】 aplayer: id: 2660651585 server: netease type: playlist fixed: true mini: false autoplay: false loop: all order: random preload: auto volume: 0.7 mutex: true # Valine评论配置【改】 valine: true v_appId: GyC3NzMvd0hT9Yyd2hYIC0MN-gzGzoHsz v_appKey: mgOpfzbkHYqU92CV4IDlAUHQ 分类页和标签页配置 分类页 ### 标签页 配置项在-cn.yml里。新增一个分类或标签最好加下哦，当然嫌麻烦可以直接使用一张默认图片（可以改主题或者直接把404图片替换下，征求下意见要不要给这个在配置文件中加个开关，可以issue或群里提出来），现在是没设置的话会使用那种倒立小狗404哦。 #category # 按分类名创建 技术: #中文标题 zh: 野生技术协会 # 英文标题 en: Geek – Only for Love # 封面图片 img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/coding.jpg 生活: zh: 生活 en: live img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/writing.jpg #tag # 标签名即是标题 悦读: # 封面图片 img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/reading.jpg 单页面封面配置 如留言板页面页面，位于source下的comment下，打开index.md如下： --- title: comment date: 2018-12-20 23:13:48 keywords: 留言板 description: comments: true # 在这里配置单页面头部图片，自定义替换哦~ photos: https://cdn.jsdelivr.net/gh/honjun/cdn@1.4/img/banner/comment.jpg --- 单页面配置 番组计划页 （请直接在下载后的文件中改，下面的添加了注释可能会有些影响） --- layout: bangumi title: bangumi comments: false date: 2019-02-10 21:32:48 keywords: description: bangumis: # 番组图片 - img: https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg # 番组名 title: 朝花夕誓——于离别之朝束起约定之花 # 追番状态 （追番ing/已追完） status: 已追完 # 追番进度 progress: 100 # 番剧日文名称 jp: さよならの朝に約束の花をかざろう # 放送时间 time: 放送时间: 2018-02-24 SUN. # 番剧介绍 desc: 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。 - img: https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg title: 朝花夕誓——于离别之朝束起约定之花 status: 已追完 progress: 50 jp: さよならの朝に約束の花をかざろう time: 放送时间: 2018-02-24 SUN. desc: 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。 --- 友链页 （请直接在下载后的文件中改，下面的添加了注释可能会有些影响） --- layout: links title: links # 创建日期，可以改下 date: 2018-12-19 23:11:06 # 图片上的标题，自定义修改 keywords: 友人帐 description: # true/false 开启/关闭评论 comments: true # 页面头部图片，自定义修改 photos: https://cdn.jsdelivr.net/gh/honjun/cdn@1.4/img/banner/links.jpg # 友链配置 links: # 类型分组 - group: 个人项目 # 类型简介 desc: 充分说明这家伙是条咸鱼 &lt; (￣︶￣)&gt; items: # 友链链接 - url: https://shino.cc/fgvf # 友链头像 img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg # 友链站点名 name: Google # 友链介绍 下面雷同 desc: Google 镜像 - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google 镜像 # 类型分组... - group: 小伙伴们 desc: 欢迎交换友链 ꉂ(ˊᗜˋ) items: - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google 镜像 - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google 镜像 --- 写文章配置 主题集成了个人插件hexo-tag-bili和hexo-tag-fancybox_img。其中hexo-tag-bili用来在文章或单页面中插入B站外链视频，使用语法如下： 详细使用教程详见hexo-tag-bili。 hexo-tag-fancybox_img用来在文章或单页面中图片，使用语法如下： 详细使用教程详见hexo-tag-fancybox_img 还有啥，一时想不起来...... To be continued...","categories":[],"tags":[{"name":"web","slug":"web","permalink":"https://littlelittlemoon.github.io/tags/web/"},{"name":"悦读","slug":"悦读","permalink":"https://littlelittlemoon.github.io/tags/%E6%82%A6%E8%AF%BB/"}],"keywords":[]}]}