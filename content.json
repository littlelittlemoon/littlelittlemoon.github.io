{"meta":{"title":"LITTLEMEEMOON","subtitle":"Every day with dreams is wonderful.","description":"This is my place where I can share my things about life, academic or just for fun, enjoy. :)","author":"Kayleen","url":"https://littlelittlemoon.github.io"},"pages":[{"title":"about","date":"2019-12-12T14:14:36.000Z","updated":"2020-04-08T13:08:47.938Z","comments":false,"path":"about/index.html","permalink":"https://littlelittlemoon.github.io/about/index.html","excerpt":"","text":"[Bear Cave - Kayleen]] ä¸&nbsp; Kayleen&nbsp; ï¼ˆ çœŸï¼ˆã¾ï¼‰ç™½ï¼ˆã—ã‚ï¼‰ ï¼‰ å¯¹è¯ä¸­... bot_ui_ini()","keywords":"å…³äº"},{"title":"bangumi","date":"2019-02-10T13:32:48.000Z","updated":"2020-04-06T16:05:44.917Z","comments":false,"path":"bangumi/index.html","permalink":"https://littlelittlemoon.github.io/bangumi/index.html","excerpt":"","text":"","keywords":null},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2020-04-06T16:22:30.837Z","comments":false,"path":"client/index.html","permalink":"https://littlelittlemoon.github.io/client/index.html","excerpt":"","text":"ç›´æ¥ä¸‹è½½ or æ‰«ç ä¸‹è½½ï¼š","keywords":"Androidå®¢æˆ·ç«¯"},{"title":"categories","date":"2019-12-20T15:13:48.000Z","updated":"2020-04-09T07:57:24.966Z","comments":true,"path":"categories/index.html","permalink":"https://littlelittlemoon.github.io/categories/index.html","excerpt":"","text":"","keywords":"æ–‡ç« åˆ†ç±»"},{"title":"donate","date":"2019-12-20T15:13:05.000Z","updated":"2020-04-07T11:26:06.240Z","comments":false,"path":"donate/index.html","permalink":"https://littlelittlemoon.github.io/donate/index.html","excerpt":"","text":"","keywords":"è°¢è°¢é¥²ä¸»äº†å–µ~"},{"title":"comment","date":"2019-12-20T15:13:48.000Z","updated":"2020-04-07T11:26:02.901Z","comments":true,"path":"comment/index.html","permalink":"https://littlelittlemoon.github.io/comment/index.html","excerpt":"","text":"å¿µä¸¤å¥è¯— äººé—²æ¡‚èŠ±è½ï¼Œå¤œé™æ˜¥å±±ç©ºã€‚ æœˆå‡ºæƒŠå±±é¸Ÿï¼Œæ—¶é¸£æ˜¥æ¶§ä¸­ã€‚ ç‹ç»´ Â·ã€Šé¸Ÿé¸£æ¶§ã€‹","keywords":"ç•™è¨€æ¿"},{"title":"lab","date":"2019-01-05T13:47:59.000Z","updated":"2020-04-07T11:26:09.358Z","comments":false,"path":"lab/index.html","permalink":"https://littlelittlemoon.github.io/lab/index.html","excerpt":"","text":"sakuraä¸»é¢˜ balabala","keywords":"Labå®éªŒå®¤"},{"title":"links","date":"2018-12-19T15:11:06.000Z","updated":"2020-04-07T11:26:13.463Z","comments":true,"path":"links/index.html","permalink":"https://littlelittlemoon.github.io/links/index.html","excerpt":"","text":"","keywords":"å‹äººå¸"},{"title":"music","date":"2018-12-20T15:14:28.000Z","updated":"2020-04-09T08:55:04.838Z","comments":false,"path":"music/index.html","permalink":"https://littlelittlemoon.github.io/music/index.html","excerpt":"","text":"","keywords":"å°æ‚¦é‚€ä½ åœä¸‹æ¥å¬ä¼šæ­ŒğŸ˜Š"},{"title":"rss","date":"2018-12-20T15:09:03.000Z","updated":"2020-04-06T16:05:44.919Z","comments":true,"path":"rss/index.html","permalink":"https://littlelittlemoon.github.io/rss/index.html","excerpt":"","text":""},{"title":"theme-sakura","date":"2019-01-04T14:53:25.000Z","updated":"2020-04-06T16:05:44.919Z","comments":false,"path":"theme-sakura/index.html","permalink":"https://littlelittlemoon.github.io/theme-sakura/index.html","excerpt":"","text":"Hexoä¸»é¢˜Sakuraä¿®æ”¹è‡ªWordPressä¸»é¢˜Sakuraï¼Œæ„Ÿè°¢åŸä½œè€…Mashiro","keywords":"Hexo ä¸»é¢˜ Sakura ğŸŒ¸"},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2020-04-07T11:26:20.372Z","comments":true,"path":"tags/index.html","permalink":"https://littlelittlemoon.github.io/tags/index.html","excerpt":"","text":"","keywords":"æ ‡ç­¾"},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2020-04-06T16:05:44.919Z","comments":false,"path":"video/index.html","permalink":"https://littlelittlemoon.github.io/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: 'æœèŠ±å¤•èª“â€”â€”äºç¦»åˆ«ä¹‹æœæŸèµ·çº¦å®šä¹‹èŠ±', status: 'å·²è¿½å®Œ', progress: 100, jp: 'ã•ã‚ˆãªã‚‰ã®æœã«ç´„æŸã®èŠ±ã‚’ã‹ã–ã‚ã†', time: 'æ”¾é€æ—¶é—´: 2018-02-24 SUN.', desc: ' ä½åœ¨è¿œç¦»å°˜åš£çš„åœŸåœ°ï¼Œä¸€è¾¹å°†æ¯å¤©çš„äº‹æƒ…ç¼–ç»‡æˆåä¸ºå¸Œæ¯”æ¬§çš„å¸ƒï¼Œä¸€è¾¹é™é™ç”Ÿæ´»çš„ä¼Šæ¬§å¤«äººæ°‘ã€‚åœ¨15å²å·¦å³å¤–è¡¨å°±åœæ­¢æˆé•¿ï¼Œæ‹¥æœ‰æ•°ç™¾å¹´å¯¿å‘½çš„ä»–ä»¬ï¼Œè¢«ç§°ä¸ºâ€œç¦»åˆ«çš„ä¸€æ—â€ï¼Œå¹¶è¢«è§†ä¸ºæ´»ç€çš„ä¼ è¯´ã€‚æ²¡æœ‰åŒäº²çš„ä¼Šæ¬§å¤«å°‘å¥³ç›å¥‡äºšï¼Œè¿‡ç€è¢«ä¼™ä¼´åŒ…å›´çš„å¹³ç¨³æ—¥å­ï¼Œå´æ€»æ„Ÿè§‰â€œå­¤èº«ä¸€äººâ€ã€‚ä»–ä»¬çš„è¿™ç§æ—¥å¸¸ï¼Œä¸€ç¬é—´å°±å´©æºƒæ¶ˆå¤±ã€‚è¿½æ±‚ä¼Šæ¬§å¤«çš„é•¿å¯¿ä¹‹è¡€ï¼Œæ¢…è¨è’‚å†›ä¹˜åç€åä¸ºé›·çº³ç‰¹çš„å¤ä»£å…½å‘åŠ¨äº†è¿›æ”»ã€‚åœ¨ç»æœ›ä¸æ··ä¹±ä¹‹ä¸­ï¼Œä¼Šæ¬§å¤«çš„ç¬¬ä¸€ç¾å¥³è•¾è‰äºšè¢«æ¢…è¨è’‚å¸¦èµ°ï¼Œè€Œç›å¥‡äºšæš—æ‹çš„å°‘å¹´å…‹é‡Œå§†ä¹Ÿå¤±è¸ªäº†ã€‚ç›å¥‡äºšè™½ç„¶æ€»ç®—é€ƒè„±äº†ï¼Œå´å¤±å»äº†ä¼™ä¼´å’Œå½’å»ä¹‹åœ°â€¦â€¦ã€‚' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: 'æœèŠ±å¤•èª“â€”â€”äºç¦»åˆ«ä¹‹æœæŸèµ·çº¦å®šä¹‹èŠ±', status: 'å·²è¿½å®Œ', progress: 100, jp: 'ã•ã‚ˆãªã‚‰ã®æœã«ç´„æŸã®èŠ±ã‚’ã‹ã–ã‚ã†', time: '2018-02-24 SUN.', desc: ' ä½åœ¨è¿œç¦»å°˜åš£çš„åœŸåœ°ï¼Œä¸€è¾¹å°†æ¯å¤©çš„äº‹æƒ…ç¼–ç»‡æˆåä¸ºå¸Œæ¯”æ¬§çš„å¸ƒï¼Œä¸€è¾¹é™é™ç”Ÿæ´»çš„ä¼Šæ¬§å¤«äººæ°‘ã€‚åœ¨15å²å·¦å³å¤–è¡¨å°±åœæ­¢æˆé•¿ï¼Œæ‹¥æœ‰æ•°ç™¾å¹´å¯¿å‘½çš„ä»–ä»¬ï¼Œè¢«ç§°ä¸ºâ€œç¦»åˆ«çš„ä¸€æ—â€ï¼Œå¹¶è¢«è§†ä¸ºæ´»ç€çš„ä¼ è¯´ã€‚æ²¡æœ‰åŒäº²çš„ä¼Šæ¬§å¤«å°‘å¥³ç›å¥‡äºšï¼Œè¿‡ç€è¢«ä¼™ä¼´åŒ…å›´çš„å¹³ç¨³æ—¥å­ï¼Œå´æ€»æ„Ÿè§‰â€œå­¤èº«ä¸€äººâ€ã€‚ä»–ä»¬çš„è¿™ç§æ—¥å¸¸ï¼Œä¸€ç¬é—´å°±å´©æºƒæ¶ˆå¤±ã€‚è¿½æ±‚ä¼Šæ¬§å¤«çš„é•¿å¯¿ä¹‹è¡€ï¼Œæ¢…è¨è’‚å†›ä¹˜åç€åä¸ºé›·çº³ç‰¹çš„å¤ä»£å…½å‘åŠ¨äº†è¿›æ”»ã€‚åœ¨ç»æœ›ä¸æ··ä¹±ä¹‹ä¸­ï¼Œä¼Šæ¬§å¤«çš„ç¬¬ä¸€ç¾å¥³è•¾è‰äºšè¢«æ¢…è¨è’‚å¸¦èµ°ï¼Œè€Œç›å¥‡äºšæš—æ‹çš„å°‘å¹´å…‹é‡Œå§†ä¹Ÿå¤±è¸ªäº†ã€‚ç›å¥‡äºšè™½ç„¶æ€»ç®—é€ƒè„±äº†ï¼Œå´å¤±å»äº†ä¼™ä¼´å’Œå½’å»ä¹‹åœ°â€¦â€¦ã€‚' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} ç•ªç»„è®¡åˆ’ è¿™é‡Œå°†æ˜¯æ°¸è¿œçš„å›å¿† window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} æ”¾é€æ—¶é—´: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"Bç«™"}],"posts":[{"title":"åƒµå°¸ä¼ä¸šåˆ†ç±»|æ¨¡å‹è®­ç»ƒ|LightGBM","slug":"åƒµå°¸ä¼ä¸šåˆ†ç±»-æ¨¡å‹è®­ç»ƒ-LightGBM","date":"2020-04-11T11:25:37.000Z","updated":"2020-04-11T10:27:42.117Z","comments":true,"path":"2020/04/11/åƒµå°¸ä¼ä¸šåˆ†ç±»-æ¨¡å‹è®­ç»ƒ-LightGBM/","link":"","permalink":"https://littlelittlemoon.github.io/2020/04/11/%E5%83%B5%E5%B0%B8%E4%BC%81%E4%B8%9A%E5%88%86%E7%B1%BB-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-LightGBM/","excerpt":"","text":"å†™åˆ°å‰é¢çš„è¯ åœ¨ä¸Šä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å¯¹åƒµå°¸ä¼ä¸šåˆ†ç±»é—®é¢˜çš„æ•°æ®é¢„å¤„ç†åšäº†ä»‹ç»ï¼Œè¿™ç¯‡æ–‡ç« ä¸»è¦ä»‹ç»æ¨¡å‹è®­ç»ƒéƒ¨åˆ†ï¼Œé‡‡ç”¨çš„ç®—æ³•æ¡†æ¶æ˜¯LightGBM(Light Gradient Boosting Machine)ï¼Œæ˜¯ä¸€ä¸ªå®ç° GBDT ç®—æ³•çš„æ¡†æ¶ã€‚è€ŒGBDT(Gradient Boosting Decision Tree)çš„ä¸»è¦æ€æƒ³æ˜¯åˆ©ç”¨å¼±åˆ†ç±»å™¨ï¼ˆå†³ç­–æ ‘ï¼‰è¿­ä»£è®­ç»ƒä»¥å¾—åˆ°æœ€ä¼˜æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰è®­ç»ƒæ•ˆæœå¥½ã€ä¸æ˜“è¿‡æ‹Ÿåˆç­‰ä¼˜ç‚¹ã€‚ LightGBMæœ‰å„ç§å¾ˆå¥½çš„ç‰¹æ€§ï¼Œç±»æ¯”å¾ˆå¤šboosting toolsä¼˜åŒ–é€Ÿåº¦å’Œå†…å­˜çš„ä½¿ç”¨ä¸Šé‡‡ç”¨åŸºäºé¢„æ’åºçš„ç®—æ³•è¿›è¡Œå†³ç­–æ ‘å­¦ä¹ ï¼ŒLightGBMä½¿ç”¨åŸºäºç›´æ–¹å›¾çš„ç®—æ³•ï¼Œæ”¯æŒé«˜æ•ˆç‡çš„å¹¶è¡Œè®­ç»ƒï¼Œå¹¶ä¸”å…·æœ‰ä»¥ä¸‹ä¼˜ç‚¹ï¼š 1. æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ 2. æ›´ä½çš„å†…å­˜æ¶ˆè€— 3. æ›´å¥½çš„å‡†ç¡®ç‡ 4. åˆ†å¸ƒå¼æ”¯æŒï¼Œå¯ä»¥å¿«é€Ÿå¤„ç†æµ·é‡æ•°æ® å…·ä½“ç®—æ³•åŸç†æˆ‘ä¹Ÿæ²¡æå¤ªæ˜ç™½ï¼Œå“ˆå“ˆã€‚æˆ‘å‡†å¤‡åé¢å¥½å¥½å­¦ä¹ ä¸€ç•ªï¼Œç„¶åå†å†™ä¸€ç¯‡å…³äºLightGBMçš„æ–‡ç« ï¼Œä¸»è¦æ˜¯è®°å½•è‡ªå·±çš„å­¦ä¹ è¿‡ç¨‹ï¼Œå†™ä¸€äº›è‡ªå·±çš„ç†è§£ã€‚è¿™é‡Œå¤§å®¶å¦‚æœæ„Ÿå…´è¶£å¯ä»¥å»çœ‹çœ‹å®˜æ–¹æ–‡æ¡£. æ¨¡å‹è®­ç»ƒ å®šä¹‰æ¨¡å‹è®­ç»ƒå‡½æ•° import lightgbm as lgb from sklearn import metrics def train_model(X_train, y_train, X_valid, y_valid, test=None, feature_cols=None, is_base=True): if feature_cols is None: feature_cols = X_train.columns.drop([&quot;è¡Œä¸š&quot;, &quot;åŒºåŸŸ&quot;, &quot;ä¼ä¸šç±»å‹&quot;, &quot;æ§åˆ¶äººç±»å‹&quot;]) dtrain = lgb.Dataset(X_train[feature_cols], label=y_train) dvalid = lgb.Dataset(X_valid[feature_cols], label=y_valid) param = {&#39;num_leaves&#39;: 64, &#39;objective&#39;: &#39;binary&#39;, &#39;metric&#39;: &#39;auc&#39;, &#39;seed&#39;: 7} num_round = 1000 print(&quot;Training model!&quot;) bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=20, verbose_eval=False) # é¢„æµ‹ç»“æœæ˜¯æ¦‚ç‡å€¼ï¼Œå°†å…¶è½¬æ¢ä¸ºbinary value valid_pred = bst.predict(X_valid[feature_cols]) valid_pred = valid_pred &gt; 0.5 valid_pred = valid_pred.astype(int) valid_score = metrics.roc_auc_score(y_valid, valid_pred) print(&quot;precision recall fscore support:&quot;) print(metrics.precision_recall_fscore_support(y_valid, valid_pred, average=&#39;micro&#39;)) print(f&quot;Validation AUC score: {valid_score}&quot;) if test is not None: test_pred = bst.predict(test[feature_cols]) test_pred = test_pred &gt; 0.5 test_pred = test_pred.astype(int) test_pred = test[[&#39;ID&#39;]].join(pd.DataFrame({&#39;flag&#39;: test_pred})) if is_base: test_pred.to_csv(&#39;test_base.txt&#39;, sep=&#39;,&#39;, index=False) else: test_pred.to_csv(&#39;test_.txt&#39;, sep=&#39;,&#39;, index=False) return bst, valid_score else: return bst, valid_score åŠ è½½å¤„ç†å¥½çš„æ•°æ®é›† import pandas as pd # load training data all_data = pd.read_csv(&quot;data/train/train.csv&quot;) # load testing data test = pd.read_csv(&quot;data/test/test.csv&quot;) test_base = pd.read_csv(&quot;data/test/base-test.csv&quot;) å°†è®­ç»ƒæ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†ä¸éªŒè¯é›† from sklearn.preprocessing import OneHotEncoder, LabelEncoder from sklearn.model_selection import train_test_split import category_encoders as ce from sklearn.utils import shuffle all_data_X = all_data[all_data.columns.drop([&quot;flag&quot;])] all_data_y = all_data[&quot;flag&quot;] # shuffle dataï¼ˆoptionalï¼‰ all_data_X, all_data_y = shuffle(all_data_X, all_data_y) # test = shuffle(test) train_X, valid_X, train_y, valid_y = train_test_split(all_data_X, all_data_y, random_state=66) ç±»å‹å˜é‡å¤„ç† é€šå¸¸ï¼Œæœ¬è´¨ä¸Šæ˜¯åˆ†ç±»çš„ä»»ä½•æ•°æ®å±æ€§éƒ½æ˜¯ç¦»æ•£å€¼ï¼Œè¿™äº›ç¦»æ•£å€¼å±äºç±»åˆ«æˆ–ç±»çš„ç‰¹å®šæœ‰é™é›†åˆã€‚åœ¨å±æ€§æˆ–ç”±æ¨¡å‹é¢„æµ‹çš„å˜é‡çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œé€šå¸¸ä¹Ÿç§°ä¸ºç±»æˆ–æ ‡ç­¾ã€‚ è¿™äº›ç¦»æ•£å€¼æœ¬è´¨ä¸Šå¯ä»¥æ˜¯æ–‡æœ¬æˆ–æ•°å­—ã€‚è¿™æ¬¡base dataé‡Œé¢çš„è¡Œä¸šï¼ŒåŒºåŸŸï¼Œä¼ä¸šç±»å‹ï¼Œæ§åˆ¶äººç±»å‹ç­‰å°±æ˜¯å±äºè¿™ä¸€ç±»å±æ€§ï¼Œæˆ‘åˆšå¼€å§‹çš„æƒ³æ³•æ˜¯æŠŠè¿™ç±»å±æ€§å€¼åšä¸€ä¸ªencodingï¼Œå…·ä½“ä»€ä¹ˆç±»å‹çš„encodingå¯ä»¥æ ¹æ®å±æ€§å¯¹æ¨¡å‹çš„å½±å“ç¨‹åº¦å’Œæ¨¡å‹è®­ç»ƒæ•ˆæœæ¥ç¡®å®šã€‚è¿™é‡Œåªç»™å‡ºäº†count encodingçš„ä¾‹å­ï¼Œå¦‚æœåé¢æœ‰æ—¶é—´æˆ‘ä¼šä¸“é—¨å†™ä¸€ç¯‡å¤„ç†è¿™ç±»å±æ€§çš„å¸¸ç”¨æ–¹æ³•ï¼Œåšä¸€ä¸ªè¯¦ç»†çš„æ€»ç»“ï¼Œè¿™é‡Œå°±ä¸å±•å¼€äº†ã€‚ Count Encoding for categorical variables cat_features = [&quot;è¡Œä¸š&quot;, &quot;åŒºåŸŸ&quot;, &quot;ä¼ä¸šç±»å‹&quot;, &quot;æ§åˆ¶äººç±»å‹&quot;] count_enc = ce.CountEncoder(cols=cat_features) # Learn encoding from the training set count_enc.fit(train_X[cat_features]) train_encoded_X = train_X.join(count_enc.transform(train_X[cat_features]) .add_suffix(&quot;_count&quot;)) valid_encoded_X = valid_X.join(count_enc.transform(valid_X[cat_features]) .add_suffix(&quot;_count&quot;)) test_encoded = test.join(count_enc.transform(test[cat_features]) .add_suffix(&quot;_count&quot;)) æ¨¡å‹è®­ç»ƒ åŸå§‹æ•°æ® ä¸ºäº†çœ‹åˆ†ç±»æ•°æ®å¤„ç†å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œä¸‹é¢çš„è®­ç»ƒæ²¡æœ‰ç”¨ç¼–ç åçš„ç±»åˆ«æ•°æ®ï¼Œè®­ç»ƒçš„æ—¶å€™ä¼šæŠŠç±»åˆ«å±æ€§åˆ—åˆ æ‰ã€‚ print(&quot;Baseline model&quot;) _ = train_model(train_X, train_y, valid_X, valid_y, test, is_base=True) Baseline model Training model! precision recall fscore support: (1.0, 1.0, 1.0, None) Validation AUC score: 1.0 ç»“æœè¶…å‡ºæ„æ–™ï¼Œå°±æ˜¯æœ€ç²—ç³™çš„æ•°æ®ï¼Œæ²¡æœ‰åšä»»ä½•ç‰¹å¾é€‰æ‹©å’Œä¼˜åŒ–ï¼Œå°±å¯ä»¥å¾—åˆ°è¿™ä¹ˆæå¾—ç²¾åº¦ï¼Œè¯´å®è¯æˆ‘å½“æ—¶æ˜¯æ€€ç–‘è‡ªå·±çš„ï¼Œåå¤ç¡®è®¤äº†å¥½å‡ éä»£ç ï¼Œå‘ç°æ²¡å•¥é—®é¢˜ã€‚ğŸ¤£å†æ¬¡è¯æ˜æ•°æ®é¢„å¤„ç†çš„é‡è¦æ€§ï¼Œæ•°æ®å¤„ç†å¥½äº†ï¼Œæœ€ç®€å•çš„æ¨¡å‹ä¹Ÿå¯ä»¥è¾¾åˆ°å¾ˆå¥½çš„æ•ˆæœã€‚ Count Encodingç±»åˆ«å±æ€§åçš„æ•°æ® print(&quot;Count Encoding model&quot;) _ = train_model(train_encoded_X, train_y, valid_encoded_X, valid_y, test_encoded, is_base=False) Count Encoding model Training model! precision recall fscore support: (1.0, 1.0, 1.0, None) Validation AUC score: 1.0 ä¸¤æ¬¡çš„è®­ç»ƒç»“æœæ²¡æœ‰ä»»ä½•å·®åˆ«ï¼Œä¹Ÿå¤±å»äº†ç‰¹å¾ä¼˜åŒ–çš„åŠ¨åŠ›ğŸ˜ƒ","categories":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Machine Learning","slug":"Technology/Machine-Learning","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/"},{"name":"Modeling","slug":"Technology/Machine-Learning/Modeling","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/Modeling/"}],"tags":[{"name":"LightGBM","slug":"LightGBM","permalink":"https://littlelittlemoon.github.io/tags/LightGBM/"},{"name":"Count Encoding","slug":"Count-Encoding","permalink":"https://littlelittlemoon.github.io/tags/Count-Encoding/"}],"keywords":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Machine Learning","slug":"Technology/Machine-Learning","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/"},{"name":"Modeling","slug":"Technology/Machine-Learning/Modeling","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/Modeling/"}]},{"title":"åƒµå°¸ä¼ä¸šåˆ†ç±»|æ•°æ®é¢„å¤„ç†","slug":"åƒµå°¸ä¼ä¸šåˆ†ç±»-æ•°æ®é¢„å¤„ç†","date":"2020-04-10T12:02:34.000Z","updated":"2020-04-11T08:13:35.896Z","comments":true,"path":"2020/04/10/åƒµå°¸ä¼ä¸šåˆ†ç±»-æ•°æ®é¢„å¤„ç†/","link":"","permalink":"https://littlelittlemoon.github.io/2020/04/10/%E5%83%B5%E5%B0%B8%E4%BC%81%E4%B8%9A%E5%88%86%E7%B1%BB-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/","excerpt":"","text":"é—®é¢˜æè¿° åƒµå°¸ä¼ä¸šæ˜¯æŒ‡ç¼ºä¹ç›ˆåˆ©èƒ½åŠ›å´èƒ½å¤Ÿä»¥ä½äºå¸‚åœºæœ€ä¼˜åˆ©ç‡æˆæœ¬è·å¾—ä¿¡è´·èµ„æºï¼Œä¾é å¤–ç•Œè¾“è¡€è€Œç¼ºä¹è‡ªç”Ÿèƒ½åŠ›çš„ä¼ä¸šã€‚åƒµå°¸ä¼ä¸šçš„å­˜åœ¨ç ´åäº†å¸‚åœºæœºåˆ¶ï¼ŒåŠ å‰§äº†ä¿¡è´·èµ„æºçš„é”™é…ï¼Œå¸¦æ¥äº†ä¸¥é‡çš„äº§èƒ½è¿‡å‰©é—®é¢˜ï¼Œè¿˜å¯¹å…¶ä»–éåƒµå°¸ä¼ä¸šäº§ç”Ÿäº†æŠ•èµ„æŒ¤å‡ºæ•ˆåº”ã€‚ å› æ­¤éœ€è¦å¯¹æ­£å¸¸ä¼ä¸šå’Œåƒµå°¸ä¼ä¸šè¿›è¡Œåˆ†ç±»ï¼Œç°ç»™å‡ºä¸€æ‰¹æœ‰æ ‡ç­¾çš„ä¼ä¸šæ•°æ®ä½œä¸ºè®­ç»ƒé›†ï¼Œæ ‡ç­¾ä¸º0è¡¨ç¤ºæ­£å¸¸ä¼ä¸šï¼Œæ ‡ç­¾ä¸º1è¡¨ç¤ºåƒµå°¸ä¼ä¸šï¼›åŒæ—¶ç»™å‡ºæ— æ ‡ç­¾æ•°æ®ä½œä¸ºæµ‹è¯•é›†ï¼Œè¯·å¯¹æ— æ ‡ç­¾æ•°æ®è¿›è¡Œåˆ†ç±»ã€‚ æ•°æ®é›†è¯´æ˜ æ•°æ®é›†åŒ…æ‹¬è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸¤éƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†åˆåŒ…æ‹¬: 1. ä¼ä¸šåŸºæœ¬æ•°æ®: åŒ…å«ä¼ä¸šçš„ä¸€äº›åŸºæœ¬å±æ€§ä»¥åŠä¼ä¸šçš„æ ‡ç­¾ï¼ˆå³flag--0ï¼šæ­£å¸¸ä¼ä¸šï¼Œ1ï¼šåƒµå°¸ä¼ä¸šï¼‰; 2. ä¼ä¸šçŸ¥è¯†äº§æƒæ•°æ®: åŒ…å«ä¼ä¸šçš„çŸ¥è¯†äº§æƒç›¸å…³ä¿¡æ¯ï¼Œæ ¹æ®idå¯ä¸åŸºæœ¬æ•°æ®ä¸€ä¸€å¯¹åº”; 3. ä¼ä¸šé‡‘èæ•°æ®: åŒ…å«ä¼ä¸š2015~2017ä¸‰å¹´çš„é‡‘èç›¸å…³ä¿¡æ¯ï¼Œæ ¹æ®idå¯ä¸åŸºæœ¬æ•°æ®ç›¸å¯¹åº”; 4. ä¼ä¸šå¹´æŠ¥æ•°æ®: åŒ…å«ä¼ä¸š2015~2017ä¸‰å¹´çš„å¹´æŠ¥æ•°æ®ï¼Œæ ¹æ®idå¯ä¸åŸºæœ¬æ•°æ®ç›¸å¯¹åº”ã€‚ æ•°æ®é¢„å¤„ç† å·¥å…·å‡½æ•°å®šä¹‰ combined_base_knowledge_data å‡½æ•°è¯´æ˜ è¯¥å‡½æ•°ç”¨äºæ ¹æ®IDåˆå¹¶base dataå’Œknowledge dataæ•°æ®é›†ã€‚ å‡½æ•°å®ç° def combined_base_knowledge_data(base_data, knowledge_data): combined_base_knowledge_data = base_data .set_index(&#39;ID&#39;) .join(knowledge_data .set_index(&#39;ID&#39;)) combined_base_knowledge_data = combined_base_knowledge_data .fillna(combined_base_knowledge_data .median()) return combined_base_knowledge_data fill_na å‡½æ•°è¯´æ˜ è¯¥å‡½æ•°ç”¨äºå¡«å……æ•°æ®é›†ä¸­ç¼ºå¤±çš„æ•°æ®ï¼š 1. å¹´ä»½æ•°æ®ï¼šæ ¹æ®IDå’Œå¹´ä»½æŸ¥æ‰¾å‡ºå¯¹åº”IDç¼ºå¤±çš„å¹´ä»½ï¼Œç„¶åè¡¥å…¨ç¼ºå¤±çš„å¹´ä»½ï¼› 2. å…¶ä»–ç¼ºå¤±æ•°æ®ç»Ÿä¸€ç”¨ä¸­ä½æ•°è¡¥é½ã€‚ å‡½æ•°å®ç° # fill null year def fill(oragin_data, years = [2015, 2016, 2017]): # pick null year data null_years = oragin_data.loc[oragin_data.year.isna()] # fill year for year in years: IDs = oragin_data[[&quot;ID&quot;]].loc[oragin_data.year == year] for null_year_id in null_years[&quot;ID&quot;].unique(): tmp = IDs.loc[IDs.ID == null_year_id] if tmp.empty: index = oragin_data.loc[( oragin_data.ID == null_year_id) &amp; (oragin_data.year.isna() )].index.tolist() if len(index) != 0: oragin_data.loc[index[0]:index[0], &quot;year&quot;] = year # fill other missing value with median value new_data = oragin_data.fillna(oragin_data.median()) return new_data; combined_new_year_money_data å‡½æ•°è¯´æ˜ è¯¥æ–¹æ³•ç”¨äºæ ¹æ®IDå’Œyearåˆå¹¶year dataå’Œmoney dataã€‚ å‡½æ•°å®ç° def combined_new_year_money_data(new_year_data, new_money_data): return pd.merge(new_year_data, new_money_data, on=[&#39;ID&#39;, &#39;year&#39;]) split_data å‡½æ•°è¯´æ˜ è¯¥å‡½æ•°ç”¨äºæ ¹æ®IDå’Œyearæ‹†åˆ†æ–°ç‰¹å¾ï¼Œå°†åŸæ•°æ®é›†ä¸‰å¹´çš„æ•°æ®æŒ‰å¹´ä»½æ‹†åˆ†æˆæ–°çš„ç‰¹å¾ï¼Œä½¿æ‹†åˆ†åçš„æ•°æ®é›†å¯ä¸base dataæ•°æ®é›†ç”¨IDä¸€ä¸€å¯¹åº”ã€‚ä¸»è¦ç”¨äºyear dataæ•°æ®é›†å’Œmoney dataæ•°æ®é›†çš„æ‹†åˆ†ã€‚ å‡½æ•°å®ç° def split_data(combined_new_year_money_data): # split data with year combined_new_year_data_2015 = combined_new_year_money_data.loc[ combined_new_year_money_data.year == 2015] .set_index(&#39;ID&#39;) .add_suffix(&quot;_2015&quot;).drop(columns=[&#39;year_2015&#39;]) combined_new_year_data_2016 = combined_new_year_money_data.loc[ combined_new_year_money_data.year == 2016] .set_index(&#39;ID&#39;) .add_suffix(&quot;_2016&quot;).drop(columns=[&#39;year_2016&#39;]) combined_new_year_data_2017 = combined_new_year_money_data.loc[ combined_new_year_money_data.year == 2017] .set_index(&#39;ID&#39;) .add_suffix(&quot;_2017&quot;).drop(columns=[&#39;year_2017&#39;]) # marge data with ID combined_new_splited_year_money_data = pd.merge( combined_new_year_data_2015, combined_new_year_data_2016, on=[&#39;ID&#39;]) combined_new_splited_year_money_data = pd.merge( combined_new_splited_year_money_data, combined_new_year_data_2017, on=[&#39;ID&#39;]) return combined_new_splited_year_money_data è®­ç»ƒæ•°æ®é›†å¤„ç† åŠ è½½åŸå§‹æ•°æ® import pandas as pd # load training data base_train_data = pd.read_csv(&quot;data/train/base-train.csv&quot;) year_train_data = pd.read_csv(&quot;data/train/year-train.csv&quot;) knowledge_train_data = pd.read_csv(&quot;data/train/knowledge-train.csv&quot;) money_train_data = pd.read_csv(&quot;data/train/money-train.csv&quot;) æŸ¥çœ‹åŸå§‹æ•°æ®é›†ä¿¡æ¯ï¼šbase data and knowledge data base_train_data base_train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID æ³¨å†Œæ—¶é—´ æ³¨å†Œèµ„æœ¬ è¡Œä¸š åŒºåŸŸ ä¼ä¸šç±»å‹ æ§åˆ¶äººç±»å‹ æ§åˆ¶äººæŒè‚¡æ¯”ä¾‹ flag 0 5986361 2014.0 7090.0 æœåŠ¡ä¸š æ¹–åŒ— æœ‰é™è´£ä»»å…¬å¸ è‡ªç„¶äºº 0.93 0 1 5991749 2007.0 5940.0 é›¶å”®ä¸š æ¹–å— åˆä¼™ä¼ä¸š ä¼ä¸šæ³•äºº 0.57 0 2 5998154 2002.0 9720.0 å·¥ä¸š ç¦å»º åˆä¼™ä¼ä¸š è‡ªç„¶äºº 0.74 0 3 5984390 2000.0 4800.0 å•†ä¸šæœåŠ¡ä¸š å±±ä¸œ è‚¡ä»½æœ‰é™å…¬å¸ NaN 0.90 0 4 5980535 2004.0 4530.0 é›¶å”®ä¸š å¹¿ä¸œ å†œæ°‘ä¸“ä¸šåˆä½œç¤¾ è‡ªç„¶äºº 0.95 0 base_train_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID æ³¨å†Œæ—¶é—´ æ³¨å†Œèµ„æœ¬ æ§åˆ¶äººæŒè‚¡æ¯”ä¾‹ flag count 2.851900e+04 28230.000000 28220.000000 28223.000000 28519.000000 mean 4.332423e+06 2007.010627 5024.659816 0.754786 0.392721 std 2.161092e+06 4.326800 2860.157458 0.145008 0.488364 min 2.800000e+01 2000.000000 100.000000 0.510000 0.000000 25% 2.324856e+06 2003.000000 2530.000000 0.630000 0.000000 50% 5.981915e+06 2007.000000 5010.000000 0.750000 0.000000 75% 5.990992e+06 2011.000000 7490.000000 0.880000 1.000000 max 6.000000e+06 2014.000000 10000.000000 1.000000 1.000000 knowledge_train_data knowledge_train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID ä¸“åˆ© å•†æ ‡ è‘—ä½œæƒ 0 28 0.0 1.0 1.0 1 230 0.0 0.0 0.0 2 693 0.0 0.0 0.0 3 990 0.0 0.0 0.0 4 1274 0.0 0.0 0.0 knowledge_train_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID ä¸“åˆ© å•†æ ‡ è‘—ä½œæƒ count 2.851900e+04 28233.000000 28216.00000 28237.000000 mean 4.332423e+06 0.342507 0.36334 0.371428 std 2.161092e+06 0.474557 0.48097 0.483195 min 2.800000e+01 0.000000 0.00000 0.000000 25% 2.324856e+06 0.000000 0.00000 0.000000 50% 5.981915e+06 0.000000 0.00000 0.000000 75% 5.990992e+06 1.000000 1.00000 1.000000 max 6.000000e+06 1.000000 1.00000 1.000000 åˆå¹¶ base data å’Œ knowledge data combined_base_knowledge_train_data = combined_base_knowledge_data( base_train_data, knowledge_train_data ) combined_base_knowledge_train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } æ³¨å†Œæ—¶é—´ æ³¨å†Œèµ„æœ¬ è¡Œä¸š åŒºåŸŸ ä¼ä¸šç±»å‹ æ§åˆ¶äººç±»å‹ æ§åˆ¶äººæŒè‚¡æ¯”ä¾‹ flag ä¸“åˆ© å•†æ ‡ è‘—ä½œæƒ ID 5986361 2014.0 7090.0 æœåŠ¡ä¸š æ¹–åŒ— æœ‰é™è´£ä»»å…¬å¸ è‡ªç„¶äºº 0.93 0 0.0 0.0 0.0 5991749 2007.0 5940.0 é›¶å”®ä¸š æ¹–å— åˆä¼™ä¼ä¸š ä¼ä¸šæ³•äºº 0.57 0 1.0 1.0 0.0 5998154 2002.0 9720.0 å·¥ä¸š ç¦å»º åˆä¼™ä¼ä¸š è‡ªç„¶äºº 0.74 0 1.0 1.0 0.0 5984390 2000.0 4800.0 å•†ä¸šæœåŠ¡ä¸š å±±ä¸œ è‚¡ä»½æœ‰é™å…¬å¸ NaN 0.90 0 0.0 0.0 0.0 5980535 2004.0 4530.0 é›¶å”®ä¸š å¹¿ä¸œ å†œæ°‘ä¸“ä¸šåˆä½œç¤¾ è‡ªç„¶äºº 0.95 0 0.0 1.0 1.0 æŸ¥çœ‹ year data å’Œ money dataæ•°æ®é›†ä¿¡æ¯ year_train_data year_train_data.head(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year ä»ä¸šäººæ•° èµ„äº§æ€»é¢ è´Ÿå€ºæ€»é¢ è¥ä¸šæ€»æ”¶å…¥ ä¸»è¥ä¸šåŠ¡æ”¶å…¥ åˆ©æ¶¦æ€»é¢ çº³ç¨æ€»é¢ æ‰€æœ‰è€…æƒç›Šåˆè®¡ 0 28 2015.0 794.0 16400.0 28700.0 72160.0 28864.0 7216.0 0.0 -12300.0 1 230 2015.0 485.0 23520.0 10080.0 115248.0 57624.0 57624.0 0.0 13440.0 2 693 2015.0 534.0 133760.0 125400.0 655424.0 262169.6 196627.2 0.0 8360.0 3 990 2015.0 863.0 33760.0 25320.0 145168.0 58067.2 14516.8 0.0 8440.0 4 1274 2015.0 254.0 74900.0 104325.0 277130.0 110852.0 55426.0 0.0 -29425.0 5 1560 2015.0 491.0 105000.0 98000.0 147000.0 73500.0 29400.0 0.0 7000.0 6 3261 2015.0 799.0 417000.0 822880.0 1751400.0 1401120.0 350280.0 0.0 -405880.0 7 3313 2015.0 784.0 501600.0 986480.0 2156880.0 1294128.0 431376.0 0.0 -484880.0 8 3537 2015.0 647.0 17800.0 13350.0 8900.0 4450.0 2670.0 0.0 4450.0 9 3719 2015.0 369.0 317000.0 465990.0 380400.0 228240.0 152160.0 0.0 -148990.0 year_train_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year ä»ä¸šäººæ•° ... åˆ©æ¶¦æ€»é¢ çº³ç¨æ€»é¢ æ‰€æœ‰è€…æƒç›Šåˆè®¡ count 8.554800e+04 84692.000000 84743.000000 ... 8.469900e+04 8.473100e+04 84673.000000 mean 4.332626e+06 2015.999870 510.808421 ... 1.027939e+05 7.079659e+04 -27390.880033 std 2.160933e+06 0.816398 283.129690 ... 1.536672e+05 1.588261e+05 108355.730296 min 2.800000e+01 2015.000000 20.000000 ... 7.800000e+00 0.000000e+00 -828340.000000 25% 2.325192e+06 2015.000000 266.000000 ... 1.396755e+04 0.000000e+00 -53130.000000 50% 5.981916e+06 2016.000000 512.000000 ... 4.514400e+04 1.240200e+03 250.000000 75% 5.990992e+06 2017.000000 756.000000 ... 1.238400e+05 6.668040e+04 8900.000000 max 5.999999e+06 2017.000000 1000.000000 ... 1.807398e+06 2.089620e+06 429570.000000 year_train_data money_train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year å€ºæƒèèµ„é¢åº¦ å€ºæƒèèµ„æˆæœ¬ è‚¡æƒèèµ„é¢åº¦ è‚¡æƒèèµ„æˆæœ¬ å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„é¢åº¦ å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„æˆæœ¬ é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„é¢åº¦ é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„æˆæœ¬ 0 28 2015.0 0.0 0.0 0.0 0.000 21648.0 1298.88 0.0 0.000 1 230 2015.0 0.0 0.0 0.0 0.000 0.0 0.00 470.4 28.224 2 693 2015.0 0.0 0.0 0.0 0.000 0.0 0.00 5350.4 321.024 3 990 NaN 0.0 0.0 0.0 0.000 0.0 0.00 675.2 40.512 4 1274 2015.0 0.0 0.0 11085.2 443.408 0.0 0.00 NaN 0.000 money_train_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year å€ºæƒèèµ„é¢åº¦ ... å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„æˆæœ¬ é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„é¢åº¦ é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„æˆæœ¬ count 8.554800e+04 84703.000000 84739.000000 ... 84720.000000 84686.000000 84695.000000 mean 4.332626e+06 2016.000378 3353.349261 ... 1555.894230 1020.851124 61.231978 std 2.160933e+06 0.816496 8883.814614 ... 4811.138407 3000.062130 179.871750 min 2.800000e+01 2015.000000 0.000000 ... 0.000000 0.000000 0.000000 25% 2.325192e+06 2015.000000 0.000000 ... 0.000000 0.000000 0.000000 50% 5.981916e+06 2016.000000 0.000000 ... 0.000000 0.000000 0.000000 75% 5.990992e+06 2017.000000 0.000000 ... 10.719000 41.000000 2.520000 max 5.999999e+06 2017.000000 84830.000000 ... 72925.920000 39720.000000 2383.200000 å¡«å……ç¼ºå¤±æ•°æ® year_train_data # fill null new_year_train_data = fill_na(year_train_data) new_year_train_data.set_index([&#39;ID&#39;, &#39;year&#39;]) new_year_train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year ä»ä¸šäººæ•° èµ„äº§æ€»é¢ è´Ÿå€ºæ€»é¢ è¥ä¸šæ€»æ”¶å…¥ ä¸»è¥ä¸šåŠ¡æ”¶å…¥ åˆ©æ¶¦æ€»é¢ çº³ç¨æ€»é¢ æ‰€æœ‰è€…æƒç›Šåˆè®¡ 0 28 2015.0 794.0 16400.0 28700.0 72160.0 28864.0 7216.0 0.0 -12300.0 1 230 2015.0 485.0 23520.0 10080.0 115248.0 57624.0 57624.0 0.0 13440.0 2 693 2015.0 534.0 133760.0 125400.0 655424.0 262169.6 196627.2 0.0 8360.0 3 990 2015.0 863.0 33760.0 25320.0 145168.0 58067.2 14516.8 0.0 8440.0 4 1274 2015.0 254.0 74900.0 104325.0 277130.0 110852.0 55426.0 0.0 -29425.0 money_train_data # fill null new_money_train_data = fill_na(money_train_data) new_money_train_data.set_index([&#39;ID&#39;, &#39;year&#39;]) # new_money_train_data.head(8) new_money_train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year å€ºæƒèèµ„é¢åº¦ å€ºæƒèèµ„æˆæœ¬ è‚¡æƒèèµ„é¢åº¦ è‚¡æƒèèµ„æˆæœ¬ å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„é¢åº¦ å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„æˆæœ¬ é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„é¢åº¦ é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„æˆæœ¬ 0 28 2015.0 0.0 0.0 0.0 0.000 21648.0 1298.88 0.0 0.000 1 230 2015.0 0.0 0.0 0.0 0.000 0.0 0.00 470.4 28.224 2 693 2015.0 0.0 0.0 0.0 0.000 0.0 0.00 5350.4 321.024 3 990 2015.0 0.0 0.0 0.0 0.000 0.0 0.00 675.2 40.512 4 1274 2015.0 0.0 0.0 11085.2 443.408 0.0 0.00 0.0 0.000 åˆå¹¶year dataå’Œmoney dataæ•°æ®é›† # Merge new year and money data combined_new_year_money_train_data = combined_new_year_money_data( new_year_train_data, new_money_train_data ) combined_new_year_money_train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year ä»ä¸šäººæ•° èµ„äº§æ€»é¢ ... å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„é¢åº¦ å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„æˆæœ¬ é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„é¢åº¦ é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„æˆæœ¬ 0 28 2015.0 794.0 16400.0 ... 21648.0 1298.88 0.0 0.000 1 230 2015.0 485.0 23520.0 ... 0.0 0.00 470.4 28.224 2 693 2015.0 534.0 133760.0 ... 0.0 0.00 5350.4 321.024 3 990 2015.0 863.0 33760.0 ... 0.0 0.00 675.2 40.512 4 1274 2015.0 254.0 74900.0 ... 0.0 0.00 0.0 0.000 æ‹†åˆ†æ–°ç‰¹å¾ å°†åˆå¹¶åçš„æ•°æ®é›†ä¸­æ¯ä¸€å¹´çš„æ•°æ®æ‹†åˆ†æˆæ–°çš„ç‰¹å¾ï¼Œä½¿ä¹‹ä¸base dataé€šè¿‡IDä¸€ä¸€å¯¹åº”ã€‚ splited_year_money_train_data = split_data(combined_new_year_money_train_data) splited_year_money_train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ä»ä¸šäººæ•°_2015 èµ„äº§æ€»é¢_2015 è´Ÿå€ºæ€»é¢_2015 ... å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„é¢åº¦_2017 å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„æˆæœ¬_2017 é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„é¢åº¦_2017 é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„æˆæœ¬_2017 ID 28 794.0 16400.0 28700.0 ... 0.0 0.000 0.0 0.0 230 485.0 23520.0 10080.0 ... 0.0 0.000 0.0 0.0 693 534.0 133760.0 125400.0 ... 0.0 0.000 0.0 0.0 990 863.0 33760.0 25320.0 ... 111661.2 6699.672 0.0 0.0 1274 254.0 74900.0 104325.0 ... 0.0 0.000 0.0 0.0 5 rows Ã— 48 columns åˆå¹¶å¤„ç†å¥½çš„æ•°æ®é›† train = pd.merge(combined_base_knowledge_train_data, splited_year_money_train_data, on=[&#39;ID&#39;]) train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } æ³¨å†Œæ—¶é—´ æ³¨å†Œèµ„æœ¬ è¡Œä¸š ... å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„é¢åº¦_2017 å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„æˆæœ¬_2017 é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„é¢åº¦_2017 é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„æˆæœ¬_2017 ID 5986361 2014.0 7090.0 æœåŠ¡ä¸š ... 0.0 0.0 0.0 0.0 5991749 2007.0 5940.0 é›¶å”®ä¸š ... 80190.0 4811.4 0.0 0.0 5998154 2002.0 9720.0 å·¥ä¸š ... 0.0 0.0 0.0 0.0 5984390 2000.0 4800.0 å•†ä¸šæœåŠ¡ä¸š ... 48960.0 2937.6 0.0 0.0 5980535 2004.0 4530.0 é›¶å”®ä¸š ... 0.0 0.0 0.0 0.0 5 rows Ã— 59 columns ä¿å­˜å¤„ç†å¥½çš„è®­ç»ƒæ•°æ®é›† train.to_csv(&quot;data/train/train.csv&quot;) train.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } æ³¨å†Œæ—¶é—´ æ³¨å†Œèµ„æœ¬ æ§åˆ¶äººæŒè‚¡æ¯”ä¾‹ ... å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„é¢åº¦_2017 å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„æˆæœ¬_2017 é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„é¢åº¦_2017 é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„æˆæœ¬_2017 count 28516.000000 28516.000000 28516.000000 ... 2.851600e+04 28516.000000 28516.000000 28516.000000 mean 2007.010836 5024.064385 0.754743 ... 2.850957e+04 1712.928322 1115.153493 67.013075 std 4.304831 2844.938780 0.144249 ... 8.651585e+04 5202.890113 3229.079861 193.992216 min 2000.000000 100.000000 0.510000 ... 0.000000e+00 0.000000 0.000000 0.000000 25% 2003.000000 2560.000000 0.630000 ... 0.000000e+00 0.000000 0.000000 0.000000 50% 2007.000000 5010.000000 0.750000 ... 0.000000e+00 0.000000 0.000000 0.000000 75% 2011.000000 7470.000000 0.880000 ... 0.000000e+00 0.000000 0.000000 0.000000 max 2014.000000 10000.000000 1.000000 ... 1.215432e+06 72925.920000 38930.000000 2335.800000 8 rows Ã— 55 columns è®­ç»ƒæ•°æ®é›†å¤„ç† æŒ‰ç…§åˆšåˆšå¤„ç†è®­ç»ƒé›†çš„æµç¨‹å¤„ç†æµ‹è¯•æ•°æ®é›†ã€‚ ### åŠ è½½åŸå§‹æ•°æ®é›† # load testing data base_test_data = pd.read_csv(&quot;data/test/base-test.csv&quot;) year_test_data = pd.read_csv(&quot;data/test/year-test.csv&quot;) knowledge_test_data = pd.read_csv(&quot;data/test/knowledge-test.csv&quot;) money_test_data = pd.read_csv(&quot;data/test/money-test.csv&quot;) åˆå¹¶ base data å’Œ knowledge data combined_base_knowledge_test_data = combined_base_knowledge_data( base_test_data, knowledge_test_data ) combined_base_knowledge_test_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } æ³¨å†Œæ—¶é—´ æ³¨å†Œèµ„æœ¬ æ§åˆ¶äººæŒè‚¡æ¯”ä¾‹ ä¸“åˆ© å•†æ ‡ è‘—ä½œæƒ count 7132.000000 7132.000000 7132.000000 7132.000000 7132.000000 7132.000000 mean 2007.077257 5039.914470 0.754799 0.342821 0.358385 0.373528 std 4.321929 2839.476208 0.143953 0.474686 0.479560 0.483774 min 2000.000000 100.000000 0.510000 0.000000 0.000000 0.000000 25% 2003.000000 2640.000000 0.630000 0.000000 0.000000 0.000000 50% 2007.000000 5040.000000 0.750000 0.000000 0.000000 0.000000 75% 2011.000000 7450.000000 0.880000 1.000000 1.000000 1.000000 max 2014.000000 10000.000000 1.000000 1.000000 1.000000 1.000000 year data and money data å¡«å……ç¼ºå¤±å€¼å¹¶éªŒè¯å¡«å……ç»“æœ year_test_data # fill null new_year_test_data = fill_na(year_test_data) new_year_test_data.set_index([&#39;ID&#39;, &#39;year&#39;]) new_year_test_data.describe() print(&quot;unique ID count in base data:&quot;, base_test_data[&quot;ID&quot;] .nunique()) print(&quot;2015 unique ID count in year data:&quot;, new_year_test_data[&quot;ID&quot;] .loc[new_year_test_data.year==2015] .nunique()) print(&quot;2016 unique ID count in year data:&quot;, new_year_test_data[&quot;ID&quot;] .loc[new_year_test_data.year==2016] .nunique()) print(&quot;2017 unique ID count in year data:&quot;, new_year_test_data[&quot;ID&quot;] .loc[new_year_test_data.year==2017] .nunique()) unique ID count in base data: 7132 2015 unique ID count in year data: 7132 2016 unique ID count in year data: 7132 2017 unique ID count in year data: 7132 money_test_data # fill null new_money_test_data = fill_na(money_test_data) new_money_test_data.set_index([&#39;ID&#39;, &#39;year&#39;]) # new_money_train_data.head(8) new_money_test_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year å€ºæƒèèµ„é¢åº¦ ... å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„é¢åº¦ å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„æˆæœ¬ é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„é¢åº¦ é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„æˆæœ¬ count 2.139600e+04 21396.000000 21396.000000 ... 2.139600e+04 21396.000000 21396.000000 21396.000000 mean 4.332655e+06 2016.000000 3209.631146 ... 2.677436e+04 1598.629057 1025.091606 61.698997 std 2.163020e+06 0.816516 8711.231069 ... 8.272665e+04 4950.288392 3050.157292 183.387577 min 4.290000e+02 2015.000000 0.000000 ... 0.000000e+00 0.000000 0.000000 0.000000 25% 2.331607e+06 2015.000000 0.000000 ... 0.000000e+00 0.000000 0.000000 0.000000 50% 5.981952e+06 2016.000000 0.000000 ... 0.000000e+00 0.000000 0.000000 0.000000 75% 5.990780e+06 2017.000000 0.000000 ... 2.565000e+02 0.850500 0.000000 0.000000 max 5.999998e+06 2017.000000 ... 1.257150e+06 75429.000000 40970.000000 2458.200000 print(&quot;unique ID count in base data:&quot;, base_test_data[&quot;ID&quot;].nunique()) print(&quot;2015 unique ID count in money data:&quot;, new_money_test_data[&quot;ID&quot;] .loc[new_money_test_data.year==2015] .nunique()) print(&quot;2016 unique ID count in money data:&quot;, new_money_test_data[&quot;ID&quot;] .loc[new_money_test_data.year==2016] .nunique()) print(&quot;2017 unique ID count in money data:&quot;, new_money_test_data[&quot;ID&quot;] .loc[new_money_test_data.year==2017] .nunique()) unique ID count in base data: 7132 2015 unique ID count in money data: 7132 2016 unique ID count in money data: 7132 2017 unique ID count in money data: 7132 åˆå¹¶ year data å’Œ money data combined_new_year_money_test_data = combined_new_year_money_data( new_year_test_data, new_money_test_data ) combined_new_year_money_test_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID year ä»ä¸šäººæ•° ... å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„é¢åº¦ å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„æˆæœ¬ é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„é¢åº¦ é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„æˆæœ¬ 0 429 2015.0 136.0 ... 0.0 0.000 0.0 0.00 1 727 2015.0 375.0 ... 0.0 0.000 0.0 0.00 2 1137 2015.0 289.0 ... 24460.8 1467.648 0.0 0.00 3 1873 2015.0 889.0 ... 0.0 0.000 0.0 0.00 4 2260 2015.0 689.0 ... 0.0 0.000 11287.5 677.25 print(&quot;unique ID count in combined data:&quot;, combined_new_year_money_test_data[&quot;ID&quot;].nunique()) unique ID count in combined data: 7132 æ‹†åˆ†æ–°ç‰¹å¾ splited_year_money_test_data = split_data(combined_new_year_money_test_data) splited_year_money_test_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ä»ä¸šäººæ•°_2015 èµ„äº§æ€»é¢_2015 è´Ÿå€ºæ€»é¢_2015 ... å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„é¢åº¦_2017 å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„æˆæœ¬_2017 é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„é¢åº¦_2017 é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„æˆæœ¬_2017 ID 429 136.0 193400.0 183730.0 ... 0.0 0.000 0.0 0.00 727 375.0 366240.0 536280.0 ... 0.0 0.000 0.0 0.00 1137 289.0 87200.0 40320.0 ... 0.0 0.000 1008.0 60.48 1873 889.0 229320.0 222950.0 ... 12612.6 756.756 0.0 0.00 2260 689.0 225750.0 325080.0 ... 0.0 0.000 0.0 0.00 5 rows Ã— 48 columns åˆå¹¶å¤„ç†å¥½çš„æ•°æ®é›† æ ¹æ®IDåˆå¹¶å¤„ç†å¥½çš„æ•°æ®é›†ã€‚ test = pd.merge(combined_base_knowledge_test_data, splited_year_money_test_data, on=[&#39;ID&#39;]) test.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } æ³¨å†Œæ—¶é—´ æ³¨å†Œèµ„æœ¬ è¡Œä¸š ... å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„é¢åº¦_2017 å†…éƒ¨èèµ„å’Œè´¸æ˜“èèµ„æˆæœ¬_2017 é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„é¢åº¦_2017 é¡¹ç›®èèµ„å’Œæ”¿ç­–èèµ„æˆæœ¬_2017 ID 5991927 2010.0 8790.0 å·¥ä¸š ... 341491.5 20489.49 0.0 0.000 5998351 2005.0 270.0 æœåŠ¡ä¸š ... 0.0 0.00 194.4 11.664 5992703 2012.0 230.0 æœåŠ¡ä¸š ... 0.0 0.00 0.0 0.000 5979231 2003.0 5980.0 å•†ä¸šæœåŠ¡ä¸š ... 75348.0 4520.88 0.0 0.000 5995422 2007.0 160.0 å·¥ä¸š ... 8856.0 531.36 0.0 0.000 5 rows Ã— 58 columns ä¿å­˜å¤„ç†å¥½çš„æµ‹è¯•æ•°æ®é›† test.to_csv(&quot;data/test/test.csv&quot;) æ€»ç»“ è¿™æ¬¡æ•°æ®å¤„ç†æ¯”è¾ƒç®€å•ï¼Œä¸»è¦æ³¨æ„ä»¥ä¸‹å‡ ä¸ªç‚¹ï¼š 1. å¯¹äºå¹´ä»½æ•°æ®ç¼ºå¤±å€¼çš„å¤„ç†ï¼Œå¯ä»¥æ ¹æ®IDå’Œå¹´ä»½åˆ¤æ–­ç¼ºå¤±çš„å¹´ä»½åº”è¯¥æ˜¯å“ªä¸€å¹´ï¼Œç„¶åå¡«å……ç›¸åº”çš„å¹´ä»½å€¼å³å¯ï¼Œå¦‚æœæ˜¯åŒä¸€IDç¼ºå¤±ä¸¤å¹´æ•°æ®ï¼Œè¿™ä¸ªå°±éšç¼˜äº†ğŸ˜†ï¼Œæˆ‘æ˜¯æŒ‰ç…§ä»ä¸Šåˆ°ä¸‹çš„é¡ºåºå¡«å……çš„ï¼ˆå¯èƒ½æœ‰æ›´å¥½çš„æ–¹æ³•ï¼Œå¤§å®¶å¯ä»¥æå‡ºè‡ªå·±çš„æƒ³æ³•ï¼‰ï¼Œä¾‹å¦‚IDä¸º123çš„ä¼ä¸šç¼ºå¤±2015å’Œ2017çš„å¹´ä»½ï¼Œé‚£ä¹ˆä¾æ¬¡å°†yearå±æ€§å€¼å¡«å……ä¸º2015å’Œ2017ï¼› 2. å¯¹äºå…¶ä»–æ•°æ®æˆ‘å°±ç›´æ¥ç²—æš´çš„å¡«å……ä¸­ä½æ•°ï¼Œæœ¬æ¥æ˜¯æƒ³å…ˆè¿™ä¹ˆå¡«å……ç„¶åçœ‹çœ‹æ•ˆæœå†ä¼˜åŒ–ï¼Œç»“æœè®­ç»ƒæµ‹è¯•åçš„ç»“æœè¿˜ä¸é”™ï¼Œæ‰€ä»¥ä¹Ÿå°±æ²¡æœ‰å†ä¼˜åŒ–äº†ï¼› ä¸‹ç¯‡æ–‡ç« ä¸­æˆ‘ä¼šå¯¹æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä½œä¸€ä¸ªè¯´æ˜ï¼Œä½¿ç”¨çš„æ˜¯LightGBMæ¡†æ¶ï¼ˆLightGBMæ˜¯ä½¿ç”¨åŸºäºæ ‘çš„å­¦ä¹ ç®—æ³•çš„æ¢¯åº¦å¢å¼ºæ¡†æ¶ï¼‰ã€‚","categories":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Machine Learning","slug":"Technology/Machine-Learning","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/"},{"name":"Data pre-processing","slug":"Technology/Machine-Learning/Data-pre-processing","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/Data-pre-processing/"}],"tags":[{"name":"Data pre-processing","slug":"Data-pre-processing","permalink":"https://littlelittlemoon.github.io/tags/Data-pre-processing/"},{"name":"Categorical Data","slug":"Categorical-Data","permalink":"https://littlelittlemoon.github.io/tags/Categorical-Data/"},{"name":"Missing value","slug":"Missing-value","permalink":"https://littlelittlemoon.github.io/tags/Missing-value/"}],"keywords":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Machine Learning","slug":"Technology/Machine-Learning","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/"},{"name":"Data pre-processing","slug":"Technology/Machine-Learning/Data-pre-processing","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/Data-pre-processing/"}]},{"title":"Leetcode Note in Apri 2020 | part 2 |day 08-14","slug":"Leetcode-Note-in-Apri-2020-part-2-day-08-14","date":"2020-04-08T05:54:16.000Z","updated":"2020-04-11T16:26:06.065Z","comments":true,"path":"2020/04/08/Leetcode-Note-in-Apri-2020-part-2-day-08-14/","link":"","permalink":"https://littlelittlemoon.github.io/2020/04/08/Leetcode-Note-in-Apri-2020-part-2-day-08-14/","excerpt":"","text":"å†™åœ¨å‰é¢çš„è¯ ç¬¬äºŒæ¬¡æ›´æ–°ï¼Œç»§ç»­å‰è¿›~ Backspace String Compare Given two strings S and T, return if they are equal when both are typed into empty text editors. # means a backspace character. Examples: &gt; Input: S = \"ab#c\", T = \"ad#c\" Output: true Explanation: Both S and T become \"ac\". &gt; Input: S = \"ab##\", T = \"c#d#\" Output: true Explanation: Both S and T become \"\". &gt; Input: S = \"a##c\", T = \"#a#c\" Output: true Explanation: Both S and T become \"c\". &gt; Input: S = \"a#c\", T = \"b\" Output: false Explanation: S becomes \"c\" while T becomes \"b\". Solution è¿™ä¸ªé¢˜å¯¹äºæˆ‘æ¥è¯´æœ‰ç‚¹éš¾åº¦ï¼Œæ˜¨å¤©(9å·)æ™šä¸Šåšäº†ä¸¤ä¸ªå°æ—¶å§ï¼Œé‡ç‚¹æ˜¯é€šä¸è¿‡æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹ï¼Œæ€»æ˜¯æœ‰ä¸€äº›ç»†èŠ‚æ²¡æœ‰è€ƒè™‘åˆ°ï¼Œè€Œä¸”ä»£ç è¶Šå†™è¶Šå¤æ‚ï¼ŒåŠ ä¸Šæ˜¨å¤©ä¸­åˆæ²¡ç¡åˆè§‰ï¼Œåˆ°æœ€åè‡ªå·±æŠŠè‡ªå·±éƒ½ææ‡µäº†ï¼Œæ°”ğŸ˜ ã€‚åé¢ç¡è§‰çš„æ—¶å€™å‘ç°è‡ªå·±è„‘å­çœŸçš„æ˜¯ä¸€æ ¹ç­‹ï¼Œæ¢ä¸ªæ€è·¯è¿™é¢˜ä¼šç®€å•å¾ˆå¤šã€‚å…ˆçœ‹çœ‹ä»£ç å§ã€‚ Mine æ ¸å¿ƒæ€è·¯ï¼šä»åå¾€å‰æ‰¾å¯èƒ½ç•™ä¸‹æ¥çš„å­—ç¬¦ï¼Œå¹¶ä¸€ä¸€æ¯”å¯¹ä¸¤ä¸ªå­—ç¬¦ä¸²å¯¹åº”ä½ç½®èƒ½ç•™ä¸‹æ¥çš„çš„å­—ç¬¦æ˜¯å¦ç›¸ç­‰ã€‚æ³¨æ„å¾ªç¯åˆ¤æ–­æ¡ä»¶çš„è®¾ç½®ï¼ class Solution: def backspaceCompare(self, S: str, T: str) -&gt; bool: s_i = len(S) - 1 t_i = len(T) - 1 s_back = t_back = 0 while True: # Loop will be stoped when any of the following situations occur: # 1. s_i completed the last character access # 2. S[s_i] is a letter (S[s_i] != &#39;#&#39;) and don&#39;t need to go back # currently (s_back = 0) # Same as T, t_i and t_back while s_i &gt;= 0: if S[s_i] == &#39;#&#39;: s_i, s_back = s_i - 1, s_back + 1 elif S[s_i] != &#39;#&#39; and s_back &gt; 0: s_i, s_back = s_i - 1, s_back - 1 else: break while t_i &gt;= 0: if T[t_i] == &#39;#&#39;: t_i, t_back = t_i - 1, t_back + 1 elif T[t_i] != &#39;#&#39; and t_back &gt; 0: t_i, t_back = t_i - 1, t_back - 1 else: break # There are only two situations will return True: # 1. both s_i and t_i are not out of index and S[s_i] = T[t_i] # 2. both s_i and t_i are out of index, it means no letter needs # to be compared and previous corresponding letters are equal. # otherwise, it will return false in situation 1 if s_i &gt;= 0 and t_i &gt;= 0: if S[s_i] != T[t_i]: return False s_i, t_i = s_i - 1, t_i - 1 else: return s_i &lt; 0 and t_i &lt; 0 å…¶å®è¿˜æ˜¯è›®å¤æ‚çš„ï¼Œå¯èƒ½æœ‰æ›´å¥½çš„è§£å†³æ–¹æ³•ï¼Œå¸Œæœ›å°ä¼™ä¼´ä»¬ç»™å‡ºä½ ä»¬çš„æ€è·¯å’Œå»ºè®®ã€‚æˆ‘åˆšå¼€å§‹çš„æ€è·¯æ˜¯ä»å‰å¾€åå»æ‰¾å¯èƒ½ç•™ä¸‹çš„å­—æ¯ï¼Œä½†æ˜¯è¿™ä¸ªå˜åŒ–å¤ªå¤§äº†ï¼Œè€Œä¸”åˆ¤æ–­æ¡ä»¶ç‰¹åˆ«å¤šï¼Œæ€»æ˜¯æœ‰å¾ˆå¤šæƒ…å†µè€ƒè™‘ä¸åˆ°ï¼Œæ•´äº†ä¸€æ™šä¸Šä¹Ÿæ²¡æ•´å‡ºæ¥ã€‚åé¢ç¡è§‰çš„æ—¶å€™çªç„¶æƒ³èµ·æˆ‘ä¸ºå•¥ä¸ä»åå¾€å‰æ‰¾å‘¢ï¼ŸğŸ¤£å…·ä½“è§£æ³•éƒ½åœ¨æ³¨é‡Šé‡Œï¼Œemmmmï¼Œå¯èƒ½è¿˜æœ‰è¯­æ³•é”™è¯¯ï¼Œå‹¿ä»‹ğŸ˜Šã€‚ Min Stack Design a stack that supports push, pop, top, and retrieving the minimum element in constant time. push(x) -- Push element x onto stack. pop() -- Removes the element on top of the stack. top() -- Get the top element. getMin() -- Retrieve the minimum element in the stack. Example: &gt; MinStack minStack = new MinStack(); minStack.push(-2); minStack.push(0); minStack.push(-3); minStack.getMin(); --&gt; Returns -3. minStack.pop(); minStack.top(); --&gt; Returns 0. minStack.getMin(); --&gt; Returns -2. Solution è¿™ä¸ªé¢˜ä¸éš¾ï¼Œå¹¶ä¸”æœ‰å¾ˆå¤šä¸­å®ç°æ–¹å¼ã€‚æˆ‘ä»¬éœ€è¦å®ç°çš„è¿™ä¸ªæ ˆæ˜¯åœ¨ä¼ ç»Ÿçš„æ ˆå·²æœ‰çš„åŠŸèƒ½ä¸Šå¢åŠ äº†è¿”å›æœ€å°å€¼çš„åŠŸèƒ½ï¼Œè¿™å°±æ¶‰åŠåˆ°æ€ä¹ˆå­˜å‚¨æœ€å°å€¼çš„é—®é¢˜ï¼Œè¿™é‡Œæœ‰ä¸‰ä¸ªæ€è·¯ï¼Œå¤§å®¶æœ‰æ›´å¥½çš„æ€è·¯ä¹Ÿå¯ä»¥åœ¨è¯„è®ºåŒºç»™å‡ºï¼š æ—¶é—´æ¢ç©ºé—´ åªè®¾è®¡ä¸€ä¸ªæ ˆï¼Œè¿”å›æœ€å°å€¼æ—¶é€šè¿‡æ’åºæ–¹å¼è¿”å›ã€‚ class MinStack: def __init__(self): &quot;&quot;&quot; initialize your data structure here. &quot;&quot;&quot; self.stack = [] def push(self, x: int) -&gt; None: self.stack.append(x) def pop(self) -&gt; None: self.stack.pop() def top(self) -&gt; int: if len(self.stack) &lt; 0: return None return self.stack[len(self.stack)-1] def getMin(self) -&gt; int: if len(self.stack) &lt; 0: return None s_sorted = sorted(self.stack) return s_sorted[0] æ›´èŠ‚çœæ—¶é—´å’Œç©ºé—´çš„æ–¹å¼(æ›´å·§å¦™) è®¾è®¡ä¸€ä¸ªæ ˆï¼Œä¸€ä¸ªminå˜é‡ï¼Œä½†æ˜¯æ ˆé‡Œé¢åªå­˜æ”¾æ¯ä¸ªå…ƒç´ ä¸æœ€å°å€¼çš„å·®å€¼ï¼ˆx-minï¼‰ï¼Œå½“å‡ºç°æ›´å°çš„æœ€å°å€¼æ—¶ï¼Œä¿å­˜çš„æ˜¯è´Ÿæ•°ï¼Œæ•…è¯¾é€šè¿‡è¿™ä¸ªæ¡ä»¶è¿½æº¯ä¹‹å‰çš„æœ€å°å€¼ï¼Œå…·ä½“å®ç°è§ä¸‹é¢çš„ä»£ç ã€‚ class MinStack: def __init__(self): &quot;&quot;&quot; initialize your data structure here. &quot;&quot;&quot; self.stack = [] self.s_min = None def push(self, x: int) -&gt; None: if self.s_min == None: self.s_min = x self.stack.append(x-self.s_min) self.s_min = min(self.s_min, x) def pop(self) -&gt; None: pop = self.stack.pop() if len(self.stack) &gt; 0: if pop &lt; 0: self.s_min = self.s_min - pop else: self.s_min = None def top(self) -&gt; int: if len(self.stack) &lt; 0: return None top = self.stack[len(self.stack)-1] if top &lt; 0: return self.s_min return top + self.s_min def getMin(self) -&gt; int: return self.s_min ç©ºé—´æ¢æ—¶é—´ è®¾è®¡ä¸¤ä¸ªæ ˆï¼Œå¤šå‡ºæ¥çš„é‚£ä¸ªæ ˆç”¨æ¥è®°å½•å‡ºç°è¿‡çš„æœ€å°å€¼ï¼Œæˆ‘è¿™é‡Œæ²¡æœ‰å»å®ç°äº†ï¼Œå¤§å®¶å¯ä»¥è¯•è¯•ã€‚ Diameter of Binary Tree Given a binary tree, you need to compute the length of the diameter of the tree. The diameter of a binary tree is the length of the longest path between any two nodes in a tree. This path may or may not pass through the root. Example: Given a binary tree Return 3, which is the length of the path [4,2,1,3] or [5,2,1,3]. Note: The length of path between two nodes is represented by the number of edges between them. Solution æ ¸å¿ƒæ€è·¯ï¼šæ ¹æ®åŠå¾„çš„å®šä¹‰å¯çŸ¥ï¼ŒåŠå¾„æ‰€åœ¨çš„è·¯å¾„ä¸€å®šæœ‰ä¸€ä¸ªä¸­å¿ƒèŠ‚ç‚¹ï¼Œæ•…å¯é€šè¿‡åˆ¤æ–­äºŒå‰æ ‘æ¯ä¸ªèŠ‚ç‚¹ï¼ˆå½“å‰å¯èƒ½çš„ä¸­å¿ƒç‚¹ï¼‰å·¦å³å­æ ‘çš„æ·±åº¦å’Œçš„å¤§å°æ¥ç¡®å®šåŠå¾„ã€‚ # Definition for a binary tree node. class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = None class Solution: def diameterOfBinaryTree(self, root: TreeNode) -&gt; int: self.diameter = 0 def count_depth(node): if not(node): return 0 l_dep, r_dep = count_depth(node.left), count_depth(node.right) self.diameter = max(self.diameter, l_dep+r_dep) # max depth: max(left depth, right depth) # +1: count node return 1 + max(l_dep, r_dep) count_depth(root) return self.diameter è¿™é¢˜åˆšå¼€å§‹æˆ‘æ˜¯æ²¡æœ‰ä»€ä¹ˆå¥½çš„æ€è·¯çš„ï¼Œå¤ªä¹…æ²¡åšå…³äºäºŒå‰æ ‘çš„é¢˜äº†ï¼Œä¸å¯¹ï¼Œå‡†ç¡®æ¥è®²åº”è¯¥æ˜¯è¯¾åå°±ä¸¢äº†ï¼Œå“ˆå“ˆå“ˆï¼Œåæ¥å»æŸ¥äº†ä¸‹äºŒå‰æ ‘çš„ç›¸å…³èµ„æ–™ï¼Œçœ‹åˆ°æ±‚äºŒå‰æ ‘çš„æ·±åº¦é—®é¢˜ç»™äº†æˆ‘çµæ„Ÿã€‚å¸Œæœ›èƒ½æŠŠä»¥å‰ä¸¢æ‰çš„çŸ¥è¯†ç‚¹æ…¢æ…¢æ¡èµ·æ¥ï¼Œå°±æ‹¿è¿™ä¸ªæ ‘ç›¸å…³çš„çŸ¥è¯†ç‚¹æ¥è¯´ï¼Œå…¶å®å¾ˆå¤šç®—æ³•åº•å±‚åŸç†éƒ½æœ‰æ¶‰åŠï¼Œä¸ºäº†ä»¥åèƒ½èµ°å¿«ä¸€ç‚¹ï¼Œç°åœ¨åº”è¯¥å¤šèµ°å‡ æ­¥ã€‚ Updating... æœªå®Œå¾…ç»­...","categories":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Coding","slug":"Technology/Coding","permalink":"https://littlelittlemoon.github.io/categories/Technology/Coding/"},{"name":"Leetcode","slug":"Technology/Coding/Leetcode","permalink":"https://littlelittlemoon.github.io/categories/Technology/Coding/Leetcode/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://littlelittlemoon.github.io/tags/Leetcode/"},{"name":"Python","slug":"Python","permalink":"https://littlelittlemoon.github.io/tags/Python/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://littlelittlemoon.github.io/tags/Algorithm/"}],"keywords":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Coding","slug":"Technology/Coding","permalink":"https://littlelittlemoon.github.io/categories/Technology/Coding/"},{"name":"Leetcode","slug":"Technology/Coding/Leetcode","permalink":"https://littlelittlemoon.github.io/categories/Technology/Coding/Leetcode/"}]},{"title":"Leetcode Note in Apri 2020 | part 1 | day 01-07","slug":"Leetcode Note","date":"2020-04-01T01:51:55.000Z","updated":"2020-04-11T16:16:26.754Z","comments":true,"path":"2020/04/01/Leetcode Note/","link":"","permalink":"https://littlelittlemoon.github.io/2020/04/01/Leetcode%20Note/","excerpt":"","text":"å†™åœ¨å‰é¢çš„è¯ è¿™ä¸€ç³»åˆ—æ–‡ç« ä¸»è¦æ˜¯ç”¨äºè®°å½•æˆ‘åœ¨LeetCodeä¸Šåˆ·é¢˜æ—¶é‡åˆ°çš„æˆ‘è®¤ä¸ºæ¯”è¾ƒæœ‰éš¾åº¦å’Œæœ‰æ„æ€çš„é¢˜ï¼Œä»¥åŠæ˜¯æŒ–æ˜åˆ°çš„å…¶ä»–å°ä¼™ä¼´æ¯”è¾ƒå¥½çš„è§£æ³•å’Œæ€è·¯ã€‚è¿™æ˜¯ç¬¬ä¸€ç¯‡ï¼Œä¸å‡ºæ„å¤–è¿™ä¸ªç³»åˆ—ä¼šä¸€ç›´æ›´æ–°ï¼Œä¹Ÿç®—æ˜¯ç£ä¿ƒè‡ªå·±åšæŒä¸‹å»å§ã€‚å¯¹äº†æˆ‘å†™ä½œæ°´å¹³çœŸçš„å¾ˆå·®ï¼Œå¸Œæœ›å¤§å®¶ä¸è¦å«Œå¼ƒï¼Œä¹Ÿå¯ä»¥å’Œæˆ‘ç•™è¨€ç»™å‡ºå»ºè®®ä»€ä¹ˆçš„ï¼ŒæœŸå¾…å’Œå¤§å®¶ä¸ªå…±åŒè¿›æ­¥ï¼é‚£æˆ‘ä»¬å¼€å§‹å§ ~ Sigle number Given a non-empty array of integers, every element appears twice except for one. Find that single one. Note: Your algorithm should have a linear runtime complexity. Could you implement it without using extra memory? Example: &gt; Input: [4,1,2,1,2] Output: 4 Solution Mine: tuple + list class Solution: def singleNumber(self, nums: List[int]) -&gt; int: # store the count of positive number p_count = [0] * (max(tuple(nums)) + 1) # store the positive number p_nums = () # store the count of negative number n_count = [0] * (abs(min(tuple(nums))) + 1) # store the negative number n_nums = () # counting for num in nums: if num &gt;= 0: p_nums = p_nums + (num, ) p_count[num] = p_count[num] + 1 else: n_nums = n_nums + (num, ) n_count[abs(num)] = n_count[abs(num)] + 1 # find the sigle number for num in p_nums: if p_count[num] == 1: return num for num in n_nums: if n_count[abs(num)] == 1: return num return None Space complexityï¼š O(n) Time complexityï¼š O(n) Optimization using XOR The most crucial trick here is to recognize that if you XOR any same number together, you cancel it out (=0). Explanation: nums = [2, 4, 5, 4, 3, 5, 2] XORing everything together = 2 ^ 4 ^ 5 ^ 4 ^ 3 ^ 5 ^ 2 = (2^2) ^ (4^4) ^ (5^5) ^ 3 = 0 ^ 0 ^0 ^ 3 = 3 class Solution: def singleNumber(self, nums: List[int]) -&gt; int: return reduce(lambda x, y: x^y, nums, 0) Space complexityï¼š O(1) time complexityï¼š O(n) Reference Reduce list(map(str, [1, 2, 3, 4, 5, 6, 7, 8, 9])) # output: [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;] Map reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4) # convert string to integer DIGITS = {&#39;0&#39;: 0, &#39;1&#39;: 1, &#39;2&#39;: 2, &#39;3&#39;: 3, &#39;4&#39;: 4, &#39;5&#39;: 5, &#39;6&#39;: 6, &#39;7&#39;: 7, &#39;8&#39;: 8, &#39;9&#39;: 9} def char2num(s): return DIGITS[s] def str2int(s): return reduce(lambda x, y: x * 10 + y, map(char2num, s)) Happy number A happy number is a number defined by the following process: Starting with any positive integer, replace the number by the sum of the squares of its digits, and repeat the process until the number equals 1 (where it will stay), or it loops endlessly in a cycle which does not include 1. Those numbers for which this process ends in 1 are happy numbers. Example: &gt; Input: 19 Output: true Explanation: \\(1^2 + 9^2 = 82\\) \\(8^2 + 2^2 = 68\\) \\(6^2 + 8^2 = 100\\) \\(1^2 + 0^2 + 0^2 = 1\\) Solution Floyd's cycle detection æ ¸å¿ƒæ€è·¯ï¼šå¦‚æœä¸€ä¸ªæ•°å­—ä¸æ˜¯ happy number é‚£ä¹ˆå®ƒæ‹†åˆ†åçš„æ•°å­—çš„å’Œä¸€å®šä¼šå¾ªç¯ï¼Œé€šè¿‡Floyd's cycle detectionç®—æ³•åšå¾ªç¯æ£€æµ‹ã€‚ Short Version utilizing walrus operator := class Solution: def isHappy(self, n: int) -&gt; bool: def next_num(num): return sum(map(lambda x:int(x)**2, str(num))) slow, fast = n, next_num(n) while (slow:=next_num(slow)) != (fast:=next_num(next_num(fast))) and fast != 1: continue return fast == 1 or not slow == fast Easier to understand version class Solution: def isHappy(self, n: int) -&gt; bool: def next_num(num): return sum(map(lambda x:int(x)**2, str(num))) slow, fast = n, next_num(n) while slow != fast and fast != 1: slow = next_num(slow) fast = next_num(next_num(fast)) return fast == 1 or not slow == fast Reference Algorithmï¼š Floyd's cycle detection What we need to do in case we need the starting point of the loop ? - Once we know for sure that a loop is present. - Move the slowPointer to start of the list,(i.e headNode) and let fastPointer remain there at the meeting point - Now move both the pointers one node at a time - The point where both pointers will meet, is our required start of the loop. The algorithm uses O(Î» + Î¼) operations of these types, and O(1) storage space. Detecting start of a loop in singly Linked List Floyd's Cycle detection algorithm | Determining the starting point of cycle Another solution class Solution: def isHappy(self, n: int) -&gt; bool: # let&#39;s try different n: # true (1) -&gt; 1 # false (2) -&gt; 4 -&gt; 16 -&gt; 37 -&gt; 58 -&gt; 89 -&gt; 145 -&gt; 42 -&gt; 20 -&gt; 4 # false (3) -&gt; 9 -&gt; 81 -&gt; 65 -&gt; 61 -&gt; 37 (look at 2) # false (4) -&gt; (look at 2) # false (5) -&gt; 25 -&gt; 29 -&gt; 85 -&gt; 89 (look at 2) # false (6) -&gt; 36 -&gt; 45 -&gt; 41 -&gt; 17 -&gt; 50 -&gt; 25 (look at 5) # true (7) -&gt; 49 -&gt; 97 -&gt; 10 # false (8) -&gt; 64 -&gt; 52 -&gt; 29 (look at 5) # false (9) -&gt; 9 -&gt; 81 -&gt; 65 (look at 3) # # All other n &gt;= 10, while computing will become [1-9], # So there are two cases 1 and 7 which are true. # # Notice, that all falses has the same path as 2 (loop). counting = 0 num = n while True: counting = 0 for str_num in str(num): counting = counting + pow(int(str_num), 2) if counting &gt;= 1 and counting &lt;=9: if counting == 1 or counting == 7: return True else: return False else: num = counting Group Anagrams Given an array of strings, group anagrams together. Example: &gt; Input: [\"eat\", \"tea\", \"tan\", \"ate\", \"nat\", \"bat\"], Output: [ [\"ate\",\"eat\",\"tea\"], [\"nat\",\"tan\"], [\"bat\"] ] My solutionï¼ˆbadï¼‰ class Solution: def groupAnagrams(self, strs: List[str]) -&gt; List[List[str]]: group_list = [] group_list.append([strs[0]]) for i in range(1, len(strs)): sorted_str = sorted(strs[i]) for j in range(0, len(group_list)): sorted_comp_str = sorted(group_list[j][0]) if sorted_str == sorted_comp_str: group_list[j].append(strs[i]) break if j == len(group_list) - 1: group_list.append([strs[i]]) break return group_list Result: Time Limit Exceeded Summary è¿™ä¸ªè§£æ³•çœ‹èµ·æ¥æ²¡ä»€ä¹ˆæ¯›ç—…ï¼Œå¦‚æœstrsé•¿åº¦å¾ˆå°ï¼Œä¹Ÿèƒ½æ­£å¸¸å·¥ä½œï¼Œä½†æ˜¯å½“å­—ç¬¦ä¸²æ•°ç»„strsé•¿åº¦ç‰¹åˆ«å¤§æ—¶ï¼Œ æ¯›ç—…å°±æš´éœ²å‡ºæ¥äº†ï¼Œé€Ÿåº¦å¾ˆæ…¢ï¼Œæ•ˆç‡ä½ï¼Œä¼šå‡ºç°è¶…æ—¶ï¼ˆTime Limit Exceededï¼‰æœªå®Œæˆçš„æƒ…å†µã€‚ Nice solution with dictionary and tuple class Solution: def groupAnagrams(self, strs: List[str]) -&gt; List[List[str]]: # create a null dictionary group_list = {} for s in strs: # sort the string&#39;s letters and save as a tuple key = tuple(sorted(s)) # search dictionary with the str&#39;s tuple and save the new value group_list[key] = group_list.get(key, []) + [s] return group_list.values() Summary Tuple can be used for the dictionary's key. wow~ Dictionary can speed up the search. Maximum Subarray Given an integer array nums, find the contiguous subarray (containing at least one number) which has the largest sum and return its sum. Example: &gt; Input: [-2,1,-3,4,-1,2,1,-5,4], Output: 6 Explanation: [4,-1,2,1] has the largest sum = 6. Follow up: If you have figured out the O(n) solution, try coding another solution using the divide and conquer approach, which is more subtle. Solution æ ¸å¿ƒæ€è·¯ï¼š å½“sum&lt;0æ—¶ï¼Œsum+xä¸€å®šæ¯”xå°ï¼Œæ•…å½“sumä¸ºè´Ÿæ•°æ—¶ï¼Œå¯ç›´æ¥å°†sumç½®é›¶ã€‚ class Solution: def maxSubArray(self, nums: List[int]) -&gt; int: sum_ = 0 max_sum = nums[0] for num in nums: sum_ = sum_ + num if sum_ &gt; max_sum: max_sum = sum_ sum_ = max(sum_, 0) return max_sum Best Time to Buy and Sell Stock II Say you have an array prices for which the ith element is the price of a given stock on day i. Design an algorithm to find the maximum profit. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times). Note: You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again). Example 1: &gt; Input: [7,1,5,3,6,4] Output: 7 Explanation: Buy on day 2 (price = 1) and sell on day 3 (price = 5), profit = 5-1 = 4. Then buy on day 4 (price = 3) and sell on day 5 (price = 6), profit = 6-3 = 3. Solution å“‡è¿™ä¸ªé¢˜ï¼ŒçœŸçš„è®©æˆ‘æ€€ç–‘è‡ªå·±çš„æ™ºå•†äº†...ğŸ˜‚ï¼Œæˆ‘åˆšå¼€å§‹çš„ç­”æ¡ˆå†™äº†å¥½å¤šè¡Œä»£ç ï¼Œç„¶åå»è®¨è®ºåŒºå­¦ä¹ çš„æ—¶å€™å‘ç°å¤§å®¶éƒ½æ˜¯äººæ‰å•Š...æ¯”å¦‚ä¸‹é¢è¿™ä¸ªï¼š class Solution: def maxProfit(self, prices: List[int]) -&gt; int: max_profit = 0 for i in range(0, len(prices)-1): max_profit += max(prices[i+1]-prices[i], 0) return max_profit Mine class Solution: def maxProfit(self, prices: List[int]) -&gt; int: max_profit = 0 buy_i = 0 sell_i = 1 need_sell = 1 need_buy = 0 for i in range(1, len(prices)): if prices[sell_i] &lt; prices[buy_i]: need_buy = 1 need_sell = 0 else: need_buy = 0 need_sell = 1 if need_buy == 1: if prices[i] &lt; prices[buy_i]: buy_i = i if i &lt; len(prices) - 1: sell_i = i + 1 else: sell_i = i continue if need_sell == 1: if prices[i] &gt;= prices[sell_i]: sell_i = i if i == len(prices) - 1: max_profit += prices[sell_i] - prices[buy_i] else: max_profit += prices[sell_i] - prices[buy_i] buy_i = i if i &lt; len(prices) - 1: sell_i = i + 1 need_buy = 1 need_sell = 0 return max_profit emmmm...æˆ‘éœ€è¦åæ€ä¸€ä¸‹ğŸ˜ Counting Elements Given an integer array arr, count element x such that x + 1 is also in arr. If there're duplicates in arr, count them seperately. Examples: &gt; Input: arr = [1,2,3] Output: 2 Explanation: 1 and 2 are counted cause 2 and 3 are in arr. &gt; Input: arr = [1,1,3,3,5,5,7,7] Output: 0 Explanation: No numbers are counted, cause there's no 2, 4, 6, or 8 in arr. &gt; Input: arr = [1,3,2,3,5,0] Output: 3 Explanation: 0, 1 and 2 are counted cause 1, 2 and 3 are in arr. Solution Mine è¿™ä¸ªé¢˜æ²¡æœ‰æ‰¾åˆ°å¤§å®¶çš„è®¨è®ºï¼Œæ‰€ä»¥æˆ‘ä¹Ÿä¸çŸ¥é“å…¶ä»–å°ä¼™ä¼´çš„è§£é¢˜æ€è·¯æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œä½†æ˜¯æˆ‘æ€»è§‰å¾—å¤§å®¶ä¼šæœ‰æ›´å¥½çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ‘è¿™è¾¹å°±å…ˆpoå‡ºæˆ‘çš„è§£æ³•ï¼Œæ¬¢è¿å¤§å®¶æä¾›è‡ªå·±çš„æ€è·¯å’Œå»ºè®®ã€‚ æ€è·¯ï¼š ç°å°†æ•°ç»„æ’åºï¼Œç„¶åé€šè¿‡ä¸¤ä¸ªæŒ‡é’ˆï¼špreå’Œcurè¿›è¡Œå¯¹æ¯”ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯æ•°å­—ç›¸åŒçš„æ•°åº”è¯¥åŒæ—¶è¢«è®¡æ•°æˆ–éƒ½ä¸è®¡æ•°ã€‚ class Solution: def countElements(self, arr: List[int]) -&gt; int: arr_sorted = sorted(arr) count = 0 # æŒ‡å‘å½“å‰éœ€è¦åˆ¤æ–­æ˜¯å¦èƒ½è¢«è®¡æ•°çš„æ•° pre = 0 # æŒ‡å‘å½“å‰preåé¢çš„æ•°ï¼Œç”¨äºåˆ¤æ–­preæ˜¯å¦èƒ½è¢«è®¡æ•° cur = 1 # å¦‚æœcuræŒ‡å‘çš„æ•°ç­‰äºpreæŒ‡å‘çš„æ•°ï¼Œåˆ™å¢åŠ leaveï¼Œ # å¦‚æœpreå¯ä»¥è¢«è®¡æ•°ï¼Œåˆ™å’Œpreç›¸ç­‰çš„æ•°ä¹Ÿåº”è¯¥è¢«è®¡æ•° leave = 1 # å½“preå’ŒcuræŒ‡å‘åŒä¸€indexæ—¶åœæ­¢å¾ªç¯ while pre^cur: sub = arr_sorted[cur] - arr_sorted[pre] if sub &lt;= 1: if sub == 1: count += leave pre = cur leave = 1 elif cur == len(arr)-1: break; else: leave +=1 else: pre = cur leave = 1 cur = min(cur+1, len(arr)-1) return count Summary æˆ‘å‘ç°æˆ‘çš„å†™ä½œèƒ½åŠ›çœŸçš„å¥½å·®å•Šï¼Œæœ‰çš„æ—¶å€™çŸ¥é“æ€ä¹ˆå›äº‹å°±æ˜¯å†™ä¸å‡ºæ¥ï¼Œæ‰æ€¥ã€‚ã€‚ã€‚å¸Œæœ›çœ‹åˆ°çš„å°ä¼™ä¼´ä¸è¦ä»‹æ„ï¼Œæˆ‘åœ¨åŠªåŠ›ï¼Œå¸Œæœ›ä»¥åä¼šè¶Šæ¥è¶Šå¥½ï¼Œå”‰ğŸ˜Œï¼Œå¯¹ä¸èµ·é«˜ä¸­è¯­æ–‡è€å¸ˆä¸€å¯¹ä¸€çš„è¾…å¯¼å•Šã€‚å¥½å•¦ï¼Œè¿™æœŸå°±æ›´æ–°å®Œäº†ã€‚ å¯¹äº†ï¼Œæˆ‘æ˜¯å‡†å¤‡ä¸€æ¬¡æ›´æ–°ä¸€å‘¨çš„å†…å®¹ï¼Œç„¶åå¯èƒ½æƒ³èµ·æ¥å°±ä¼šæ›´æ–°ä¸€éƒ¨åˆ†å‘å¸ƒå‡ºæ¥ï¼Œå¦‚æœæ²¡æ›´å®Œæœ€åä¼šæ˜¾ç¤ºUpdating...ã€‚ä¸€èµ·æœŸå¾…ç¬¬äºŒæœŸå§~ğŸ˜Š","categories":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Coding","slug":"Technology/Coding","permalink":"https://littlelittlemoon.github.io/categories/Technology/Coding/"},{"name":"Leetcode","slug":"Technology/Coding/Leetcode","permalink":"https://littlelittlemoon.github.io/categories/Technology/Coding/Leetcode/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://littlelittlemoon.github.io/tags/Leetcode/"},{"name":"Python","slug":"Python","permalink":"https://littlelittlemoon.github.io/tags/Python/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://littlelittlemoon.github.io/tags/Algorithm/"}],"keywords":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Coding","slug":"Technology/Coding","permalink":"https://littlelittlemoon.github.io/categories/Technology/Coding/"},{"name":"Leetcode","slug":"Technology/Coding/Leetcode","permalink":"https://littlelittlemoon.github.io/categories/Technology/Coding/Leetcode/"}]},{"title":"Fraud Detection | Imbalanced data modeling","slug":"Credit Card Fraud Detection","date":"2020-03-21T13:53:06.000Z","updated":"2020-04-09T07:01:31.447Z","comments":true,"path":"2020/03/21/Credit Card Fraud Detection/","link":"","permalink":"https://littlelittlemoon.github.io/2020/03/21/Credit%20Card%20Fraud%20Detection/","excerpt":"","text":"LDA, QDA and LR for fraud detection | Imbalanced data modeling %matplotlib inline # import warnings filter from warnings import simplefilter # ignore all future warnings simplefilter(action=&#39;ignore&#39;, category=FutureWarning) prepare data import pandas as pd # load data default_data = pd.read_csv(&quot;data/Default.csv&quot;) # prepare data default_data.loc[default_data[&#39;default&#39;] == &#39;No&#39;, &quot;default&quot;] = 0 default_data.loc[default_data[&#39;default&#39;] == &#39;Yes&#39;, &quot;default&quot;] = 1 default_data.loc[default_data[&#39;student&#39;] == &#39;No&#39;, &quot;student&quot;] = 0 default_data.loc[default_data[&#39;student&#39;] == &#39;Yes&#39;, &quot;student&quot;] = 1 default_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 default student balance income count 10000.00000 10000.000000 10000.000000 10000.000000 10000.000000 mean 5000.50000 0.033300 0.294400 835.374886 33516.981876 std 2886.89568 0.179428 0.455795 483.714985 13336.639563 min 1.00000 0.000000 0.000000 0.000000 771.967729 25% 2500.75000 0.000000 0.000000 481.731105 21340.462903 50% 5000.50000 0.000000 0.000000 823.636973 34552.644802 75% 7500.25000 0.000000 1.000000 1166.308386 43807.729272 max 10000.00000 1.000000 1.000000 2654.322576 73554.233495 split training and testing set from sklearn.model_selection import train_test_split # create features and target features = [&quot;balance&quot;, &quot;income&quot;] X = default_data[features] y = default_data.default # slipt data set into training and testing set train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.7, random_state=1) import numpy as np import matplotlib as mpl from scipy import linalg from matplotlib import colors import matplotlib.pyplot as plt from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.linear_model import LogisticRegression Plot function # set colormap cmap = colors.LinearSegmentedColormap( &#39;red_blue_classes&#39;, {&#39;red&#39;: [(0, 1, 1), (1, 0.7, 0.7)], &#39;green&#39;: [(0, 0.7, 0.7), (1, 0.7, 0.7)], &#39;blue&#39;: [(0, 0.7, 0.7), (1, 1, 1)]}) plt.cm.register_cmap(cmap=cmap) # Plot function def plot_data(model, X, y, y_pred): plt.ylabel(&#39;income&#39;) plt.xlabel(&#39;balance&#39;) tp = (y == y_pred) # True Positive tp0, tp1 = tp[y == 0], tp[y == 1] X0, X1 = X[y == 0], X[y == 1] X0_tp, X0_fp = X0[tp0], X0[~tp0] X1_tp, X1_fp = X1[tp1], X1[~tp1] # true class 0: dots, false class 0: x plt.scatter(X0_tp[&quot;balance&quot;], X0_tp[&quot;income&quot;], marker=&#39;.&#39;, color=&#39;red&#39;) plt.scatter(X0_fp[&quot;balance&quot;], X0_fp[&quot;income&quot;], marker=&#39;x&#39;, s=20, color=&#39;#990000&#39;) # dark red # true class 1: dots, false class 1: x plt.scatter(X1_tp[&quot;balance&quot;], X1_tp[&quot;income&quot;], marker=&#39;.&#39;, color=&#39;blue&#39;) plt.scatter(X1_fp[&quot;balance&quot;], X1_fp[&quot;income&quot;], marker=&#39;x&#39;, s=20, color=&#39;#000099&#39;) # dark blue # class 0 and 1 : all areas for decision boundary nx, ny = 200, 100 x_min, x_max = plt.xlim() y_min, y_max = plt.ylim() xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx), np.linspace(y_min, y_max, ny)) Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()]) Z = Z[:, 1].reshape(xx.shape) plt.pcolormesh(xx, yy, Z, cmap=&#39;red_blue_classes&#39;, norm=colors.Normalize(0., 1.), zorder=0) # plot decision boundary plt.contour(xx, yy, Z, [0.5], linewidths=2., colors=&#39;white&#39;) plt.axis(&#39;tight&#39;) plt.tight_layout() plt.subplots_adjust(top=0.92) plt.show() Linear Discriminant Analysis # Linear Discriminant Analysis plt.figure(figsize=(6, 7), facecolor=&#39;white&#39;) plt.title(&#39;Linear Discriminant Analysis&#39;, y=1, fontsize=15) lda = LinearDiscriminantAnalysis(solver=&quot;svd&quot;, store_covariance=True) y_pred = lda.fit(train_X, train_y).predict(test_X) plot_data(lda, test_X, test_y, y_pred) png Quadratic Discriminant Analysis # Quadratic Discriminant Analysis plt.figure(figsize=(6, 7), facecolor=&#39;white&#39;) plt.title(&#39;Quadratic Discriminant Analysis&#39;, y=1, fontsize=15) qda = QuadraticDiscriminantAnalysis(store_covariance=True) y_pred = qda.fit(train_X, train_y).predict(test_X) plot_data(qda, test_X, test_y, y_pred) png Use LogisticRegression directly to model the data If I use this data directly to feed the LogisticRegression model, the model will prefer to predict all as 0 for a high accuracy of 0 prediction. print(default_data.default.value_counts(dropna = False)) print(&quot;The mean of default: &quot;, default_data.default.mean()) 0 9667 1 333 Name: default, dtype: int64 The mean of default: 0.0333 NOTE: As it showing above: the provided data with very low proportion of positive signals. Conclusion: The provided data is imbalanced ! Solution: usually for imbalanced data, there are some solutions: 1. Collect more data 2. Down-Sampling or Over-Sampling to get balanced samples 3. Change the Thresholds to adjust the prediction 4. Assign class weights for the low rate class from sklearn.metrics import confusion_matrix, auc, roc_curve, roc_auc_score, recall_score, precision_recall_curve from sklearn.metrics import make_scorer, precision_score from sklearn.model_selection import GridSearchCV # Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = .33, stratify = y) logitreg_parameters = {&#39;C&#39;: np.power(10.0, np.arange(-3, 3))} logitreg = LogisticRegression(verbose = 3, warm_start = True) logitreg_grid = GridSearchCV(logitreg, param_grid = logitreg_parameters, scoring = &#39;roc_auc&#39;, n_jobs = 1) logitreg_grid.fit(train_X, train_y) GridSearchCV(cv=None, error_score=nan, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=3, warm_start=True), iid=&#39;deprecated&#39;, n_jobs=1, param_grid={&#39;C&#39;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=&#39;roc_auc&#39;, verbose=0) # draw decision boundary with LogisticRegression directly plt.figure(figsize=(6, 7), facecolor=&#39;white&#39;) plt.title(&#39;Logistic Regression directly&#39;, y=1, fontsize=15) y_pred = logitreg_grid.predict(test_X) splot = plot_data(logitreg_grid, test_X, test_y, y_pred) png # on OVER-Sampled TRAINing data print(&quot;\\n The recall score on Training data is:&quot;, recall_score(train_y, logitreg_grid.predict(train_X))) # 0.32 print(&quot;\\n The precision score on Training data is:&quot;, precision_score(train_y, logitreg_grid.predict(train_X))) # 0.74 # on the separated TEST data print(&quot;\\n Thre recall score on Test data is:&quot;, recall_score(test_y, logitreg_grid.predict(test_X))) # 0.32 print(&quot;\\n Thre precision score on Test data is:&quot;, precision_score(test_y, logitreg_grid.predict(test_X))) # 0.75 print(&quot;\\n Thre Confusion Matrix on Test data is:&quot;, confusion_matrix(test_y, logitreg_grid.predict(test_X))) # [[3178 12][ 74 36]] The recall score on Training data is: 0.32231404958677684 The precision score on Training data is: 0.7222222222222222 Thre recall score on Test data is: 0.3626373626373626 Thre precision score on Test data is: 0.673469387755102 Thre Confusion Matrix on Test data is: [[2893 16] [ 58 33]] Conclusions: From the output above, on the training data, the recall score is 0.32 which means 32 over 100 of the True positive conditions are predicted correctly. And 74 over 100 of the predicted positives are True Positive. On the Test data, the model performance metric evalued by recall or precision are close to the Training data. There is a precision score of 0.81 on the Test data, which means 81 out of 100 predicted positives are True positives. From Confusion Matrix, 36 of 110 True Positives are predicted as positives. And of all 48 predicted as positive, 36 of them are True positives. Change the Thresholds plot roc curve def plot_roc(new_thresholds, logitreg_grid): y_train_pred_probas = logitreg_grid.predict_proba(train_X)[:, 1] # prob of predict as 1 fpr, tpr, thresholds = roc_curve(train_y, y_train_pred_probas) # precision_recall_curve roc = pd.DataFrame({&#39;FPR&#39;:fpr, &#39;TPR&#39;:tpr, &#39;Thresholds&#39;:thresholds}) plt.figure() plt.title(&#39;ROC Curve&#39;, y = 1, fontsize = 15) plt.plot(roc.FPR, roc.TPR) plt.axvline(new_thresholds, color = &#39;#00C851&#39;, linestyle = &#39;--&#39;) plt.xlabel(&quot;FPR&quot;) plt.ylabel(&quot;TPR&quot;) plt.show() new_threshold = 0.1 # 0.5 is the default value plot_roc(new_threshold, logitreg_grid) png By default, the threshold is 0.5. Since the recall score is low, I'm trying to lower the threshold to get more predicted as Positive. At the same time, more True Negative data will be falsely predicted as Positive. So the Precision score will be lower. y_test_pred_probas = logitreg_grid.predict_proba(test_X)[:, 1] y_test_pred = (y_test_pred_probas &gt;= new_threshold).astype(int) print(&quot;After change threshold to 0.1, the recall socre on Test data is:&quot;) print(recall_score(test_y, y_test_pred)) # 0.736 print(&quot;After change threshold to 0.1, the precision socre on Test data is:&quot;) print(precision_score(test_y, y_test_pred)) # 0.301 print(&quot;After change threshold to 0.1, the Confusion Matrix on Test data is:&quot;) print(confusion_matrix(test_y, y_test_pred)) # [[3002 188][ 29 81]] After change threshold to 0.1, the recall socre on Test data is: 0.7142857142857143 After change threshold to 0.1, the precision socre on Test data is: 0.25 After change threshold to 0.1, the Confusion Matrix on Test data is: [[2714 195] [ 26 65]] Create Over-sampling data and Fit the model oversample_ratio = sum(train_y == 0) / sum(train_y == 1) # size to repeat y == 1 # repeat the positive data for X and y y_train_pos_oversample = pd.concat([train_y[train_y==1]] * int(oversample_ratio), axis = 0) X_train_pos_oversample = pd.concat([train_X.loc[train_y==1, :]] * int(oversample_ratio), axis = 0) # concat the repeated data with the original data together y_train_oversample = pd.concat([train_y, y_train_pos_oversample], axis = 0).reset_index(drop = True) X_train_oversample = pd.concat([train_X, X_train_pos_oversample], axis = 0).reset_index(drop = True) print(y_train_oversample.value_counts(dropna = False, normalize = True)) logitreg_parameters = {&#39;C&#39;: np.power(10.0, np.arange(-3, 3))} logitreg = LogisticRegression(verbose = 3, warm_start = True) logitreg_grid = GridSearchCV(logitreg, param_grid = logitreg_parameters, scoring = &#39;roc_auc&#39;, n_jobs = 1) logitreg_grid.fit(X_train_oversample, y_train_oversample) 1 0.500665 0 0.499335 Name: default, dtype: float64 GridSearchCV(cv=None, error_score=nan, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=3, warm_start=True), iid=&#39;deprecated&#39;, n_jobs=1, param_grid={&#39;C&#39;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=&#39;roc_auc&#39;, verbose=0) # Logistic Regression with Over-sampling plt.figure(figsize=(6, 7), facecolor=&#39;white&#39;) plt.title(&#39;Logistic Regression with Over-sampling&#39;, y=1, fontsize=15) y_pred = logitreg_grid.predict(test_X) plot_data(logitreg_grid, test_X, test_y, y_pred) png # on OVER-Sampled TRAINing data print(&quot;After Over-Sampling, the recall score on Training data is&quot;) print(recall_score(y_train_oversample, logitreg_grid.predict(X_train_oversample))) # 0.865 print(&quot;After Over-Sampling, the precision score on Training data is&quot;) print(precision_score(y_train_oversample, logitreg_grid.predict(X_train_oversample))) # 0.727 # on the TESTing data print(&quot;After Over-Sampling, the recall score on Test data is&quot;) print(recall_score(test_y, logitreg_grid.predict(test_X))) # 0.854 print(&quot;After Over-Sampling, the precision score on Test data is&quot;) print(precision_score(test_y, logitreg_grid.predict(test_X))) # 0.080 print(&quot;After Over-Sampling, the Confusion Matrix on Test data is&quot;) print(confusion_matrix(test_y, logitreg_grid.predict(test_X))) # [[2113 1077][ 16 94]] After Over-Sampling, the recall score on Training data is 0.8884297520661157 After Over-Sampling, the precision score on Training data is 0.8717057631045467 After Over-Sampling, the recall score on Test data is 0.8791208791208791 After Over-Sampling, the precision score on Test data is 0.17094017094017094 After Over-Sampling, the Confusion Matrix on Test data is [[2521 388] [ 11 80]] Conclusion: From the output above, on the training data, the recall score is 0.865 which means 86.5 over 100 of the True conditions are predicted correctly. And 85.4 over 100 of the predicted positives are really positive. However, there is only a precision score of 0.080 on the Test data, which means only 8 out of 100 predicted positives are real positives. From Confusion Matrix, 94 of 110 True Positives are predicted as positives. However, the model predicted 1077 Negative data as Positive. That is, this model has pretty strong over-fitting. Change the Thresholds new_threshold = 0.2 plot_roc(new_threshold, logitreg_grid) png y_test_pred_probas = logitreg_grid.predict_proba(test_X)[:, 1] y_test_pred = (y_test_pred_probas &gt;= new_threshold).astype(int) print(&quot;After change threshold to 0.2, the recall socre on Test data is:&quot;) print(recall_score(test_y, y_test_pred)) # 0.990 print(&quot;After change threshold to 0.2, the precision socre on Test data is:&quot;) print(precision_score(test_y, y_test_pred)) # 0.047 print(&quot;After change threshold to 0.2, the Confusion Matrix on Test data is:&quot;) print(confusion_matrix(test_y, y_test_pred)) # [[ 1013 2177][ 1 109]] After change threshold to 0.2, the recall socre on Test data is: 0.9340659340659341 After change threshold to 0.2, the precision socre on Test data is: 0.10023584905660378 After change threshold to 0.2, the Confusion Matrix on Test data is: [[2146 763] [ 6 85]] Conclusion: After over-sampling, the model will have higher recall rate. That is, the model will work better on detect the Frauds from True Frauds. The price is the lower precision rate. Logistic Regression with class_weight Rather than over-sampling, we can assign more weights to the lower rate class. we can write out the Likelihood function for Logistic Regression, the Over-Sampling and the assigning more Weights will be equivalent. positive_weight = sum(train_y == 0) / sum(train_y == 1) # size to repeat y == 1 logitreg_parameters = {&#39;C&#39;: np.power(10.0, np.arange(-3, 3))} logitreg = LogisticRegression(class_weight = {0 : 1, 1 : positive_weight}, verbose = 3, warm_start = True) logitreg_grid = GridSearchCV(logitreg, param_grid = logitreg_parameters, scoring = &#39;roc_auc&#39;, n_jobs = 1) logitreg_grid.fit(train_X, train_y) GridSearchCV(cv=None, error_score=nan, estimator=LogisticRegression(C=1.0, class_weight={0: 1, 1: 27.925619834710744}, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=3, warm_start=True), iid=&#39;deprecated&#39;, n_jobs=1, param_grid={&#39;C&#39;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=&#39;roc_auc&#39;, verbose=0) # Logistic Regression with class_weight plt.figure(figsize=(6, 7), facecolor=&#39;white&#39;) plt.title(&#39;Logistic Regression&#39;, y=1, fontsize=15) y_pred = logitreg_grid.predict(test_X) plot_data(logitreg_grid, test_X, test_y, y_pred) png print(&quot;After assign class_weight, the recall score on Training data is&quot;) print(recall_score(y_train_oversample, logitreg_grid.predict(X_train_oversample))) # 0.856 print(&quot;After assign class_weight, the precision score on Training data is&quot;) print(precision_score(y_train_oversample, logitreg_grid.predict(X_train_oversample))) # 0.729 # on the separated TEST data print(&quot;After assign class_weight, the recall score on Test data is&quot;) print(recall_score(test_y, logitreg_grid.predict(test_X))) # 0.845 print(&quot;After assign class_weight, the precision score on Test data is&quot;) print(precision_score(test_y, logitreg_grid.predict(test_X))) # 0.081 print(&quot;After assign class_weight, the Confusion Matrix on Test data is&quot;) print(confusion_matrix(test_y, logitreg_grid.predict(test_X))) # [[2135 1055] [ 17 93]] print(&quot;After assign class_weight, the ROC AUC Score on Test data is&quot;) print(roc_auc_score(test_y, logitreg_grid.predict(test_X))) # 0.757 After assign class_weight, the recall score on Training data is 0.859504132231405 After assign class_weight, the precision score on Training data is 0.7171530599679843 After assign class_weight, the recall score on Test data is 0.8791208791208791 After assign class_weight, the precision score on Test data is 0.075046904315197 After assign class_weight, the Confusion Matrix on Test data is [[1923 986] [ 11 80]] After assign class_weight, the ROC AUC Score on Test data is 0.7700863934965 Conclusion: If I set up the class weight for the positive as the ratio of non-Fault / Fault, I will get the result close to the over-sampling. So, in summary: This specific data is about fraud detection. So the model should focus on to find the frauds to avoid potential loss for the bank. That is, we focus on recall rate. Conclusion If we use the imbalanced data directly, we will get low performance model since the model prefer to predict to the class with dominated frequency class. The recall rate is 0.31. That is, only 31% of the frauds can be detected by this model. To fix that, one way is to do over-sampling or down-sampling. If we use over-sampling, the model performance will be improved a lot. For this specific case, the recall rate on the independent test set will be improved from 0.31 to 0.87 Another way to improve the model performance is to assign more weights to the low frequency class. Generally speaking, for Logistic Regression, assigning weights is similar to over-sampling, from the likelihood function perspective. The final output results are close too as demonstrated above. Reference Credit Card Fraud Detection / Imbalanced data modeling - Part I: Logistic Regression Credit Fraud || Dealing with Imbalanced Datasets","categories":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Machine Learning","slug":"Technology/Machine-Learning","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/"},{"name":"Imbalanced Data Modeling","slug":"Technology/Machine-Learning/Imbalanced-Data-Modeling","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/Imbalanced-Data-Modeling/"}],"tags":[{"name":"LDA","slug":"LDA","permalink":"https://littlelittlemoon.github.io/tags/LDA/"},{"name":"QDA","slug":"QDA","permalink":"https://littlelittlemoon.github.io/tags/QDA/"},{"name":"LR","slug":"LR","permalink":"https://littlelittlemoon.github.io/tags/LR/"},{"name":"Imbalanced Data Modeling","slug":"Imbalanced-Data-Modeling","permalink":"https://littlelittlemoon.github.io/tags/Imbalanced-Data-Modeling/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://littlelittlemoon.github.io/tags/Machine-Learning/"},{"name":"Classify","slug":"Classify","permalink":"https://littlelittlemoon.github.io/tags/Classify/"}],"keywords":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Machine Learning","slug":"Technology/Machine-Learning","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/"},{"name":"Imbalanced Data Modeling","slug":"Technology/Machine-Learning/Imbalanced-Data-Modeling","permalink":"https://littlelittlemoon.github.io/categories/Technology/Machine-Learning/Imbalanced-Data-Modeling/"}]},{"title":"Evalution of hair and scalp condition based on microscopy image analysis | Hair Thickness - part 1","slug":"Evalution of hair and scalp condition based on microscopy image analysis  | Hair Thickness - 1","date":"2019-12-29T13:21:24.000Z","updated":"2020-04-10T06:09:34.142Z","comments":true,"path":"2019/12/29/Evalution of hair and scalp condition based on microscopy image analysis  | Hair Thickness - 1/","link":"","permalink":"https://littlelittlemoon.github.io/2019/12/29/Evalution%20of%20hair%20and%20scalp%20condition%20based%20on%20microscopy%20image%20analysis%20%20|%20Hair%20Thickness%20-%201/","excerpt":"","text":"Paper: Evalution of hair and scalp condition based on microscopy image analysis æ‘˜è¦ç¿»è¯‘ï¼šç”±äºITæŠ€æœ¯çš„å¿«é€Ÿéƒ¨ç½²ï¼ŒåŒ»ç–—ä¿å¥æœåŠ¡è¿›å…¥äº†ä¸€ä¸ªæ–°æ—¶ä»£ã€‚è¯¸å¦‚å¿ƒè„ç›‘æŠ¤ä¹‹ç±»çš„æŸäº›æœåŠ¡å¯¹äºç”Ÿå‘½è‡³å…³é‡è¦ï¼Œå¹¶æœ‰åŠ©äºæŒ½æ•‘ç”Ÿå‘½ã€‚å¦ä¸€æ–¹é¢ï¼Œç›‘æµ‹è„±å‘æ˜¯å¦ä¸€ç§æœ‰è¶£çš„ä¿å¥æœåŠ¡ã€‚å°½ç®¡è¿™å¯¹ç”Ÿæ´»å¹¶ä¸é‡è¦ï¼Œä½†äººä»¬è¿˜æ˜¯ä¼šéå¸¸æ³¨æ„è‡ªå·±çš„å¤´å‘çŠ¶å†µã€‚è„±å‘æ˜¯ä¸å¤´å‘çŠ¶å†µæœ‰å…³çš„ä¸»è¦é—®é¢˜ä¹‹ä¸€ï¼Œå› ä¸ºè¿‡å¤šå’Œæ— æ„çš„è„±å‘å¯èƒ½å¯¼è‡´ç§ƒå¤´ã€‚å¯ä»¥åœ¨æŠ¤å‘åº—ä¸“ä¸šè¿›è¡ŒæŠ¤å‘ï¼Œä½†æ˜¯è¿™éœ€è¦å¾ˆå¤šæ—¶é—´å’Œæˆæœ¬ã€‚æœ€è¿‘ï¼Œç”±äºå»‰ä»·çš„æ™ºèƒ½è®¾å¤‡ï¼Œå¯¹å¤´å‘çŠ¶å†µçš„è‡ªæˆ‘è¯Šæ–­å·²æˆä¸ºå¯èƒ½ã€‚ä»ç„¶å¾ˆå°‘å¼€å‘ç”¨äºè¯„ä¼°å¤´å‘çŠ¶å†µçš„åº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ¡ˆï¼Œé€šè¿‡ä»æ˜¾å¾®é•œå›¾åƒä¸­æå–å„ç§ç‰¹å¾æ¥è¯„ä¼°å¤´å‘å’Œå¤´çš®çš„çŠ¶å†µã€‚å…¶ç‰¹å¾åŒ…æ‹¬å¤´å‘çš„åšåº¦ï¼Œå¤´å‘çš„å¯†åº¦å’Œå¤´çš®çš„æ–‘ç‚¹ã€‚é€šè¿‡å¯¹åŸå‹ç³»ç»Ÿè¿›è¡Œå¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚ ä¸ºäº†åˆ†æå¤´çš®å›¾åƒï¼Œåº”è¯¥å°†å¤´å‘å’Œå¤´çš®å½¼æ­¤åˆ†å¼€ã€‚ä¸¤è€…ä¹‹é—´æœ€æ˜æ˜¾çš„åŒºåˆ«æ˜¯å®ƒä»¬çš„é¢œè‰²ã€‚å¤´çš®ç›¸å¯¹æ˜äº®ï¼Œå¤´å‘ç›¸å¯¹æ·±è‰²ã€‚å› æ­¤ï¼Œåœ¨è®¸å¤šç ”ç©¶ä¸­ï¼Œæ ¹æ®é¢œè‰²å¯¹å¤´å‘å’Œå¤´çš®åŒºåŸŸè¿›è¡Œåˆ†ç±»ï¼Œå¹¶æ ¹æ®è¿™ç§åˆ†ç¦»è¿›è¡Œå›¾åƒåˆ†æã€‚ ## Overall steps for feature extraction Pre-processing è¯¥è®ºæ–‡ä¸­æ‰€æåˆ°çš„å›¾ç‰‡é¢„å¤„ç†æ–¹æ³•å’Œâ€œAn Unsupervised Hair Segmentation and Counting System in Microscopy Imagesâ€ä¸­ç›¸ä¼¼ï¼Œå…·ä½“å¯å‚è€ƒï¼šè®ºæ–‡è§£è¯» - An Unsupervised Hair Segmentation and Counting System in Microscopy Images ä¹‹å¤´å‘è®¡æ•°é—®é¢˜ã€‚ 1. è£åˆ‡å›¾åƒ 2. å›¾åƒå¢å¼º: ä½¿ç”¨Contrast stretchingæ–¹æ³•ï¼Œå¢åŠ å›¾åƒçš„å¯¹æ¯”åº¦ 3. Morphological openingï¼šå»é™¤æ²¹æ€§å’Œæ¹¿æ¶¦çš„å¤´å‘åœ¨å…¶æ˜¾å¾®é•œå›¾åƒä¸­å½¢æˆçš„äº®ç‚¹ 4. äºŒå€¼åŒ–ï¼šç»è¿‡ä¸Šè¿°é¢„å¤„ç†åï¼Œå°†æ‰€å¾—å›¾åƒè½¬æ¢ä¸ºç°åº¦å›¾åƒï¼Œç„¶åæ ¹æ®Otsué˜ˆå€¼è½¬æ¢ä¸ºäºŒè¿›åˆ¶å›¾åƒã€‚ åœ¨äºŒå€¼å›¾åƒä¸­ï¼Œâ€œ 0â€å’Œâ€œ 1â€åˆ†åˆ«è¡¨ç¤ºå¤´å‘åƒç´ å’Œå¤´çš®åƒç´ ã€‚ hair/scalp image analysis å¤´å‘æ£€æµ‹ æŠ€æœ¯ ä½¿ç”¨Cannyè¾¹ç¼˜æ£€æµ‹ç®—æ³•ä»äºŒè¿›åˆ¶å›¾åƒä¸­è·å–æ¯›å‘è½®å»“ã€‚ ç»“æœ - å›¾2æ˜¾ç¤ºäº†æ£€æµ‹å’Œå»ºæ¨¡å¤´å‘çš„æ‰€æœ‰æ­¥éª¤ã€‚å¯¹äºå›¾2ï¼ˆaï¼‰ä¸­çš„åŸå§‹æ˜¾å¾®é•œå›¾åƒï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡ºå¤´å‘è½®å»“å’Œéª¨éª¼ã€‚ - é€šè¿‡ä½¿ç”¨äºŒè¿›åˆ¶å›¾åƒä¸Šçš„ç¨€ç–è¿ç®—ï¼ˆThinning operationï¼‰ æ¥è®¡ç®—å¤´å‘éª¨æ¶ã€‚Thinningæ˜¯ä¸€ç§å½¢æ€å­¦è¿ç®—ï¼Œå¯å»é™¤æ•´ä¸ªäºŒè¿›åˆ¶å›¾åƒä¸­çš„å‰æ™¯ã€‚ - å›¾2ï¼ˆdï¼‰æ˜¾ç¤ºäº†é€šè¿‡å åŠ å¤´å‘è½®å»“å’Œéª¨éª¼å¾—åˆ°çš„æœ€ç»ˆå›¾åƒã€‚ å¤´å‘åšåº¦è®¡ç®— å¤´å‘åšåº¦å¯ä»¥é€šè¿‡ä¸å¤´å‘å‚ç›´çº¿çš„é•¿åº¦æ¥å®šä¹‰ã€‚è¦è·å¾—å‚ç›´çº¿ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦è®¡ç®—å¤´å‘æ–¹å‘. é€šè¿‡è€ƒè™‘ç›¸é‚»åƒç´ å¹¶åº”ç”¨PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰ç®—æ³•æ¥è®¡ç®—æ¯ä¸ªåƒç´ çš„æ–¹å‘. å½“è®¡ç®—å¤´å‘éª¨æ¶ä¸Šæ‰€æœ‰ç‚¹çš„æ–¹å‘æ—¶ï¼Œå¯ä»¥è®¡ç®—å‡ºæ¯ä¸ªç‚¹çš„å‚ç›´çº¿ã€‚ç„¶åï¼Œå‚ç›´çº¿ä¸å¤´å‘è¾¹ç•Œçš„äº¤ç‚¹ä¹‹é—´çš„è·ç¦»å°±æ˜¯å¤´å‘çš„åšåº¦ï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨æ¬§æ°è·ç¦»æ¥è®¡ç®—ï¼š \\[ \\rho = \\sqrt{\\smash[b]{(x_2-x_1)^2 + (y_2-y_1)^2}} \\] æ ¹æ®æ¬§å¼è·ç¦»\\(\\rho\\)å¯è®¡ç®—å‡ºå¤´å‘çš„å¹³å‡åšåº¦ï¼š \\[ Thinckness_{avg} =\\frac{1}{n}\\displaystyle\\sum_{i=1}^n\\sqrt{\\smash[b]{(x_{i2}-x_{i1})^2 + (y_{i2}-y_{i1})^2}} \\] è¿™é‡Œ\\(Thinckness_{avg}\\)çš„å•ä½æ˜¯åƒç´ ï¼Œå› æ­¤éœ€è¦ä½¿ç”¨ç­‰å¼å°†å…¶æ›´æ”¹ä¸ºä»ªè¡¨å•ä½: \\[ Thinckness_{actual}(um) = \\frac{Thinckness_{avg}(px) Ã—UL(um/px)}{mf} \\] \\(mf\\): æ˜¯ç›¸æœºçš„æ”¾å¤§å€ç‡ \\(UL\\): æ˜¯å•ä½é•¿åº¦ï¼Œè¡¨ç¤ºä¸€ä¸ªåƒç´ çš„å¾®ç±³é•¿åº¦ å®éªŒç»“æœ ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„å¤´å‘åšåº¦è¯„ä¼°æ–¹æ³•çš„å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨ç”µå­æ˜¾å¾®é•œæµ‹é‡äº†å®é™…çš„å¤´å‘åšåº¦ã€‚å¤´å‘åšåº¦æµ‹é‡çš„å‡†ç¡®æ€§å¦‚è¡¨1æ‰€ç¤ºã€‚ table_1_AVERAGE_HAIR_THICKNESS_AND_ERROR_RATE æ®æŠ¥é“ï¼ŒéŸ©å›½äººçš„å¹³å‡å¤´å‘åšåº¦ä¸º84.9Î¼mã€‚ä¸æ­¤ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„90.29Î¼mçš„ç»“æœç›¸å½“ä¸é”™ã€‚å®é™…å€¼ï¼ˆé€šè¿‡ç”µå­æ˜¾å¾®é•œè®¡ç®—ï¼‰ä¸ä¼°è®¡å€¼ä¹‹é—´çš„å·®å¼‚å¾ˆå°ã€‚ å¯èƒ½çš„åŸå› ä¹‹ä¸€æ˜¯é˜´å½±æ•ˆæœã€‚å¦ä¸€ä¸ªå¯èƒ½çš„åŸå› æ˜¯ç›¸æœºé•œå¤´å˜å½¢ã€‚ å›¾åƒå°ºå¯¸ä¸º640x480ã€‚ä½†æ˜¯ï¼Œç›¸æœºæ‰€è¦†ç›–åŒºåŸŸçš„çœŸå®å½¢çŠ¶å‡ ä¹æ˜¯æ¤­åœ†å½¢ã€‚å› æ­¤ï¼Œç›¸æœºæ”¾å¤§ç‡åœ¨è¡Œå’Œåˆ—ä¹‹é—´å…·æœ‰å·®å¼‚ã€‚æ ¹æ®æ‹æ‘„è§’åº¦ï¼Œæ”¾å¤§å€ç‡å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚ æ€»ç»“ é’ˆå¯¹å¤´å‘ç²—ç»†ï¼ˆåšåº¦ï¼‰è®¡ç®—é—®é¢˜ï¼Œä¸Šè¿°è®ºæ–‡æå‡ºäº†åŸºäºå¤´å‘è½®å»“å’Œéª¨éª¼å›¾ï¼Œé€šè¿‡è®¡ç®—æ¯ä¸ªåƒç´ ç‚¹çš„æ–¹å‘å’Œå‚çº¿ï¼Œè¿›ä¸€æ­¥ä½¿ç”¨æ¬§å¼è·ç¦»å…¬å¼æ¥è®¡ç®—å¤´å‘çš„å‚ç›´ç›´å¾„ï¼Œä½†å¯¹ç›¸å…³æŠ€æœ¯çš„åº”ç”¨ç»†èŠ‚æè¿°ä¸å¤šã€‚ å‚è€ƒæ–‡çŒ® Evalution of hair and scalp condition based on microscopy image analysis An Unsupervised Hair Segmentation and Counting System in Microscopy Images Euclidean distance Principal component analysis","categories":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Paper Smash","slug":"Technology/Paper-Smash","permalink":"https://littlelittlemoon.github.io/categories/Technology/Paper-Smash/"}],"tags":[{"name":"Digital Image Processing","slug":"Digital-Image-Processing","permalink":"https://littlelittlemoon.github.io/tags/Digital-Image-Processing/"},{"name":"Line Detection","slug":"Line-Detection","permalink":"https://littlelittlemoon.github.io/tags/Line-Detection/"},{"name":"Paper","slug":"Paper","permalink":"https://littlelittlemoon.github.io/tags/Paper/"},{"name":"Hair Thickness","slug":"Hair-Thickness","permalink":"https://littlelittlemoon.github.io/tags/Hair-Thickness/"}],"keywords":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Paper Smash","slug":"Technology/Paper-Smash","permalink":"https://littlelittlemoon.github.io/categories/Technology/Paper-Smash/"}]},{"title":"An Unsupervised Hair Segmentation and Counting System in Microscopy Images | Hair Counting - part 2","slug":"An Unsupervised Hair Segmentation and Counting System in Microscopy Images | Hair Counting - 2","date":"2019-12-28T15:51:27.000Z","updated":"2020-04-10T06:12:17.760Z","comments":true,"path":"2019/12/28/An Unsupervised Hair Segmentation and Counting System in Microscopy Images | Hair Counting - 2/","link":"","permalink":"https://littlelittlemoon.github.io/2019/12/28/An%20Unsupervised%20Hair%20Segmentation%20and%20Counting%20System%20in%20Microscopy%20Images%20|%20Hair%20Counting%20-%202/","excerpt":"","text":"Paper: An Unsupervised Hair Segmentation and Counting System in Microscopy Images æ‘˜è¦ç¿»è¯‘ï¼šæœ¬æ–‡é‡ç‚¹ä»‹ç»ä½¿ç”¨é«˜çº§å›¾åƒå¤„ç†ç®—æ³•å¼€å‘ç”¨äºä¸´åºŠçš„åŒ»å­¦è½¯ä»¶ã€‚æœ¬æ–‡è®¨è®ºäº†å¤´å‘åˆ†å‰²å’Œè®¡æ•°çš„ä¸‰ä¸ªå…³é”®é—®é¢˜: - é¦–å…ˆï¼Œå»é™¤ç”±äºæ²¹è„‚æˆ–æ°´åˆ†å¼•èµ·çš„ä»»ä½•äº®ç‚¹ï¼Œè¿™äº›äº®ç‚¹åœ¨å¤´å‘çš„ä¸­éƒ¨å½¢æˆåœ†å½¢å›¾æ¡ˆï¼Œå¹¶æ˜¾ç€å½±å“ç¡®å®šçº¿æ¡çš„å‡†ç¡®æ€§ã€‚ - ç¬¬äºŒï¼Œè¯†åˆ«å‡ºä¸¤ä¸ªæ¥è§¦æˆ–é‡å çš„å¤´å‘ï¼Œå¹¶å°†å…¶è§†ä¸ºå•ä¸ªå¤´å‘ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤´å‘æ†ç»‘ç®—æ³•(hair-bundling algorithm)æ¥è®¡ç®—ä»»ä½•éšè—çš„å¤´å‘ã€‚ - æœ€åï¼Œå¤´å‘å¯èƒ½å‘ˆæ³¢æµªçŠ¶æˆ–å·æ›²çŠ¶ï¼Œè¿™ä½¿ä¼ ç»Ÿçš„åŸºäºHoughçš„çº¿æ£€æµ‹ç®—æ³•ä¸åˆé€‚ï¼Œå› ä¸ºå®ƒä¼šå—åˆ°å‚æ•°é€‰æ‹©çš„å½±å“ï¼Œä¾‹å¦‚çº¿æ®µçš„æœ€å°é•¿åº¦ä»¥åŠçº¿æ®µä¹‹é—´çš„è·ç¦»ã€‚æˆ‘ä»¬æå‡ºçš„æ¯›å‘è®¡æ•°ç®—æ³•æ¯”åŸºäºHoughçš„æ¯›å‘è®¡æ•°ç®—æ³•è¦å‡†ç¡®å¾—å¤šï¼Œå¹¶ä¸”åœ¨å„ç§ç™½å¹³è¡¡ä¸‹å¯¹å·å‘ï¼Œæ²¹æ€§å¤´çš®ï¼Œå™ªå£°è…èš€å’Œé‡å çš„å¤´å‘éƒ½å…·æœ‰é€‚ç”¨æ€§ã€‚ - å…³é”®è¯ï¼šæ¯›å‘è®¡æ•°ï¼Œå¤´çš®è¯Šæ–­ï¼ŒæŠ¤å‘è¯Šæ–­ï¼Œæ¯›å›Šè¯Šæ–­ï¼Œçº¿æ®µæ£€æµ‹ã€‚ System flowchart figure 1 System flowchart é¢„å¤„ç†é˜¶æ®µ (Preprocessing Stage) ä½¿ç”¨å¯¹æ¯”åº¦æ‹‰ä¼¸æ–¹æ³•(the contrast-stretching method)æ¥å¢åŠ å¤´çš®å’Œå¤´å‘åƒç´ ä¹‹é—´çš„å¯¹æ¯”åº¦; ä¸ºäº†å‡å°‘äº®ç‚¹çš„å½±å“ï¼Œæå‡ºäº†ä¸€ç§å¥å£®çš„é¢œè‰²å½¢æ€ç®—æ³•(morphological algorithm)ï¼Œä»¥ä½¿é¢œè‰²å¹³æ»‘å¹¶ä¿æŒå¤´å‘çš„ä¿çœŸåº¦; ä¸ºæ¯ä¸ªé¢œè‰²åˆ†é‡åº”ç”¨äº†Karhunen-LoÃ¨veå˜æ¢(KLT)ï¼Œå¹¶ä¿ç•™äº†å…·æœ‰æœ€é«˜èƒ½é‡çš„åˆ†é‡ï¼Œå¹¶ä½¿ç”¨Otsué˜ˆå€¼è·å¾—äº†å¯é çš„äºŒè¿›åˆ¶å›¾åƒã€‚ æ•°æ®é‡‡é›†è§„å®š è¿™é¡¹ç ”ç©¶çš„å”¯ä¸€å‡è®¾: å¤´å‘çš„é¢œè‰²æ¯”çš®è‚¤çš„é¢œè‰²æ·±ã€‚ å¤´å‘å›¾åƒæ˜¯ä»æ•°ç æ˜¾å¾®é•œç›¸æœºï¼ˆDMCï¼‰æ•è·ï¼Œå†…ç½®LEDå¢å¼ºï¼Œå¯è‡ªåŠ¨ä¿æŒäº®åº¦ç¨³å®šã€‚åº”ç”¨85å€çš„å˜ç„¦å€ç‡æ•è·å›¾åƒã€‚é€šå¸¸ï¼Œä½¿ç”¨åˆ†è¾¨ç‡ä¸º1024Ã—768ï¼Œç›¸å½“äºå¤´çš®é¢ç§¯æ˜¯0.25Ã—0.19è‹±å¯¸ã€‚æ­¤å¤–ï¼ŒåŸºäºDMCçš„ç™½å¹³è¡¡å°†æ•è·çš„å›¾åƒåˆ†ä¸ºä¸¤ç»„ï¼š - å…·æœ‰æ—¥å…‰çš„å›¾åƒè¢«åˆ†ç±»ä¸ºæ•°æ®é›†ï¼ƒ1ï¼Œ - å…·æœ‰è§å…‰çš„å›¾åƒè¢«åˆ†ç±»ä¸ºæ•°æ®é›†ï¼ƒ2ã€‚ ä½¿ç”¨å¯¹æ¯”åº¦æ‹‰ä¼¸(Contrast Stretching)è¿›è¡Œå›¾åƒå¢å¼º ç›®çš„ å¢åŠ å¤´å‘å’Œå¤´çš®ä¹‹é—´çš„å¯¹æ¯”åº¦; å¢åŠ å¤´å‘å’Œå¤´çš®åƒç´ ä¹‹é—´çš„è‰²å·®ã€‚ æŠ€æœ¯ é€šè¿‡åˆ†æ®µçº¿æ€§å¯¹æ¯”åº¦æ‹‰ä¼¸(color transformation by means of piecewise linear contrast stretching)è¿›è¡Œé¢œè‰²å˜æ¢æ¥å¢å¼ºå›¾åƒï¼Œæé«˜å¯¹æ¯”åº¦ï¼› stretched the middle-intensity level, and kept the levels of the low-intensity and high-intensity so as to prevent creating false colors. ç»“æœ figure_2_image_enhancement_for_the_datasets è¿›è¡Œå¯¹æ¯”åº¦æ‹‰ä¼¸æ—¶ï¼Œä¸ä¼šæ›´æ”¹åŸå§‹å¤´å‘åƒç´ ï¼Œä¹Ÿä¸ä¼šå¤¸å¤§æ²¹äº®åƒç´ ï¼š - é™ä½äº†å¤´çš®åƒç´ çš„å¼ºåº¦; - å¢åŠ äº†å¤´çš®å’Œå¤´å‘ä¹‹é—´çš„è‰²å·®; - äº®ç‚¹çš„åƒç´ ä¿æŒä¸å˜ã€‚ Bright Spot Removal (BSR) ç›®çš„ é™¤å™ª: å»é™¤æ²¹æ€§å’Œæ¹¿æ¶¦çš„å¤´å‘åœ¨å¤´å‘çš„ä¸­éƒ¨äº§ç”Ÿçš„äº®ç‚¹: æŠ€æœ¯ color morphological processing approach 1. éçº¿æ€§ä¸­å€¼æ»¤æ³¢å™¨(nonlinear median filter)æ¶ˆé™¤ç™½ç‚¹; 2. ç©ºé—´å¹³æ»‘æ»¤æ³¢å™¨(spatial smooth filter)é™ä½ç™½ç‚¹çš„å¼ºåº¦, ç¼ºç‚¹æ˜¯æµ‹è¯•å›¾åƒçš„éæ¯›å‘åŒºåŸŸä¹Ÿå°†å˜å¾—æ¨¡ç³Š; 3. color-based mathematical morphology (MM) method, used it as an ordering process. adopted the MM opening operator to depress the bright spot in the middle of the hairs. - ä¾µèš€å›¾åƒ - æ”¾å¤§å›¾åƒ - opening operation of image f : Î³M,nB(f) = Ï„M,nB(ÎµM,nB(f)), å…¶ä¸­ÎµM,nBå’ŒÏ„M,nBåˆ†åˆ«è¡¨ç¤ºç»“æ„å…ƒç´ Bå¯¹å¤§å°ä¸ºnçš„å›¾åƒfçš„å½¢æ€ä¾µèš€å’Œæ”¾å¤§, å¯¹äºåƒç´ x: - ÎµM,nB(f)(x) = {f(y) : f(y) = âˆ§M[f(z)],z âˆˆ n(Bx)} - Ï„M,nB(f)(x) = {f(y) : f(y) = âˆ¨M[f(z)],z âˆˆ n(Bx)} - âˆ§Må’Œâˆ¨Måˆ†åˆ«è¡¨ç¤ºM-orderingçš„æœ€é«˜å’Œæœ€å°å³° 4. ä½¿ç”¨KLTå°†å½©è‰²å›¾åƒè½¬æ¢ä¸ºç°åº¦å›¾åƒï¼› 5. å›¾åƒäºŒå€¼åŒ–æ­¥éª¤ä¸­ï¼Œä½¿ç”¨äº†Otsué˜ˆå€¼ï¼ˆæŒ‡äº®åº¦çš„èƒ½é‡ï¼‰ä»¥è·å¾—å¯é çš„äºŒå€¼å›¾åƒï¼› ç»“æœ å›¾3ç¤ºå‡ºäº†å»é™¤æ²¹æ€§äº®ç‚¹çš„ç»“æœï¼š å›¾4æ¯”è¾ƒäº†ä½¿ç”¨BSRæ“ä½œæ—¶çš„çº¿è·¯æ£€æµ‹ï¼Œå¹¶æ˜¾ç¤ºäº†å¯¹äºŒå€¼åŒ–å’Œç»†åŒ–æ“ä½œçš„æ˜æ˜¾å½±å“ã€‚å¸¦æœ‰BSRçš„äºŒå€¼åŒ–å›¾åƒå…·æœ‰å‡å°‘çš„äº®ç‚¹åå°„ï¼Œå¹¶ä¸”åœ¨ç»†åŒ–å›¾åƒä¸­ï¼Œäº®ç‚¹è¢«è½¬æ¢ä¸ºå°åœ†åœˆã€‚å›¾4ï¼ˆfï¼‰æ˜¾ç¤ºï¼Œåœ¨çº¿æ£€æµ‹é˜¶æ®µï¼Œä½¿ç”¨BSRç”Ÿæˆçš„å›¾åƒå…·æœ‰è¾ƒå°‘çš„ä¸å¿…è¦çš„çº¿æ®µï¼š figure_4_comparisons_of_the_hair_thining_and_line_detection Multi-scale Line Detection Stage (MSLD) é‡‡ç”¨æ”¹è¿›çš„éœå¤«å˜æ¢(the Hough transform)ç®—æ³•æ¥æ£€æµ‹ä¸åŒçš„å¤´å‘é•¿åº¦ï¼Œå¹¶å‡å°‘ç”±äºå™ªå£°å¼•èµ·çš„ä»»ä½•é”™è¯¯æ£€æµ‹; å°†å¼¯æ›²çš„å¤´å‘è§†ä¸ºå¤šæ¡ç›´çº¿; ä¸ºäº†é¿å…åœ¨åº”ç”¨ç¨€ç–è¿‡ç¨‹æ—¶ä¸¢å¤±å¤´å‘ï¼Œæˆ‘ä»¬ä½¿ç”¨è¾¹ç¼˜ä¿¡æ¯(edge information)æ¥å‘ç°ä»»ä½•éšè—æˆ–é‡å çš„å¤´å‘ã€‚ #### æ€»ä½“ç»“æ„åˆ†æ ##### ç›®çš„ æå‡ºå¤šå°ºåº¦æ¡†æ¶æ¥æ˜¯ä¸ºäº†æé«˜å¤´å‘æ£€æµ‹çš„å‡†ç¡®æ€§ï¼› åº”ç”¨å¹³è¡Œçº¿æ†ç»‘ï¼ˆPLBï¼‰ç®—æ³•ï¼ˆparallel line bundling algorithmï¼‰æ¥è¿˜åŸä»»ä½•éšè—æˆ–é‡å çš„å¤´å‘ã€‚ æœ€åï¼Œå°†çŸ¢é‡åŒ–çš„çº¿æ®µç”¨ä½œæ¯›å‘æ ‡è®°å’Œè®¡æ•°æ¨¡å—çš„è¾“å…¥æ•°æ®ã€‚ æŠ€æœ¯ å¯¹HTåº”ç”¨äº†ä¸‰ä¸ªæ¯”ä¾‹çš„å›¾åƒï¼š1024Ã—768ï¼ˆåŸå§‹æ¯”ä¾‹ï¼‰ï¼Œ512Ã—384å’Œ256Ã—192ã€‚å¯¹ç¼©æ”¾æ¯”ä¾‹å›¾åƒåº”ç”¨äº†ä¸¤ç§å¤„ç†æ–¹æ³•ï¼šè¾¹ç¼˜æ£€æµ‹å’Œç»†åŒ–å¤„ç†ã€‚ PLBç®—æ³•åº”ç”¨äºè¾¹ç¼˜å›¾åƒä»¥å‘ç°ç¼ºå¤±çš„çº¿æ®µ HTè¢«åº”ç”¨äºç»†åŒ–å›¾åƒä»¥æå–çº¿æ®µã€‚é€šè¿‡åˆ©ç”¨PLBç®—æ³•ï¼Œå¯ä»¥æ¢å¤éšè—å’Œé‡å çš„å¤´å‘ã€‚ æœ€åï¼Œå°†çŸ¢é‡åŒ–çš„çº¿æ®µé‡æ–°ç¼©æ”¾ä¸ºåŸå§‹å°ºå¯¸1024Ã—768ï¼Œå¹¶ç”±é€»è¾‘æˆ–è¿ç®—ç¬¦è¿›è¡Œæ•´åˆã€‚ ç»“æœ å¦‚å›¾5æ‰€ç¤ºï¼Œç”±äºå¤´å‘çš„é•¿åº¦å’Œå·æ›²åº¦çš„å˜åŒ–ï¼Œä½¿ç”¨å•å°ºåº¦HTä¸èƒ½æ£€æµ‹æ‰€æœ‰çš„å¤´å‘ï¼š figure_5_comparison_of_the_single_scale_and_muti-scale_line_detection å›¾5ï¼ˆaï¼‰æ˜¾ç¤ºï¼Œå½“ä¸€æ ¹å¤´å‘å‡ ä¹å¹³è¡Œå‡ºç°ï¼Œä¸å…¶ä»–å¤´å‘é‡å æ—¶ï¼Œæˆ–è€…å¦‚æœå¤´å‘çš„æ›²ç‡è¶…è¿‡HTçš„å®¹é™ï¼Œé‚£ä¹ˆæœ€ç»ˆä¼šé—æ¼å¤§é‡çš„å¤´å‘ï¼Œè®¸å¤šçº¿æ®µæ ‡ç­¾é”™è¯¯ã€‚ å›¾5ï¼ˆbï¼‰ç¤ºå‡ºäº†ä½¿ç”¨æ¥è‡ªæ‰€æœ‰ç¼©æ”¾å›¾çš„çº¿æ®µçš„ç»“æœï¼Œä»è€Œæ”¹å–„äº†å•ä¸ªåˆ»åº¦çš„ä¸è¶³ã€‚ Parallel Line Bundling (PLB) åŸç† åº”ç”¨Cannyè¾¹ç¼˜æ£€æµ‹å™¨è·å¾—è¾¹ç¼˜å›¾ã€‚åœ¨å›¾6ä¸­ï¼Œå‡è®¾æ£€æµ‹åˆ°ä¸¤æ¡å¹³è¡Œçº¿Aå’ŒBï¼Œç”¨\\(ax + by + c_a = 0\\)å’Œ\\(ax + by + c_b = 0\\)è¡¨ç¤º, \\(d=\\frac{|c_a - c_b|} {\\sqrt{a^2 + b^2}}\\)è¡¨ç¤ºçº¿æ®µA,Bä¹‹é—´çš„è·ç¦»; figure_6_Thining_line_sandwiched_by_two_parallel_lines_with_the_distances_d è®¡ç®—å‡ºå¤¹æœ‰ç»†çº¿çš„å¹³è¡Œçº¿ä¹‹é—´çš„å¹³å‡è·ç¦»\\(d_{avg}\\)ã€‚ å¦‚æœ\\(d &gt; d_{avg}\\)ï¼Œåˆ™å½“\\((dd_{avg})&gt;w_{th}\\)æ—¶ï¼Œå°†å‘ç°éšè—çš„å¤´å‘ï¼Œå…¶ä¸­\\(w_{th}\\)è¡¨ç¤ºè¾¹ç•Œå› å­ï¼Œå°†é€šè¿‡å¤´çš®å›¾åƒçš„åˆ†è¾¨ç‡æ ¹æ®ç»éªŒè¿›è¡Œä¿®æ”¹ã€‚ ç»“æœ åœ¨å›¾7ï¼ˆaï¼‰ä¸­ï¼Œåœ†åœˆè¡¨ç¤ºéšè—çš„å¤´å‘ï¼Œå¦‚å›¾7ï¼ˆbï¼‰æ‰€ç¤ºï¼š Hair Labling and Counting ä½¿ç”¨MSLDæ¨¡å—ï¼Œå¾—å‡ºäº†ä¸€ç»„çº¿æ®µã€‚æ ¹æ®å¤´å‘çš„æ›²ç‡å’Œæ–¹å‘ï¼Œå°†å¤´å‘å®ç°ä¸ºå…·æœ‰ä¸åŒé•¿åº¦çš„åˆ†æ®µçº¿å‘é‡ç°‡ã€‚è¿™é¡¹ç ”ç©¶çš„ç›®çš„æ˜¯å‡†ç¡®è®¡ç®—å¤´çš®ä¸Šçš„æ¯›å‘æ•°é‡ã€‚è¿™å¯ä»¥çœ‹ä½œæ˜¯èšç±»å’Œæ ‡è®°é—®é¢˜ã€‚ç›®æ ‡æ˜¯å°†ä¸€ç»„çº¿æ®µç»„åˆæˆè¯­ä¹‰â€œå¤´å‘â€å¹¶åˆ†é…å”¯ä¸€çš„æ ‡ç­¾ã€‚ç”±äºæ¯æ ¹å¤´å‘éƒ½ç”±ç›¸äº’å…³è”çš„çº¿æ®µç»„æˆï¼Œå› æ­¤æˆ‘ä»¬ä¸ºæ¯ä¸ªç°‡åˆ†é…äº†å”¯ä¸€çš„æ ‡ç­¾ã€‚ æˆ‘ä»¬é‡‡ç”¨äº†æ¾å¼›æ ‡è®°ç®—æ³•(Relaxation labeling algorithm)æ¥è¯†åˆ«æ¯ä¸ªå•ç‹¬çš„çº¿æ®µï¼Œä»¥ç¡®å®šä¸å“ªä¸ªçº¿æ®µç›¸å…³è”ã€‚ å›¾8ï¼ˆaï¼‰æ˜¾ç¤ºäº†10ä¸ªå•ç‹¬çš„çº¿æ®µçš„ç¤ºä¾‹ï¼Œè¿™äº›çº¿æ®µè¢«æ ‡è®°ä¸ºæ¥è‡ªåŒä¸€æ ¹å¤´å‘ï¼Œç„¶åç»˜åˆ¶åˆ°\\(ï¼ˆÏï¼ŒÎ¸ï¼‰\\)åæ ‡ç³»ä¸Šï¼Œå¦‚å›¾8ï¼ˆbï¼‰æ‰€ç¤ºã€‚ äº¤å‰ç‚¹å¤„ç´¯ç§¯å›¾çš„ç»“æœå³°å€¼ä¸º10ã€‚ figure_8_Example_of_line_segment_labeling Relaxation Labeling (RL) æ¾å¼›æ ‡æ³¨ï¼ˆRLï¼‰æ˜¯ä¸€ç§ç›¸äº’å…³è”çš„å›å½’æ–¹æ³•ï¼Œå®ƒä½¿ç”¨ç¬¦å·æ¥æè¿°æ¨¡å‹çš„å½¢çŠ¶ã€‚ å®ƒæ—¨åœ¨å°†ç›®æ ‡å¯¹è±¡ï¼ˆå³æœ¬æ–‡ä¸­çš„çº¿æ®µï¼‰ä¸ç¬¦å·æˆ–æ‰€è°“çš„æ ‡è®°ï¼ˆå³å¤´å‘æ ‡ç­¾ï¼‰è¿›è¡ŒåŒ¹é…ã€‚RLç®—æ³•é¦–å…ˆåˆ†é…ä¸€ç»„éšæœºæ ‡è®°ã€‚ç„¶åï¼Œé€šè¿‡è¿­ä»£è®¡ç®—ï¼Œå¯ä»¥è·å¾—æ›´å‡†ç¡®ï¼Œæ›´ç²¾ç¡®çš„æ ‡è®°é›†ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼ŒRLç®—æ³•è¢«è§†ä¸ºç”¨äºæ ‡è®°æ¯ä¸ªçº¿æ®µçš„èšç±»æ–¹æ³•ã€‚ åŸç† 1. ä»¤\\(C(iï¼ŒÎ»ï¼Œjï¼ŒÎ»&#39;)\\)è¡¨ç¤ºçº¦æŸä¸ºÎ»çš„çº¿æ®µä¸çº¦æŸä¸ºÎ»'çš„çº¿æ®µjçš„å…¼å®¹æ€§ï¼Œå…¶çº¦æŸä¸º: \\(\\displaystyle\\sum_{Î»}C(iï¼ŒÎ»ï¼Œjï¼ŒÎ»&#39;) = 1\\) for âˆ€i,j,Î»,Î»'ã€‚ 2. å…¼å®¹æ€§Cè¡¨ç¤ºæ ‡è®°ä¸ºÎ»'çš„çº¿æ®µjå’Œæ ‡è®°ä¸ºÎ»çš„çº¿æ®µiä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§ã€‚ å¦‚æœå…¼å®¹æ€§ä»…ç”±åˆ°åŸç‚¹çš„è·ç¦»å†³å®šï¼Œåˆ™å¯èƒ½ä¼šå‡ºç°é”™è¯¯çš„è§£é‡Šã€‚ å› æ­¤å°†å…¼å®¹æ€§å®šä¹‰å¦‚ä¸‹ï¼š \\[ C(iï¼ŒÎ»ï¼Œjï¼ŒÎ»&#39;) = \\begin{cases} -1 &amp;\\text{if } i \\notin S_j \\\\ \\varepsilon |\\cos[\\theta_i(\\lambda) - \\theta_j(Î»&#39;)]| + (1-\\varepsilon) \\frac{\\rho_i(\\lambda)}{\\rho_j(Î»&#39;)} &amp;\\text{if } iâˆˆS_jâˆ©Î»{=}\\mathllap{/\\,}Î»&#39; \\\\ 0 &amp;\\text{otherwise } \\end{cases} \\] - ä¸Šå¼ä¸­ï¼š Îµè¡¨ç¤ºè·ç¦»å’Œæ–¹å‘ç½®ä¿¡åº¦ä¹‹é—´çš„åŠ æƒå› å­ \\(S_j\\)è¡¨ç¤ºçº¿æ®µjçš„ç›¸é‚»å‡è®¾ \\(Î¸_i(Î»)\\)è¡¨ç¤ºä»çº¿æ®µiåˆ°æ ‡è®°ä¸ºÎ»çš„çº¿æ®µjçš„æ–¹å‘ \\(Ï_i(Î»)\\)è¡¨ç¤ºåŸç‚¹åˆ°çº¿æ®µiå’Œæ ‡è®°ä¸ºÎ»çš„çº¿æ®µjä¹‹é—´çš„è·ç¦» å¦‚æœ\\(Ï_j(Î»)\\)é«˜, ä¸”\\(C(iï¼ŒÎ»ï¼Œjï¼ŒÎ»&#39;)\\)ä¸ºæ­£ï¼Œåˆ™\\(Ï_i(Î»)\\)å¢åŠ ã€‚ æ­¤æ ‡è®°ç®—æ³•æ˜¯ä¸€ä¸ªè¿­ä»£å¹¶è¡Œè¿‡ç¨‹ï¼Œç±»ä¼¼äºæ¦‚ç‡æ¾å¼›ä¸­ä½¿ç”¨çš„æ ‡è®°ä¸¢å¼ƒè§„åˆ™ï¼Œæ“ä½œå‘˜æ ¹æ®å…¶ä»–é‡é‡å’Œå…¼å®¹æ€§åå¤è°ƒæ•´æ ‡ç­¾é‡é‡ã€‚ å¯¹äºæ¯ä¸ªçº¿æ®µå’Œæ¯ä¸ªæ ‡ç­¾ï¼Œæ–°æƒé‡\\(q_i^{(r)}(\\lambda)\\)çš„è®¡ç®—å¦‚ä¸‹ï¼š \\[ q_i^{(r)}(\\lambda) = \\sum_{\\mathclap{j, j{=}\\mathllap{/\\,}i}} \\sum_{\\mathclap{Î»&#39;}}C(iï¼ŒÎ»ï¼Œjï¼ŒÎ»&#39;)p_j^{(r)}(Î»&#39;) \\] å…¶ä¸­: rè¡¨ç¤ºç¬¬ræ¬¡è¿­ä»£ åœ¨ç­‰å¼ä¸­ï¼Œä¹˜ç§¯å’Œæ˜¯è¢«æ ‡è®°ä¸ºÎ»çš„ç»™å®šçº¿æ®µiçš„æœŸæœ› \\(q_i^{(r)}(\\lambda)\\)æ˜¯å½“å‰èµ‹å€¼\\(p_j^{(r)}(Î»&#39;)\\)çš„åŠ æƒå’Œã€‚ æ–°ä»»åŠ¡å¯ä»¥ç”¨å·²ä¸‹å…¬å¼æ›´æ–°ï¼š \\[ p_i^{(r+1)}(\\lambda) = \\frac{p_i^{(r)}(Î»)[1+q_i^{(r)}(Î»)]}{\\displaystyle\\sum_{j=1}^mp_i^{(r)}(Î»&#39;)[1+q_i^{(r)}(Î»&#39;)]} \\] åœ¨è¿™é‡Œï¼Œåªé€‰æ‹©\\(p_i^{(r)}(\\lambda)\\)å’Œ\\(C(iï¼ŒÎ»ï¼Œjï¼ŒÎ»&#39;)\\)å¹¶åº”ç”¨è¯¥ç­‰å¼é€’å½’æ›´æ–°\\(p_i^{(r)}(\\lambda)\\)ï¼Œç›´åˆ°å®ƒä»¬åœæ­¢å˜åŒ–æˆ–æ”¶æ•›åˆ°1ã€‚å¯¹æ¯ä¸ªçº¿æ®µè¿›è¡Œè¿­ä»£éªŒè¯ï¼Œç›´åˆ°å°†å…¶åˆ†é…ç»™æ­£ç¡®çš„è¯­ä¹‰æ ‡ç­¾â€œ hairâ€ä¸ºæ­¢ã€‚ å®éªŒç»“æœ ä¸ºäº†è¯„ä¼°è¯¥ç³»ç»Ÿï¼Œæˆ‘ä»¬ä»UPMOSTï¼ˆUPG622ï¼‰DMCæ•è·äº†40ä¸ªåˆ†è¾¨ç‡ä¸º1024Ã—768çš„æ¯”ä¾‹å°ºå›¾åƒä½œä¸ºæµ‹è¯•æ•°æ®é›†ã€‚æ ¹æ®DMCçš„ç™½å¹³è¡¡ï¼Œæˆ‘ä»¬å°†æµ‹è¯•å›¾åƒåˆ†ä¸ºä¸¤ç»„ï¼Œåˆ†åˆ«æ˜¯æ•°æ®é›†1å’Œæ•°æ®é›†2ã€‚ Experiment 1: Cross-Validation of the Line Detection 1. å›¾10ï¼ˆbï¼‰-ï¼ˆdï¼‰æ˜¾ç¤ºäº†é¢„å¤„ç†æ¨¡å—çš„ç»“æœï¼ŒåŒ…æ‹¬äº®ç‚¹å»é™¤ï¼ˆBSRï¼‰ï¼ŒäºŒå€¼åŒ–å’Œç¨€åŒ–è¿‡ç¨‹ã€‚ ä¸ºäº†è¯„ä¼°å¤šå°ºåº¦çº¿æ£€æµ‹ç®—æ³•çš„æ€§èƒ½ï¼Œæµ‹è¯•å›¾åƒæˆ‘ä»¬å°†HTåº”ç”¨äºä¸‰ç§ä¸åŒå°ºåº¦ï¼Œä»¥æå–çº¿æ®µã€‚ ç„¶åä½¿ç”¨å¤´å‘æ ‡ç­¾æœºåˆ¶ç¡®å®šå¤´å‘çš„æ•°é‡ã€‚ 2. å¦‚å›¾10ï¼ˆeï¼‰-ï¼ˆfï¼‰æ‰€ç¤ºï¼Œå½©è‰²çº¿ä»£è¡¨å¤´å‘çš„æ ‡ç­¾ã€‚ æ¢å¥è¯è¯´ï¼Œå³ä½¿å¤´å‘äº¤å‰æˆ–é‡å ï¼Œä¹Ÿå¯ä»¥å‡†ç¡®åœ°æ ‡è®°å¤´å‘ã€‚ ä»¥ä¸‹éƒ¨åˆ†æ¼”ç¤ºäº†æœ‰å…³ç²¾ç¡®åº¦å’Œå¬å›ç‡çš„æ¯›å‘è®¡æ•°çš„å®¢è§‚æµ‹é‡ã€‚ æˆ‘ä»¬æ¯”è¾ƒäº†ä½¿ç”¨BSRå’ŒMSLDæ¨¡å—ç»„åˆçš„å››ç§æƒ…å†µï¼ŒåŒ…æ‹¬BSR + MSLDï¼Œè€Œæ²¡æœ‰åŒæ—¶ä½¿ç”¨BSRå’ŒMSLDï¼Œä»…BSRå’Œä»…MSLDã€‚ table_1_comarison_of_the_system_sensitivity_forthe_module_usages_in_the_line_detection è¡¨Iæ¯”è¾ƒäº†åŸºäºæ¨¡å—ä½¿ç”¨æƒ…å†µçš„ç³»ç»Ÿæ•æ„Ÿæ€§ã€‚åŸºäºå½¢æ€å­¦çš„BSRå’ŒMSLDæœºåˆ¶ï¼Œä»¥æé«˜å¤´å‘æ£€æµ‹çš„æ€§èƒ½ï¼Œå…¶å‡†ç¡®ç‡åˆ†åˆ«ä¸º94.98ï¼…å’Œ98.05ï¼…ã€‚ä¸ä¼ ç»Ÿçš„HTçº¿æ£€æµ‹æ–¹æ³•ç›¸æ¯”ï¼Œæ•°æ®é›†1å’Œæ•°æ®é›†2åˆ†åˆ«æé«˜äº†2ï¼…å’Œ1.5ï¼…ã€‚ä»å¬å›ç‡çš„è§’åº¦æ¥çœ‹ï¼Œå®ƒä¹Ÿé«˜äºå…¶ä»–æ¨¡å—ç»„åˆã€‚ å¹³å‡è€Œè¨€ï¼Œæˆ‘ä»¬çš„å¬å›ç‡åˆ†åˆ«ä¸º9ï¼…å’Œ10ï¼…ã€‚æ ¹æ®æˆ‘ä»¬çš„è§‚å¯Ÿï¼Œåº”ç”¨BSRåï¼Œå‡†ç¡®ç‡å¾—åˆ°äº†æé«˜ã€‚ ä½†æ˜¯ï¼Œå°†MSLDåº”ç”¨äºæ•°æ®é›†ï¼ƒ2æ—¶ï¼Œå‡†ç¡®ç‡ä¸‹é™ï¼Œå› ä¸ºå®ƒåœ¨æ²¹æ€§å¤´çš®åŒºåŸŸä¸­äº§ç”Ÿäº†å¤§é‡çš„å…‰åå°„ã€‚åŒæ ·ï¼Œå¤´å‘ä¸­éƒ¨çš„äº®ç‚¹ä¹Ÿè¢«å¤§å¤§å¢å¼ºï¼Œå¯¼è‡´ç²¾ç¡®åº¦é™ä½ã€‚ Experiment 2: System Refinement Using the PLB Algorithm 1. è¡¨IIæ˜¾ç¤ºäº†PLBç®—æ³•çš„æ•ˆç‡ã€‚PLBæ–¹æ³•ä½¿ç³»ç»Ÿèƒ½å¤Ÿæå–ç¼ºå¤±çš„æ¯›å‘ï¼Œç²¾ç¡®ç‡æé«˜äº†0.5ï¼…ï¼Œå¬å›ç‡æé«˜äº†5ï¼…ã€‚ 2. PLBç®—æ³•å¯ä»¥ç”¨ä½œç³»ç»Ÿå¾®è°ƒè¿‡ç¨‹ï¼Œå°†ç³»ç»Ÿæ€§èƒ½å¹³å‡æé«˜åˆ°96.89ï¼…ã€‚è¿™æ˜¯åˆç†çš„ï¼Œå› ä¸ºéšè—çš„å¤´å‘ä¸ç»å¸¸å‡ºç°ã€‚ 3. æ­¤å¤–ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ˜¾å¾®é•œæ§åˆ¶å¤´å‘å›¾åƒçš„åˆ†è¾¨ç‡å’Œè§’åº¦ï¼Œä»¥é¿å…è¿™ç§æƒ…å†µã€‚ Experiment 3: Complexity Analysis table_3_time_occupancy_of_each_module_compared_with_total_execution å½“æ¶‰åŠåˆ°ç³»ç»Ÿå¤æ‚æ€§åˆ†ææ—¶ï¼Œæˆ‘ä»¬çŸ¥é“æ‹Ÿè®®çš„å¤´å‘è®¡æ•°ç³»ç»Ÿä¼šèŠ±è´¹æ›´å¤šæ—¶é—´ã€‚å¹³å‡è€Œè¨€ï¼Œé«˜åˆ†è¾¨ç‡æµ‹è¯•å›¾åƒéœ€è¦ä¸åˆ°4ç§’çš„æ—¶é—´å³å¯å¾—å‡ºæ¯›å‘è®¡æ•°ä¿¡æ¯ã€‚ è¡¨IIIåˆ—å‡ºäº†æ¯ä¸ªæ¨¡å—æ‰€éœ€çš„æ—¶é—´ä¸æ€»æ‰§è¡Œæ—¶é—´çš„æ¯”è¾ƒã€‚ MSLDä½¿ç”¨ä¸‰ä¸ªæ¯”ä¾‹å°ºæ¥è·å–ä¸€ç»„çº¿æ®µï¼Œè¿™éœ€è¦å¤§éƒ¨åˆ†æ‰§è¡Œæ—¶é—´ã€‚ HTå°†è¾¹ç¼˜æŠ•å½±åˆ°ï¼ˆÏï¼ŒÎ¸ï¼‰ç©ºé—´ä»¥æå–å±€éƒ¨æœ€å¤§å€¼çš„è¿‡ç¨‹éå¸¸è€—æ—¶ã€‚ ä¸ºäº†å…‹æœè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åœ¨MSLDä¸­ä»…ä½¿ç”¨äº†ä¸¤ä¸ªæ ‡åº¦ã€‚ä½†æ˜¯ï¼Œæ­¤å°è¯•å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ æ­¤å¤–ï¼Œç»†åŒ–è¿‡ç¨‹å ç”¨äº†æ€»å¤„ç†æ—¶é—´çš„å››åˆ†ä¹‹ä¸€ä»¥ä¸Šã€‚è¿™æ˜¯æ‰§è¡Œæ—¶é—´å’Œç³»ç»Ÿç²¾åº¦ä¹‹é—´çš„æƒè¡¡ã€‚ æ€»ç»“ è¿™é¡¹ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªåŠ¨çš„å¤´å‘åˆ†å‰²å’Œè®¡æ•°ç³»ç»Ÿï¼Œä»¥å‡å°‘äººå·¥è¯„ä¼°è€…è¿›è¡Œè¯¦ç»†å¤´çš®è¯„ä¼°æ‰€éœ€çš„æ—¶é—´ã€‚ 1. é¦–å…ˆï¼Œæ²¹æ€§å’Œæ¹¿æ¶¦çš„å¤´å‘ä¼šåœ¨å¤´å‘ä¸­é—´äº§ç”Ÿäº®ç‚¹ã€‚åœ¨è®¡ç®—å¤´å‘æ•°ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ¶ˆé™¤å¤´å‘ä¸Šçš„äº®ç‚¹ï¼Œä»¥é¿å…é‡å¤è®¡ç®—ä¸€äº›å¤´å‘çš„é—®é¢˜ã€‚ 2. ç¬¬äºŒï¼Œæ³¢æµªçŠ¶å’Œå·å‘å®¹æ˜“å¯¼è‡´çº¿æ£€æµ‹æ•…éšœã€‚å½“å¤´å‘ä¸ç›´æ—¶ï¼Œå¸¸è§„çš„çº¿æ£€æµ‹ç®—æ³•æ— æ•ˆã€‚ 3. ç¬¬ä¸‰ï¼Œå½“å¤´å‘ç›¸äº’äº¤å‰å¹¶äº’ç›¸å’¬åˆæ—¶ï¼Œä¼šå‡ºç°å¯¹å¤´å‘æ•°é‡çš„ä½ä¼°ï¼Œè¿™ä½¿å¾—ç²¾ç¡®å®šä½æ‰€æœ‰å¤´å‘éå¸¸å›°éš¾ã€‚ 4. æœ€åï¼Œç”±äºå¤´çš®ç›¸å¯¹æœªæ›å…‰ï¼Œå› æ­¤å¤´çš®çš„å›¾åƒé€šå¸¸æ¨¡ç³Šæˆ–éš¾ä»¥çœ‹è§ã€‚å¦å¤–ï¼Œå¤´çš®é€šå¸¸ç…§æ˜ä¸è¶³æˆ–æ›å…‰è¿‡åº¦ã€‚ è®¡æ•°æ€è·¯ é€šè¿‡å¯¹ä¸åŒç¼©ç•¥å›¾ä¸­çš„å¤´å‘åšHTçº¿æ®µæ£€æµ‹ï¼Œç„¶åå°†æ‰€æœ‰æ£€æµ‹ç»“æœè¿›è¡Œâ€œé€»è¾‘æˆ–â€æ•´åˆï¼Œä»¥å‡å°‘ç¼ºå¤±å¤´å‘çš„è®¡æ•°ï¼Œè§£å†³é‡å å¤´å‘çš„æ¼è®¡æ•°çš„é—®é¢˜ã€‚ æœ¬æ–‡çš„æ¡†æ¶å¯ä»¥è¢«è§†ä¸ºè¿ˆå‘åŒ–å¦†å“å’Œå¤´çš®æ²»ç–—åº”ç”¨çš„æ™ºèƒ½è®¡ç®—æœºè¾…åŠ©åŒ»å­¦å›¾åƒå¤„ç†çš„ç¬¬ä¸€æ­¥ã€‚ æŠ€æœ¯æ€»ç»“ é¢„å¤„ç†é˜¶æ®µ å¯¹è¾“å…¥æ•°æ®ï¼ˆå¤´çš®å›¾ç‰‡ï¼‰è¿›è¡Œé¢„å¤„ç†ï¼Œä¸ºä¸‹ä¸€é˜¶æ®µå¯¹å›¾ç‰‡ä¸­å¤´å‘è¿›è¡Œç²¾å‡†è®¡æ•°ç­‰åŠŸèƒ½æ€§æ“ä½œåšå¥½é“ºå«ã€‚ä¸»è¦ç”¨åˆ°ä»¥ä¸‹æŠ€æœ¯ï¼š #### Contrast Stretching (Normalization): - å¯¹æ¯”åº¦æ‹‰ä¼¸ï¼ˆé€šå¸¸ç§°ä¸ºå½’ä¸€åŒ–ï¼‰æ˜¯ä¸€ç§ç®€å•çš„å›¾åƒå¢å¼ºæŠ€æœ¯,æ—¨åœ¨é€šè¿‡â€œæ‹‰ä¼¸â€å›¾åƒæ‰€åŒ…å«çš„å¼ºåº¦å€¼èŒƒå›´ä»¥è¦†ç›–æ‰€éœ€çš„å€¼èŒƒå›´æ¥æ”¹å–„å›¾åƒçš„å¯¹æ¯”åº¦ã€‚ - æŠ€æœ¯åº”ç”¨ï¼šå¢åŠ å¤´çš®ä¸å¤´å‘ä¹‹é—´çš„å¯¹æ¯”åº¦ï¼Œæ–¹ä¾¿ä¸‹é˜¶æ®µå¯¹å¤´å‘è¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚ Color Morphology EXTENDING MATHEMATICAL MORPHOLOGY TO COLOR IMAGE PROCESSING æŠ€æœ¯åº”ç”¨ï¼š é™¤å™ªï¼Œå»é™¤æ²¹æ€§å’Œæ¹¿æ¶¦çš„å¤´å‘åœ¨å¤´å‘çš„ä¸­éƒ¨äº§ç”Ÿçš„äº®ç‚¹ Karhunen-Loeve Transform (KLT) Karhunen-Loeveå˜æ¢ï¼ˆKLTï¼‰ï¼ˆä¹Ÿç§°ä¸ºHotellingå˜æ¢å’Œç‰¹å¾å‘é‡å˜æ¢ï¼‰ï¼Œå®ƒä¸ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰å¯†åˆ‡ç›¸å…³ï¼Œå¹¶å¹¿æ³›ç”¨äºè®¸å¤šé¢†åŸŸçš„æ•°æ®åˆ†æä¸­, KLå˜æ¢åŸºäºå›¾åƒçš„ç»Ÿè®¡å±æ€§ï¼Œå¹¶å…·æœ‰ä¸€äº›é‡è¦çš„å±æ€§ï¼Œä½¿å…¶å¯ç”¨äºå›¾åƒå¤„ç†ï¼Œç‰¹åˆ«æ˜¯å›¾åƒå‹ç¼©ã€‚ æŠ€æœ¯åº”ç”¨ï¼š å°†å½©è‰²å›¾åƒè½¬æ¢ä¸ºç°åº¦å›¾åƒã€‚ Otsu thresholding Otsu thresholdingï¼šç®€å•è¯´æ¥è¯¥æ–¹æ³•å¯å°†ç°åº¦å›¾åƒè¿˜åŸä¸ºäºŒè¿›åˆ¶å›¾åƒã€‚åœ¨å›¾åƒå¤„ç†å’Œåˆ†æä¸­ï¼Œæœ‰æ—¶éœ€è¦ä¸€ç§æ–¹æ³•æ¥åˆ†ç¦»ä¸¤ä¸ªç›¸å…³æ•°æ®ï¼Œä¾‹å¦‚èƒŒæ™¯å’Œå‰æ™¯ã€‚Otsué˜ˆå€¼æ˜¯ä¸€ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥è‡ªé€‚åº”åœ°æ‰¾åˆ°æœ€ä½³é˜ˆå€¼ä»¥åŒºåˆ†ä¸¤ç±»æ•°æ®ã€‚ æŠ€æœ¯åº”ç”¨ï¼š å›¾åƒåˆ†å‰²å’Œå›¾åƒäºŒå€¼åŒ–ã€‚ å¤šå°ºåº¦çº¿æ£€æµ‹é˜¶æ®µï¼ˆMSLDï¼‰ è¯¥é˜¶æ®µä¸»è¦æ˜¯é€šè¿‡å„ç§æŠ€æœ¯ï¼Œå…‹æœé‡å å¤´å‘æ— æ³•è®¡æ•°ï¼Œä»¥åŠç›¸é‚»å¤´å‘ä¹‹é—´çš„å¤´çš®è¢«è¯†åˆ«ä¸ºå¤´å‘ç­‰å½±å“å¤´å‘è®¡æ•°çš„ç›¸å…³é—®é¢˜ã€‚ä¸ºä¸‹é˜¶æ®µå¯¹å¤´å‘è¿›è¡Œç²¾å‡†è®¡æ•°çš„åšå¥½é“ºå«ã€‚ä¸»è¦ç”¨åˆ°ä»¥ä¸‹æŠ€æœ¯ï¼š éœå¤«å˜æ¢ï¼ˆHT) éœå¤«å˜æ¢ï¼ˆHTï¼‰ éœå¤«å˜æ¢æ˜¯ä¸€ç§å¯ç”¨äºéš”ç¦»å›¾åƒä¸­ç‰¹å®šå½¢çŠ¶çš„ç‰¹å¾çš„æŠ€æœ¯ã€‚å› ä¸ºå®ƒè¦æ±‚ä»¥æŸç§å‚æ•°å½¢å¼æŒ‡å®šæ‰€éœ€çš„ç‰¹å¾ï¼Œæ‰€ä»¥ç»å…¸çš„Houghå˜æ¢æœ€å¸¸ç”¨äºæ£€æµ‹è§„åˆ™æ›²çº¿ï¼ˆä¾‹å¦‚ç›´çº¿ï¼Œåœ†ï¼Œæ¤­åœ†ç­‰ï¼‰ã€‚ æŠ€æœ¯åº”ç”¨ï¼š HTæ˜¯æœ€å¸¸ç”¨çš„çº¿æ£€æµ‹æ¡†æ¶ä¹‹ä¸€ã€‚ä½†æ˜¯å½“ä½¿ç”¨å¸¸è§„çš„å•å°ºåº¦HTæ—¶ï¼Œå¯èƒ½ä¼šä¸¢å¤±å¤§é‡çš„å¤´å‘æ®µã€‚æ•…æå‡ºå°†HTåº”ç”¨äºä¸‰ä¸ªæ¯”ä¾‹çš„å›¾åƒï¼ŒåŒ…æ‹¬1024Ã—768ï¼ˆåŸå§‹æ¯”ä¾‹ï¼‰ï¼Œ512Ã—384å’Œ256Ã—192ï¼Œæœ€åé€šè¿‡é€»è¾‘æˆ–å°†ä¸‰ä¸ªæ¯”ä¾‹å›¾åƒä¸­çš„è¢«æ£€æµ‹å‡ºçš„å¤´å‘è¿›è¡Œæ•´åˆï¼Œä»¥æé«˜å¤´å‘è®¡æ•°å‡†ç¡®åº¦ã€‚ Canny Edge Detection Cannyè¾¹ç¼˜æ£€æµ‹æ˜¯ä¸€ç§å¤šæ­¥éª¤ç®—æ³•ï¼Œå¯ä»¥åŒæ—¶æ£€æµ‹åˆ°å™ªå£°è¢«æŠ‘åˆ¶çš„è¾¹ç¼˜ã€‚åœ¨å›¾åƒäºŒå€¼åŒ–æ­¥éª¤ä¸­ï¼Œç”±äºé”™è¯¯åœ°å‡å®šäº†ä¸¤æ ¹å•ç‹¬çš„å¤´å‘çš„è¿æ¥ï¼Œä½äºä¸¤æ ¹å¤´å‘ä¹‹é—´çš„å¤´çš®åƒç´ è¢«æ ‡è®°ä¸ºå¤´å‘çš„ä¸€éƒ¨åˆ†ã€‚è€Œä¸”ï¼Œå½“ä¸¤æ ¹å¤´å‘å¤ªé è¿‘æˆ–å½¼æ­¤é‡å æ—¶ï¼Œå¦‚æœç›´æ¥åº”ç”¨ç¨€ç–ç®—æ³•ï¼Œåˆ™ä¼šé”™è¿‡ä¸€æ ¹æˆ–ä¸¤æ ¹å¤´å‘ã€‚ æŠ€æœ¯åº”ç”¨ï¼š ä½¿ç”¨è¾¹ç¼˜ä¿¡æ¯æ¥æ‰¾å‡ºéšè—æˆ–é‡å çš„å¤´å‘ã€‚å¯ä»¥ä»éšè—çš„å¤´å‘æˆ–é‡å çš„å¤šæ ¹å¤´å‘ä¸­æå–ä¸¤ä¸ªå¹³è¡Œçš„è¾¹ç¼˜ã€‚ Thinning Parallel Line Bundling (PLB) å¤´å‘æ ‡è®°å’Œè®¡æ•°é˜¶æ®µï¼ˆHair Labling and Countingï¼‰ è¯¥é˜¶æ®µçš„ç›®çš„æ˜¯å‡†ç¡®è®¡ç®—å¤´çš®ä¸Šçš„æ¯›å‘æ•°é‡ã€‚è¿™å¯ä»¥çœ‹ä½œæ˜¯èšç±»å’Œæ ‡è®°é—®é¢˜ã€‚ç›®æ ‡æ˜¯å°†ä¸€ç»„çº¿æ®µç»„åˆæˆè¯­ä¹‰â€œå¤´å‘â€å¹¶åˆ†é…å”¯ä¸€çš„æ ‡ç­¾ã€‚ ä¸»è¦ç”¨åˆ°ä»¥ä¸‹æŠ€æœ¯ï¼š Relaxation Labeling æ¾å¼›æ ‡è®°æ˜¯ä¸€ç§å›¾åƒå¤„ç†æ–¹æ³•ã€‚å…¶ç›®æ ‡æ˜¯å°†æ ‡ç­¾ä¸ç»™å®šå›¾åƒçš„åƒç´ æˆ–ç»™å®šå›¾çš„èŠ‚ç‚¹ç›¸å…³è”ã€‚ æŠ€æœ¯åº”ç”¨ï¼š åœ¨æœ¬ç ”ç©¶ä¸­ï¼ŒRLç®—æ³•è¢«è§†ä¸ºç”¨äºæ ‡è®°æ¯ä¸ªçº¿æ®µçš„èšç±»æ–¹æ³•ã€‚ å‚è€ƒæ–‡çŒ® An Unsupervised Hair Segmentation and Counting System in Microscopy Images Contrast Stretching (Normalization) Color Morphology EXTENDING MATHEMATICAL MORPHOLOGY TO COLOR IMAGE PROCESSING Karhunen-Loeve Transform (KLT) Otsu thresholding Hough Transformï¼ˆHT) Canny Edge Detection Thinning Evaluation of a Bundling Technique for Parallel Coordinates Relaxation Labeling","categories":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Paper Smash","slug":"Technology/Paper-Smash","permalink":"https://littlelittlemoon.github.io/categories/Technology/Paper-Smash/"}],"tags":[{"name":"Hair Counting","slug":"Hair-Counting","permalink":"https://littlelittlemoon.github.io/tags/Hair-Counting/"},{"name":"Digital Image Processing","slug":"Digital-Image-Processing","permalink":"https://littlelittlemoon.github.io/tags/Digital-Image-Processing/"},{"name":"Line Detection","slug":"Line-Detection","permalink":"https://littlelittlemoon.github.io/tags/Line-Detection/"},{"name":"Paper","slug":"Paper","permalink":"https://littlelittlemoon.github.io/tags/Paper/"}],"keywords":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Paper Smash","slug":"Technology/Paper-Smash","permalink":"https://littlelittlemoon.github.io/categories/Technology/Paper-Smash/"}]},{"title":"Evalution of hair and scalp condition based on microscopy image analysis | Hair Counting - part 1","slug":"Evalution of hair and scalp condition based on microscopy image analysis  | Hair Counting - 1","date":"2019-12-24T12:35:33.000Z","updated":"2020-04-09T07:00:21.540Z","comments":true,"path":"2019/12/24/Evalution of hair and scalp condition based on microscopy image analysis  | Hair Counting - 1/","link":"","permalink":"https://littlelittlemoon.github.io/2019/12/24/Evalution%20of%20hair%20and%20scalp%20condition%20based%20on%20microscopy%20image%20analysis%20%20|%20Hair%20Counting%20-%201/","excerpt":"","text":"Paper: Evalution of hair and scalp condition based on microscopy image analysis è®¡æ•°æ–¹æ³• å…¸å‹çš„ä¾¿æºå¼æ˜¾å¾®é•œç›¸æœºé€šå¸¸è¦†ç›–5mm x 5mmçš„çŸ©å½¢ã€‚å› æ­¤ï¼Œå¦‚æœä¸€æ ¹å¤´å‘é•¿äº5æ¯«ç±³ï¼Œåˆ™å®ƒå¿…é¡»è¶…å‡ºçŸ©å½¢ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬å¯¹æ¯›å‘è®¡æ•°æœ‰ä¸¤ä¸ªå‡è®¾ï¼š 1. æ¯æ ¹å¤´å‘ç”±ä¸€ä¸ªèµ·ç‚¹å’Œä¸€ä¸ªç»ˆç‚¹è¡¨ç¤º; 2. èµ·ç‚¹ä½äºçŸ©å½¢å†…éƒ¨ï¼Œç»ˆç‚¹ä½äºå›¾åƒçš„è¾¹ç•Œä¸Šã€‚ å¤´å‘è®¡æ•°ï¼šä½¿ç”¨é¢„å¤„ç†é˜¶æ®µè·å¾—çš„éª¨æ¶å›¾åƒï¼ŒåŸºäºä¸Šè¿°ä¸¤ç‚¹å‡è®¾ï¼Œå¦‚æœå¤´å‘çš„éª¨æ¶çº¿çš„ä¸€ä¸ªç‚¹åœ¨çŸ©å½¢å†…è€Œå¦ä¸€ç‚¹åœ¨è¾¹ç•Œä¸Šï¼Œåˆ™æˆ‘ä»¬å¯¹å¤´å‘è¿›è¡Œè®¡æ•°ã€‚ fig_3_Example_of_hair_counting å›¾3ç¤ºå‡ºäº†å¤´å‘è®¡æ•°çš„ç¤ºä¾‹ã€‚åœ¨å›¾ä¸­ï¼Œè®¡æ•°çš„åƒç´ ç”¨çº¢è‰²ç‚¹æ ‡è®°ï¼Œå¹¶ä¸”è¿™äº›ç‚¹å åŠ åœ¨åŸå§‹å›¾åƒä¸Šã€‚ç„¶åï¼Œå¯ä»¥é€šè¿‡å°†è®¡æ•°çš„åƒç´ æ•°é™¤ä»¥å›¾åƒå°ºå¯¸æ¥è®¡ç®—å¤´å‘å¯†åº¦ã€‚ å®éªŒç»“æœ ä¸ºäº†è¯„ä¼°å¤´å‘è®¡æ•°ç®—æ³•ï¼Œæˆ‘ä»¬å¯¹200ä¸ªå¤´çš®å›¾åƒçš„æ•°æ®é›†è¿›è¡Œäº†å®éªŒã€‚ è¡¨2åˆ—å‡ºäº†æ ¹æ®æˆ‘ä»¬çš„ç®—æ³•è®¡ç®—å‡ºçš„å®é™…æ¯›å‘æ•°ä¸ä¼°è®¡æ¯›å‘æ•°ä¹‹é—´çš„ç²¾ç¡®åº¦/å¬å›ç‡ã€‚ å¹³å‡å‡†ç¡®åº¦å’Œå¬å›ç‡åˆ†åˆ«ä¸º91.35ï¼…å’Œ92.01ï¼…ã€‚å³ä½¿æ€§èƒ½ç›¸å½“å¥½ï¼Œç²¾åº¦/è°ƒç”¨ç‡ä¹Ÿå¯ä»¥è¿›ä¸€æ­¥æé«˜ã€‚é”™è¯¯çš„ä¸€ä¸ªå…³é”®åŸå› æ˜¯é¢„å¤„ç†å’ŒåŸå§‹å›¾åƒçš„è´¨é‡ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œä¸€æ—¦å›¾åƒä¸­å‡ºç°æ¨¡ç³Šç‚¹ï¼Œå°±ä¸å¯èƒ½åœ¨é¢„å¤„ç†æ­¥éª¤ä¸­æ¶ˆé™¤æ‰€æœ‰å™ªéŸ³ã€‚å› æ­¤ï¼Œè€ƒè™‘åˆ°ç›¸æœºå™ªå£°ï¼Œç²¾åº¦éå¸¸å¥½ã€‚ åŒæ ·ï¼Œæˆ‘ä»¬æµ‹è¯•äº†æ¯›å­”è®¡æ•°ç®—æ³•ï¼Œç»“æœæ˜¾ç¤ºå¹³å‡å‡†ç¡®ç‡çº¦ä¸º90ï¼…ã€‚ æ€»ç»“ è®¡æ•°æ€è·¯ å°†å›¾ç‰‡è¾¹ç¼˜çœ‹ä½œä¸€çŸ©å½¢ï¼› å¯¹æ»¡è¶³â€œå¤´å‘ç”±ä¸€ä¸ªèµ·ç‚¹å’Œä¸€ä¸ªç»ˆç‚¹è¡¨ç¤ºï¼Œä¸”èµ·ç‚¹ä½äºçŸ©å½¢å†…éƒ¨ï¼Œç»ˆç‚¹ä½äºå›¾åƒçš„è¾¹ç•Œä¸Šâ€æ¡ä»¶çš„å¤´å‘è¿›è¡Œè®¡æ•°ï¼› ä½¿ç”¨é€šè¿‡ç»†åŒ–æ“ä½œè·å¾—çš„å¤´å‘éª¨æ¶å›¾åƒå¯¹æ»¡è¶³æ¡ä»¶çš„å¤´å‘è¿›è¡Œè®¡æ•°ï¼› é€šè¿‡å°†è®¡æ•°çš„åƒç´ æ•°é™¤ä»¥å›¾åƒå°ºå¯¸æ¥è®¡ç®—å¤´å‘å¯†åº¦ã€‚ å‚è€ƒæ–‡çŒ® Evalution of hair and scalp condition based on microscopy image analysis","categories":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Paper Smash","slug":"Technology/Paper-Smash","permalink":"https://littlelittlemoon.github.io/categories/Technology/Paper-Smash/"}],"tags":[{"name":"Hair Counting","slug":"Hair-Counting","permalink":"https://littlelittlemoon.github.io/tags/Hair-Counting/"},{"name":"Digital Image Processing","slug":"Digital-Image-Processing","permalink":"https://littlelittlemoon.github.io/tags/Digital-Image-Processing/"},{"name":"Line Detection","slug":"Line-Detection","permalink":"https://littlelittlemoon.github.io/tags/Line-Detection/"},{"name":"Paper","slug":"Paper","permalink":"https://littlelittlemoon.github.io/tags/Paper/"}],"keywords":[{"name":"Technology","slug":"Technology","permalink":"https://littlelittlemoon.github.io/categories/Technology/"},{"name":"Paper Smash","slug":"Technology/Paper-Smash","permalink":"https://littlelittlemoon.github.io/categories/Technology/Paper-Smash/"}]},{"title":"Hexo-Theme-Sakura","slug":"Hexo-Theme-Sakura","date":"2018-12-12T14:16:01.000Z","updated":"2020-04-10T11:55:43.302Z","comments":true,"path":"2018/12/12/Hexo-Theme-Sakura/","link":"","permalink":"https://littlelittlemoon.github.io/2018/12/12/Hexo-Theme-Sakura/","excerpt":"","text":"hexo-theme-sakuraä¸»é¢˜ English document åŸºäºWordPressä¸»é¢˜Sakuraä¿®æ”¹æˆHexoçš„ä¸»é¢˜ã€‚ demoé¢„è§ˆ æ­£åœ¨å¼€å‘ä¸­...... äº¤æµç¾¤ è‹¥ä½ æ˜¯ä½¿ç”¨è€…ï¼ŒåŠ ç¾¤QQ: 801511924 è‹¥ä½ æ˜¯åˆ›ä½œè€…ï¼ŒåŠ ç¾¤QQ: 194472590 ä¸»é¢˜ç‰¹æ€§ é¦–é¡µå¤§å±è§†é¢‘ é¦–é¡µéšæœºå°é¢ å›¾ç‰‡æ‡’åŠ è½½ valineè¯„è®º fancy-boxç›¸å†Œ pjaxæ”¯æŒï¼ŒéŸ³ä¹ä¸é—´æ–­ aplayeréŸ³ä¹æ’­æ”¾å™¨ å¤šçº§å¯¼èˆªèœå•ï¼ˆæŒ‰ç°åœ¨å¤§éƒ¨åˆ†hexoä¸»é¢˜æ¥è¯´ï¼Œè¿™ä¹Ÿç®—æ˜¯ä¸ªç‰¹æ€§äº†ï¼‰ èµèµä½œè€… å¦‚æœå–œæ¬¢hexo-theme-sakuraä¸»é¢˜ï¼Œå¯ä»¥è€ƒè™‘èµ„åŠ©ä¸€ä¸‹å“¦~éå¸¸æ„Ÿæ¿€ï¼ paypal | Alipay æ”¯ä»˜å® | WeChat Pay å¾®ä¿¡æ”¯ä»˜ æœªå®Œå–„çš„ä½¿ç”¨æ•™ç¨‹ é‚£å•¥ï¼Ÿè€å®è¯´æˆ‘ç›®å‰ä¹Ÿä¸æ˜¯å¾ˆæœ‰æ¡ç†233333333~ 1ã€ä¸»é¢˜ä¸‹è½½å®‰è£… hexo-theme-sakuraå»ºè®®ä¸‹è½½å‹ç¼©åŒ…æ ¼å¼ï¼Œå› ä¸ºé™¤äº†ä¸»é¢˜å†…å®¹è¿˜æœ‰äº›sourceçš„é…ç½®å¯¹æ–°æ‰‹æ¥è¯´æ¯”è¾ƒå¤ªéº»çƒ¦ï¼Œç›´æ¥ä¸‹è½½è§£å‹å°±çœå»è¿™äº›éº»çƒ¦å’¯ã€‚ ä¸‹è½½å¥½åè§£å‹åˆ°åšå®¢æ ¹ç›®å½•ï¼ˆä¸æ˜¯ä¸»é¢˜ç›®å½•å“¦ï¼Œé‡å¤çš„é€‰æ‹©æ›¿æ¢ï¼‰ã€‚æ¥ç€åœ¨å‘½ä»¤è¡Œï¼ˆcmdã€bashï¼‰è¿è¡Œnpm iå®‰è£…ä¾èµ–ã€‚ 2ã€ä¸»é¢˜é…ç½® åšå®¢æ ¹ç›®å½•ä¸‹çš„_configé…ç½® ç«™ç‚¹ # Site title: ä½ çš„ç«™ç‚¹å subtitle: description: ç«™ç‚¹ç®€ä»‹ keywords: author: ä½œè€…å language: zh-cn timezone: éƒ¨ç½² deploy: type: git repo: github: ä½ çš„githubä»“åº“åœ°å€ # coding: ä½ çš„codingä»“åº“åœ°å€ branch: master å¤‡ä»½ ï¼ˆä½¿ç”¨hexo bå‘å¸ƒå¤‡ä»½åˆ°è¿œç¨‹ä»“åº“ï¼‰ backup: type: git message: backup my blog of https://honjun.github.io/ repository: # ä½ çš„githubä»“åº“åœ°å€,å¤‡ä»½åˆ†æ”¯å ï¼ˆå»ºè®®æ–°å»ºbackupåˆ†æ”¯ï¼‰ github: https://github.com/honjun/honjun.github.io.git,backup # coding: https://git.coding.net/hojun/hojun.git,backup ä¸»é¢˜ç›®å½•ä¸‹çš„_configé…ç½® å…¶ä¸­æ ‡æ˜ã€æ”¹ã€‘çš„æ˜¯éœ€è¦ä¿®æ”¹éƒ¨é—¨ï¼Œæ ‡æ˜ã€é€‰ã€‘æ˜¯å¯æ”¹å¯ä¸æ”¹ï¼Œæ ‡æ˜ã€éã€‘æ˜¯ä¸ç”¨æ”¹çš„éƒ¨åˆ† # site name # ç«™ç‚¹å ã€æ”¹ã€‘ prefixName: ã•ãã‚‰è˜ãã® siteName: hojun # favicon and site master avatar # ç«™ç‚¹çš„faviconå’Œå¤´åƒ è¾“å…¥å›¾ç‰‡è·¯å¾„ï¼ˆä¸‹é¢çš„é…ç½®æ˜¯éƒ½æ˜¯cdnçš„ç›¸å¯¹è·¯å¾„ï¼Œæ²¡æœ‰cdnè¯·å¡«å†™å®Œæ•´è·¯å¾„ï¼Œå»ºè®®ä½¿ç”¨jsdeliveræ­å»ºä¸€ä¸ªcdnå•¦ï¼Œå…ˆå»ä¸‹è½½æˆ‘çš„cdnæ›¿æ¢ä¸‹å›¾ç‰‡å°±è¡Œäº†ï¼Œç®€å•æ–¹ä¾¿~ï¼‰ã€æ”¹ã€‘ favicon: /images/favicon.ico avatar: /img/custom/avatar.jpg # ç«™ç‚¹url ã€æ”¹ã€‘ url: https://sakura.hojun.cn # ç«™ç‚¹ä»‹ç»ï¼ˆæˆ–è€…è¯´æ˜¯ä¸ªäººç­¾åï¼‰ã€æ”¹ã€‘ description: Live your life with passion! With some drive! # ç«™ç‚¹cdnï¼Œæ²¡æœ‰å°±ä¸ºç©º ã€æ”¹ã€‘ è‹¥æ˜¯cdnä¸ºç©ºï¼Œä¸€äº›å›¾ç‰‡åœ°å€å°±è¦å¡«å®Œæ•´åœ°å€äº†ï¼Œæ¯”å¦‚ä¹‹å‰avatarå°±è¦å¡«https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/custom/avatar.jpg cdn: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6 # å¼€å¯pjax ã€é€‰ã€‘ pjax: 1 # ç«™ç‚¹é¦–é¡µçš„å…¬å‘Šä¿¡æ¯ ã€æ”¹ã€‘ notice: hexo-Sakuraä¸»é¢˜å·²ç»å¼€æºï¼Œç›®å‰æ­£åœ¨å¼€å‘ä¸­... # æ‡’åŠ è½½çš„åŠ è½½ä¸­å›¾ç‰‡ ã€é€‰ã€‘ lazyloadImg: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/loader/orange.progress-bar-stripe-loader.svg # ç«™ç‚¹èœå•é…ç½® ã€é€‰ã€‘ menus: é¦–é¡µ: { path: /, fa: fa-fort-awesome faa-shake } å½’æ¡£: { path: /archives, fa: fa-archive faa-shake, submenus: { æŠ€æœ¯: {path: /categories/æŠ€æœ¯/, fa: fa-code }, ç”Ÿæ´»: {path: /categories/ç”Ÿæ´»/, fa: fa-file-text-o }, èµ„æº: {path: /categories/èµ„æº/, fa: fa-cloud-download }, éšæƒ³: {path: /categories/éšæƒ³/, fa: fa-commenting-o }, è½¬è½½: {path: /categories/è½¬è½½/, fa: fa-book } } } æ¸…å•: { path: javascript:;, fa: fa-list-ul faa-vertical, submenus: { ä¹¦å•: {path: /tags/æ‚¦è¯»/, fa: fa-th-list faa-bounce }, ç•ªç»„: {path: /bangumi/, fa: fa-film faa-vertical }, æ­Œå•: {path: /music/, fa: fa-headphones }, å›¾é›†: {path: /tags/å›¾é›†/, fa: fa-photo } } } ç•™è¨€æ¿: { path: /comment/, fa: fa-pencil-square-o faa-tada } å‹äººå¸: { path: /links/, fa: fa-link faa-shake } èµèµ: { path: /donate/, fa: fa-heart faa-pulse } å…³äº: { path: /, fa: fa-leaf faa-wrench , submenus: { æˆ‘ï¼Ÿ: {path: /about/, fa: fa-meetup}, ä¸»é¢˜: {path: /theme-sakura/, fa: iconfont icon-sakura }, Lab: {path: /lab/, fa: fa-cogs }, } } å®¢æˆ·ç«¯: { path: /client/, fa: fa-android faa-vertical } RSS: { path: /atom.xml, fa: fa-rss faa-pulse } # Home page sort type: -1: newer firstï¼Œ1: older first. ã€éã€‘ homePageSortType: -1 # Home page article shown number) ã€éã€‘ homeArticleShown: 10 # èƒŒæ™¯å›¾ç‰‡ ã€é€‰ã€‘ bgn: 8 # startdashé¢æ¿ url, title, desc img ã€æ”¹ã€‘ startdash: - {url: /theme-sakura/, title: Sakura, desc: æœ¬ç«™ hexo ä¸»é¢˜, img: /img/startdash/sakura.md.png} - {url: http://space.bilibili.com/271849279, title: Bilibili, desc: åšä¸»çš„bç«™è§†é¢‘, img: /img/startdash/bilibili.jpg} - {url: /, title: hojunçš„ä¸‡äº‹å±‹, desc: æŠ€æœ¯æœåŠ¡, img: /img/startdash/wangshiwu.jpg} # your site build time or founded date # ä½ çš„ç«™ç‚¹å»ºç«‹æ—¥æœŸ ã€æ”¹ã€‘ siteBuildingTime: 07/17/2018 # ç¤¾äº¤æŒ‰é’®(social) url, img PCç«¯é…ç½® ã€æ”¹ã€‘ social: github: {url: http://github.com/honjun, img: /img/social/github.png} sina: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/sina.png} wangyiyun: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/wangyiyun.png} zhihu: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/zhihu.png} email: {url: http://weibo.com/mashirozx?is_all=1, img: /img/social/email.svg} wechat: {url: /#, qrcode: /img/custom/wechat.jpg, img: /img/social/wechat.png} # ç¤¾äº¤æŒ‰é’®(msocial) url, img ç§»åŠ¨ç«¯é…ç½® ã€æ”¹ã€‘ msocial: github: {url: http://github.com/honjun, fa: fa-github, color: 333} weibo: {url: http://weibo.com/mashirozx?is_all=1, fa: fa-weibo, color: dd4b39} qq: {url: https://wpa.qq.com/msgrd?v=3&amp;uin=954655431&amp;site=qq&amp;menu=yes, fa: fa-qq, color: 25c6fe} # èµèµäºŒç»´ç ï¼ˆå…¶ä¸­wechatSQæ˜¯èµèµå•é¡µé¢çš„èµèµç å›¾ç‰‡ï¼‰ã€æ”¹ã€‘ donate: alipay: /img/custom/donate/AliPayQR.jpg wechat: /img/custom/donate/WeChanQR.jpg wechatSQ: /img/custom/donate/WeChanSQ.jpg # é¦–é¡µè§†é¢‘åœ°å€ä¸ºhttps://cdn.jsdelivr.net/gh/honjun/hojun@1.2/Unbroken.mp4ï¼Œé…ç½®å¦‚ä¸‹ ã€æ”¹ã€‘ movies: url: https://cdn.jsdelivr.net/gh/honjun/hojun@1.2 # å¤šä¸ªè§†é¢‘ç”¨é€—å·éš”å¼€ï¼Œéšæœºè·å–ã€‚æ”¯æŒçš„æ ¼å¼ç›®å‰å·²çŸ¥MP4,Flvã€‚å…¶ä»–çš„å¯ä»¥è¯•ä¸‹ï¼Œä¸ä¿è¯æœ‰ç”¨ name: Unbroken.mp4 # å·¦ä¸‹è§’aplayeræ’­æ”¾å™¨é…ç½® ä¸»è¦æ”¹idå’Œserverè¿™ä¸¤é¡¹ï¼Œä¿®æ”¹è¯¦è§[aplayeræ–‡æ¡£] ã€æ”¹ã€‘ aplayer: id: 2660651585 server: netease type: playlist fixed: true mini: false autoplay: false loop: all order: random preload: auto volume: 0.7 mutex: true # Valineè¯„è®ºé…ç½®ã€æ”¹ã€‘ valine: true v_appId: GyC3NzMvd0hT9Yyd2hYIC0MN-gzGzoHsz v_appKey: mgOpfzbkHYqU92CV4IDlAUHQ åˆ†ç±»é¡µå’Œæ ‡ç­¾é¡µé…ç½® åˆ†ç±»é¡µ ### æ ‡ç­¾é¡µ é…ç½®é¡¹åœ¨-cn.ymlé‡Œã€‚æ–°å¢ä¸€ä¸ªåˆ†ç±»æˆ–æ ‡ç­¾æœ€å¥½åŠ ä¸‹å“¦ï¼Œå½“ç„¶å«Œéº»çƒ¦å¯ä»¥ç›´æ¥ä½¿ç”¨ä¸€å¼ é»˜è®¤å›¾ç‰‡ï¼ˆå¯ä»¥æ”¹ä¸»é¢˜æˆ–è€…ç›´æ¥æŠŠ404å›¾ç‰‡æ›¿æ¢ä¸‹ï¼Œå¾æ±‚ä¸‹æ„è§è¦ä¸è¦ç»™è¿™ä¸ªåœ¨é…ç½®æ–‡ä»¶ä¸­åŠ ä¸ªå¼€å…³ï¼Œå¯ä»¥issueæˆ–ç¾¤é‡Œæå‡ºæ¥ï¼‰ï¼Œç°åœ¨æ˜¯æ²¡è®¾ç½®çš„è¯ä¼šä½¿ç”¨é‚£ç§å€’ç«‹å°ç‹—404å“¦ã€‚ #category # æŒ‰åˆ†ç±»ååˆ›å»º æŠ€æœ¯: #ä¸­æ–‡æ ‡é¢˜ zh: é‡ç”ŸæŠ€æœ¯åä¼š # è‹±æ–‡æ ‡é¢˜ en: Geek â€“ Only for Love # å°é¢å›¾ç‰‡ img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/coding.jpg ç”Ÿæ´»: zh: ç”Ÿæ´» en: live img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/writing.jpg #tag # æ ‡ç­¾åå³æ˜¯æ ‡é¢˜ æ‚¦è¯»: # å°é¢å›¾ç‰‡ img: https://cdn.jsdelivr.net/gh/honjun/cdn@1.6/img/banner/reading.jpg å•é¡µé¢å°é¢é…ç½® å¦‚ç•™è¨€æ¿é¡µé¢é¡µé¢ï¼Œä½äºsourceä¸‹çš„commentä¸‹ï¼Œæ‰“å¼€index.mdå¦‚ä¸‹ï¼š --- title: comment date: 2018-12-20 23:13:48 keywords: ç•™è¨€æ¿ description: comments: true # åœ¨è¿™é‡Œé…ç½®å•é¡µé¢å¤´éƒ¨å›¾ç‰‡ï¼Œè‡ªå®šä¹‰æ›¿æ¢å“¦~ photos: https://cdn.jsdelivr.net/gh/honjun/cdn@1.4/img/banner/comment.jpg --- å•é¡µé¢é…ç½® ç•ªç»„è®¡åˆ’é¡µ ï¼ˆè¯·ç›´æ¥åœ¨ä¸‹è½½åçš„æ–‡ä»¶ä¸­æ”¹ï¼Œä¸‹é¢çš„æ·»åŠ äº†æ³¨é‡Šå¯èƒ½ä¼šæœ‰äº›å½±å“ï¼‰ --- layout: bangumi title: bangumi comments: false date: 2019-02-10 21:32:48 keywords: description: bangumis: # ç•ªç»„å›¾ç‰‡ - img: https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg # ç•ªç»„å title: æœèŠ±å¤•èª“â€”â€”äºç¦»åˆ«ä¹‹æœæŸèµ·çº¦å®šä¹‹èŠ± # è¿½ç•ªçŠ¶æ€ ï¼ˆè¿½ç•ªing/å·²è¿½å®Œï¼‰ status: å·²è¿½å®Œ # è¿½ç•ªè¿›åº¦ progress: 100 # ç•ªå‰§æ—¥æ–‡åç§° jp: ã•ã‚ˆãªã‚‰ã®æœã«ç´„æŸã®èŠ±ã‚’ã‹ã–ã‚ã† # æ”¾é€æ—¶é—´ time: æ”¾é€æ—¶é—´: 2018-02-24 SUN. # ç•ªå‰§ä»‹ç» desc: ä½åœ¨è¿œç¦»å°˜åš£çš„åœŸåœ°ï¼Œä¸€è¾¹å°†æ¯å¤©çš„äº‹æƒ…ç¼–ç»‡æˆåä¸ºå¸Œæ¯”æ¬§çš„å¸ƒï¼Œä¸€è¾¹é™é™ç”Ÿæ´»çš„ä¼Šæ¬§å¤«äººæ°‘ã€‚åœ¨15å²å·¦å³å¤–è¡¨å°±åœæ­¢æˆé•¿ï¼Œæ‹¥æœ‰æ•°ç™¾å¹´å¯¿å‘½çš„ä»–ä»¬ï¼Œè¢«ç§°ä¸ºâ€œç¦»åˆ«çš„ä¸€æ—â€ï¼Œå¹¶è¢«è§†ä¸ºæ´»ç€çš„ä¼ è¯´ã€‚æ²¡æœ‰åŒäº²çš„ä¼Šæ¬§å¤«å°‘å¥³ç›å¥‡äºšï¼Œè¿‡ç€è¢«ä¼™ä¼´åŒ…å›´çš„å¹³ç¨³æ—¥å­ï¼Œå´æ€»æ„Ÿè§‰â€œå­¤èº«ä¸€äººâ€ã€‚ä»–ä»¬çš„è¿™ç§æ—¥å¸¸ï¼Œä¸€ç¬é—´å°±å´©æºƒæ¶ˆå¤±ã€‚è¿½æ±‚ä¼Šæ¬§å¤«çš„é•¿å¯¿ä¹‹è¡€ï¼Œæ¢…è¨è’‚å†›ä¹˜åç€åä¸ºé›·çº³ç‰¹çš„å¤ä»£å…½å‘åŠ¨äº†è¿›æ”»ã€‚åœ¨ç»æœ›ä¸æ··ä¹±ä¹‹ä¸­ï¼Œä¼Šæ¬§å¤«çš„ç¬¬ä¸€ç¾å¥³è•¾è‰äºšè¢«æ¢…è¨è’‚å¸¦èµ°ï¼Œè€Œç›å¥‡äºšæš—æ‹çš„å°‘å¹´å…‹é‡Œå§†ä¹Ÿå¤±è¸ªäº†ã€‚ç›å¥‡äºšè™½ç„¶æ€»ç®—é€ƒè„±äº†ï¼Œå´å¤±å»äº†ä¼™ä¼´å’Œå½’å»ä¹‹åœ°â€¦â€¦ã€‚ - img: https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg title: æœèŠ±å¤•èª“â€”â€”äºç¦»åˆ«ä¹‹æœæŸèµ·çº¦å®šä¹‹èŠ± status: å·²è¿½å®Œ progress: 50 jp: ã•ã‚ˆãªã‚‰ã®æœã«ç´„æŸã®èŠ±ã‚’ã‹ã–ã‚ã† time: æ”¾é€æ—¶é—´: 2018-02-24 SUN. desc: ä½åœ¨è¿œç¦»å°˜åš£çš„åœŸåœ°ï¼Œä¸€è¾¹å°†æ¯å¤©çš„äº‹æƒ…ç¼–ç»‡æˆåä¸ºå¸Œæ¯”æ¬§çš„å¸ƒï¼Œä¸€è¾¹é™é™ç”Ÿæ´»çš„ä¼Šæ¬§å¤«äººæ°‘ã€‚åœ¨15å²å·¦å³å¤–è¡¨å°±åœæ­¢æˆé•¿ï¼Œæ‹¥æœ‰æ•°ç™¾å¹´å¯¿å‘½çš„ä»–ä»¬ï¼Œè¢«ç§°ä¸ºâ€œç¦»åˆ«çš„ä¸€æ—â€ï¼Œå¹¶è¢«è§†ä¸ºæ´»ç€çš„ä¼ è¯´ã€‚æ²¡æœ‰åŒäº²çš„ä¼Šæ¬§å¤«å°‘å¥³ç›å¥‡äºšï¼Œè¿‡ç€è¢«ä¼™ä¼´åŒ…å›´çš„å¹³ç¨³æ—¥å­ï¼Œå´æ€»æ„Ÿè§‰â€œå­¤èº«ä¸€äººâ€ã€‚ä»–ä»¬çš„è¿™ç§æ—¥å¸¸ï¼Œä¸€ç¬é—´å°±å´©æºƒæ¶ˆå¤±ã€‚è¿½æ±‚ä¼Šæ¬§å¤«çš„é•¿å¯¿ä¹‹è¡€ï¼Œæ¢…è¨è’‚å†›ä¹˜åç€åä¸ºé›·çº³ç‰¹çš„å¤ä»£å…½å‘åŠ¨äº†è¿›æ”»ã€‚åœ¨ç»æœ›ä¸æ··ä¹±ä¹‹ä¸­ï¼Œä¼Šæ¬§å¤«çš„ç¬¬ä¸€ç¾å¥³è•¾è‰äºšè¢«æ¢…è¨è’‚å¸¦èµ°ï¼Œè€Œç›å¥‡äºšæš—æ‹çš„å°‘å¹´å…‹é‡Œå§†ä¹Ÿå¤±è¸ªäº†ã€‚ç›å¥‡äºšè™½ç„¶æ€»ç®—é€ƒè„±äº†ï¼Œå´å¤±å»äº†ä¼™ä¼´å’Œå½’å»ä¹‹åœ°â€¦â€¦ã€‚ --- å‹é“¾é¡µ ï¼ˆè¯·ç›´æ¥åœ¨ä¸‹è½½åçš„æ–‡ä»¶ä¸­æ”¹ï¼Œä¸‹é¢çš„æ·»åŠ äº†æ³¨é‡Šå¯èƒ½ä¼šæœ‰äº›å½±å“ï¼‰ --- layout: links title: links # åˆ›å»ºæ—¥æœŸï¼Œå¯ä»¥æ”¹ä¸‹ date: 2018-12-19 23:11:06 # å›¾ç‰‡ä¸Šçš„æ ‡é¢˜ï¼Œè‡ªå®šä¹‰ä¿®æ”¹ keywords: å‹äººå¸ description: # true/false å¼€å¯/å…³é—­è¯„è®º comments: true # é¡µé¢å¤´éƒ¨å›¾ç‰‡ï¼Œè‡ªå®šä¹‰ä¿®æ”¹ photos: https://cdn.jsdelivr.net/gh/honjun/cdn@1.4/img/banner/links.jpg # å‹é“¾é…ç½® links: # ç±»å‹åˆ†ç»„ - group: ä¸ªäººé¡¹ç›® # ç±»å‹ç®€ä»‹ desc: å……åˆ†è¯´æ˜è¿™å®¶ä¼™æ˜¯æ¡å’¸é±¼ &lt; (ï¿£ï¸¶ï¿£)&gt; items: # å‹é“¾é“¾æ¥ - url: https://shino.cc/fgvf # å‹é“¾å¤´åƒ img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg # å‹é“¾ç«™ç‚¹å name: Google # å‹é“¾ä»‹ç» ä¸‹é¢é›·åŒ desc: Google é•œåƒ - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google é•œåƒ # ç±»å‹åˆ†ç»„... - group: å°ä¼™ä¼´ä»¬ desc: æ¬¢è¿äº¤æ¢å‹é“¾ ê‰‚(ËŠá—œË‹) items: - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google é•œåƒ - url: https://shino.cc/fgvf img: https://cloud.moezx.cc/Picture/svg/landscape/fields.svg name: Google desc: Google é•œåƒ --- å†™æ–‡ç« é…ç½® ä¸»é¢˜é›†æˆäº†ä¸ªäººæ’ä»¶hexo-tag-biliå’Œhexo-tag-fancybox_imgã€‚å…¶ä¸­hexo-tag-biliç”¨æ¥åœ¨æ–‡ç« æˆ–å•é¡µé¢ä¸­æ’å…¥Bç«™å¤–é“¾è§†é¢‘ï¼Œä½¿ç”¨è¯­æ³•å¦‚ä¸‹ï¼š è¯¦ç»†ä½¿ç”¨æ•™ç¨‹è¯¦è§hexo-tag-biliã€‚ hexo-tag-fancybox_imgç”¨æ¥åœ¨æ–‡ç« æˆ–å•é¡µé¢ä¸­å›¾ç‰‡ï¼Œä½¿ç”¨è¯­æ³•å¦‚ä¸‹ï¼š è¯¦ç»†ä½¿ç”¨æ•™ç¨‹è¯¦è§hexo-tag-fancybox_img è¿˜æœ‰å•¥ï¼Œä¸€æ—¶æƒ³ä¸èµ·æ¥...... To be continued...","categories":[],"tags":[{"name":"web","slug":"web","permalink":"https://littlelittlemoon.github.io/tags/web/"},{"name":"æ‚¦è¯»","slug":"æ‚¦è¯»","permalink":"https://littlelittlemoon.github.io/tags/%E6%82%A6%E8%AF%BB/"}],"keywords":[]}]}